Code:

import os
import json
import time
import random
from pathlib import Path
from tqdm import tqdm

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    BitsAndBytesConfig
)
from langdetect import detect, DetectorFactory

# ============================================================
# GLOBALS
# ============================================================

DetectorFactory.seed = 42
torch.set_grad_enabled(False)

print("Torch:", torch.__version__)
print("CUDA:", torch.version.cuda)

# ============================================================
# CONFIG
# ============================================================

# -------- IndicTrans2 --------
INDICTRANS_MODEL = "ai4bharat/indictrans2-en-indic-1B"

# -------- Summarizer --------
QWEN_MODEL = "Qwen/Qwen2.5-7B-Instruct"

USE_4BIT = True

# -------- Data --------
TEST_DIR = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/test"
TRAIN_SPLIT_DIR = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/train_split"
OUTPUT_DIR = "/workspace/data/KZ_2117574/EACL/IndicTrans2_Qwen_pipeline"

TARGET_LANGS = ["Hindi", "Marathi", "Bangla"]

# -------- Few-shot --------
FEW_SHOT_K = 2          # set to 0 for zero-shot
FEW_SHOT_SEED = 42

# -------- Safety limits --------
MAX_TOTAL_INPUT_TOKENS = 8192
TARGET_DIALOGUE_TOKENS = 2048
EXAMPLE_DIALOGUE_TOKENS = 512
MAX_NEW_TOKENS = 512
MAX_TEST_EXAMPLES = 100

SYSTEM_PROMPT = (
    "You are a clinical summarization assistant. "
    "Read a doctor–patient dialogue and write a fluent English summary "
    "focusing on symptoms, diagnosis, investigations, management, and follow-up. "
    "Do not hallucinate new information. "
    "End your summary with <<END>>."
)

# ============================================================
# UTILS
# ============================================================

def tprint(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def safe_read_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            try:
                rows.append(json.loads(line))
            except:
                continue
    return rows

def write_text(path, text):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text.strip() + "\n")

def clip_tokens(tok, text, max_tokens):
    ids = tok.encode(text, add_special_tokens=False)
    if len(ids) <= max_tokens:
        return text
    return tok.decode(ids[-max_tokens:], skip_special_tokens=True)

def clip_after_template(tok, text, max_tokens):
    ids = tok.encode(text, add_special_tokens=False)
    if len(ids) <= max_tokens:
        return text
    return tok.decode(ids[-max_tokens:], skip_special_tokens=True)

def build_messages(system_prompt, user_prompt):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

# ============================================================
# INDIC TRANS2 TRANSLATION
# ============================================================

def load_indictrans():
    tprint("Loading IndicTrans2")
    tok = AutoTokenizer.from_pretrained(INDICTRANS_MODEL)
    model = AutoModelForSeq2SeqLM.from_pretrained(
        INDICTRANS_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    model.eval()
    return model, tok

def translate_to_english(model, tok, text):
    inputs = tok(text, return_tensors="pt", truncation=True).to(model.device)
    with torch.no_grad():
        out = model.generate(**inputs, max_new_tokens=2048)
    return tok.decode(out[0], skip_special_tokens=True)

# ============================================================
# QWEN SUMMARIZER
# ============================================================

def load_qwen():
    tprint("Loading Qwen-2.5-7B-Instruct")
    tok = AutoTokenizer.from_pretrained(QWEN_MODEL, use_fast=True)
    tok.pad_token = tok.eos_token

    quant_cfg = BitsAndBytesConfig(
        load_in_4bit=USE_4BIT,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    model = AutoModelForCausalLM.from_pretrained(
        QWEN_MODEL,
        device_map="auto",
        quantization_config=quant_cfg,
        dtype=torch.float16,
        trust_remote_code=True
    )

    model.eval()
    return model, tok

def summarize_qwen(model, tok, messages):
    prompt = tok.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    prompt = clip_after_template(tok, prompt, MAX_TOTAL_INPUT_TOKENS)

    inputs = tok([prompt], return_tensors="pt").to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            num_beams=1,
            use_cache=True,
            pad_token_id=tok.eos_token_id
        )

    gen = out[0][len(inputs.input_ids[0]):]
    return tok.decode(gen, skip_special_tokens=True)

# ============================================================
# FEW-SHOT COLLECTION (ENGLISH SUMMARIES)
# ============================================================

def collect_few_shot(train_root, lang, k, seed):
    rng = random.Random(seed)
    dlg = Path(train_root) / lang / "Dialogues"
    summ = Path(train_root) / lang / "Summary_Text"
    if not dlg.exists():
        return []

    files = sorted(dlg.glob("*.jsonl"))[:100]
    rng.shuffle(files)

    examples = []
    for f in files:
        sf = summ / f"{f.stem}_summary.txt"
        if not sf.exists():
            continue

        dialogue = " ".join(
            r.get("dialogue", "")
            for r in safe_read_jsonl(f)
            if isinstance(r, dict)
        ).strip()

        summary = sf.read_text(encoding="utf-8").strip()
        if dialogue and summary:
            examples.append((dialogue, summary))
        if len(examples) == k:
            break

    return examples

# ============================================================
# MAIN PIPELINE
# ============================================================

def run_pipeline():
    trans_model, trans_tok = load_indictrans()
    qwen_model, qwen_tok = load_qwen()

    for lang_dir in Path(TEST_DIR).iterdir():
        if lang_dir.name not in TARGET_LANGS:
            continue

        tprint(f"Language: {lang_dir.name}")
        dlg_dir = lang_dir / "Dialogues"
        out_dir = Path(OUTPUT_DIR) / lang_dir.name / "Summary_Text"

        files = sorted(dlg_dir.glob("*.jsonl"))[:MAX_TEST_EXAMPLES]

        few_shot = collect_few_shot(
            TRAIN_SPLIT_DIR,
            lang_dir.name,
            FEW_SHOT_K,
            FEW_SHOT_SEED
        )

        for f in tqdm(files, desc=lang_dir.name):
            out_file = out_dir / f"{f.stem}_summary.txt"
            if out_file.exists():
                continue

            # -------- Load dialogue --------
            dialogue_native = " ".join(
                r.get("dialogue", "")
                for r in safe_read_jsonl(f)
                if isinstance(r, dict)
            )

            # -------- Translate --------
            dialogue_en = translate_to_english(
                trans_model,
                trans_tok,
                dialogue_native
            )
            dialogue_en = clip_tokens(qwen_tok, dialogue_en, TARGET_DIALOGUE_TOKENS)

            # -------- Few-shot prefix --------
            prefix = ""
            for i, (d, s) in enumerate(few_shot, 1):
                d = clip_tokens(qwen_tok, d, EXAMPLE_DIALOGUE_TOKENS)
                s = s.replace("<<END>>", "").strip()
                prefix += (
                    f"Example {i}:\nDialogue:\n{d}\n\n"
                    f"English Summary:\n{s}\n---\n"
                )

            user_prompt = (
                f"{prefix}\nDialogue:\n{dialogue_en}\n\n"
                "Write a detailed English clinical summary and end with <<END>>."
            )

            summary_raw = summarize_qwen(
                qwen_model,
                qwen_tok,
                build_messages(SYSTEM_PROMPT, user_prompt)
            )

            summary = summary_raw.split("<<END>>")[0].strip()
            write_text(out_file, summary)

        torch.cuda.empty_cache()

    tprint("Pipeline complete.")

# ============================================================
# ENTRY
# ============================================================

if __name__ == "__main__":
    run_pipeline()


Error:
?ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
cudf 23.6.0 requires protobuf<4.22,>=4.21.6, but you have protobuf 4.25.1 which is incompatible.
torch-tensorrt 2.0.0.dev0 requires torch<2.2,>=2.1.dev, but you have torch 2.2.2 which is incompatible.
torchdata 0.7.0a0 requires torch==2.1.0a0+29c30b1, but you have torch 2.2.2 which is incompatible.
torchtext 0.16.0a0 requires torch==2.1.0a0+29c30b1, but you have torch 2.2.2 which is incompatible.

�Successfully installed accelerate-0.34.0 bitsandbytes-0.43.3 colorama-0.4.6 datasets-2.19.0 dill-0.3.8 evaluate-0.4.3 hf-xet-1.2.1 huggingface_hub-0.36.0 langdetect-1.0.9 lxml-6.0.2 multiprocess-0.70.16 nltk-3.9.2 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 peft-0.10.0 portalocker-3.2.0 protobuf-4.25.1 pyarrow-22.0.0 pyarrow-hotfix-0.7 rouge-score-0.1.2 sacrebleu-2.5.1 safetensors-0.7.0 scikit-learn-1.3.2 scipy-1.10.1 sentencepiece-0.2.0 soxr-1.0.0 tokenizers-0.22.1 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2 transformers-4.57.3 triton-2.2.0 typing-extensions-4.15.0 xxhash-3.6.0

�WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv

y
[notice] A new release of pip is available: 23.2.1 -> 25.3
[notice] To update, run: python -m pip install --upgrade pip

dSuccessfully logged into Hugging Face!
Torch: 2.2.2+cu121
CUDA: 12.1
[12:32:15] Loading IndicTrans2

�The repository ai4bharat/indictrans2-indic-en-1B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-1B .
 You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-1B.
You can avoid this prompt in future by passing the argument `trust_remote_code=True`.

Do you wish to run the custom code? [y/N] 
�Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py", line 757, in resolve_trust_remote_code

�    answer = input(
EOFError: EOF when reading a line

During handling of the above exception, another exception occurred:

Traceback (most recent call last):

E  File "/workspace/multi_summ/indictrans2.py", line 305, in <module>

\    run_pipeline()
  File "/workspace/multi_summ/indictrans2.py", line 232, in run_pipeline

{    trans_model, trans_tok = load_indictrans()
  File "/workspace/multi_summ/indictrans2.py", line 128, in load_indictrans

�    tok = AutoTokenizer.from_pretrained(INDICTRANS_MODEL)
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py", line 1133, in from_pretrained

�    trust_remote_code = resolve_trust_remote_code(
  File "/usr/local/lib/python3.10/dist-packages/transformers/dynamic_module_utils.py", line 769, in resolve_trust_remote_code

�    raise ValueError(
ValueError: The repository ai4bharat/indictrans2-indic-en-1B contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-1B .
 You can inspect the repository content at https://hf.co/ai4bharat/indictrans2-indic-en-1B.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
