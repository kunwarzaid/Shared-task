import os, json, sys, inspect, traceback, torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# ============================================================
# ACCELERATE COMPATIBILITY SHIM
# ============================================================
def accelerate_compat_shim():
    try:
        import accelerate
        sig = inspect.signature(accelerate.Accelerator.unwrap_model)
        if "keep_torch_compile" not in sig.parameters:
            _orig = accelerate.Accelerator.unwrap_model
            def _shim(self, model, *a, **kw):
                kw.pop("keep_torch_compile", None)
                return _orig(self, model, *a, **kw)
            accelerate.Accelerator.unwrap_model = _shim
            print("[shim] unwrap_model patched for Accelerate.")
    except Exception as e:
        print("[shim] Could not apply accelerate shim:", e)
        traceback.print_exc()
accelerate_compat_shim()

# ============================================================
# PATCH: BYPASS TORCH 2.6 SAFETY CHECK
# ============================================================
try:
    import transformers.utils.import_utils as imp_utils
    if hasattr(imp_utils, "check_torch_load_is_safe"):
        def _patched_check_torch_load_is_safe():
            print("‚ö†Ô∏è  [patch] Skipping torch.load safety version check (temporary patch for torch<2.6)")
            return None
        imp_utils.check_torch_load_is_safe = _patched_check_torch_load_is_safe
        print("[shim] Patched Transformers torch.load safety check.")
except Exception as e:
    print("[shim] Could not patch torch safety check:", e)
    traceback.print_exc()

# ============================================================
# CONFIG
# ============================================================
DATA_DIR   = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set"
BASE_MODEL = "/workspace/data/KZ_2117574/Qwen2_1.5B"
OUTPUT_DIR = "/workspace/data/KZ_2117574/qwen15b_lora_multilingual"

SEED = 42
CTX_MAX = 2048
PROMPT_MAX = 768
TARGET_MAX = CTX_MAX - PROMPT_MAX
WINDOW_OVERLAP = 256
BATCH_SIZE = 1
GRAD_ACCUM = 8
NUM_EPOCHS = 1
LR = 2e-4
SAVE_STEPS = 1000
FP16 = True

# ============================================================
# HELPERS
# ============================================================
def tprint(msg): print(f"[{msg}]", flush=True)

def safe_read_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            s = line.strip()
            if s:
                try: rows.append(json.loads(s))
                except: continue
    return rows

def join_dialogue(rows):
    parts=[]
    for r in rows:
        if isinstance(r, dict):
            val = r.get("dialogue","")
            if isinstance(val, list): val=" ".join(map(str,val))
            parts.append(str(val))
        else:
            parts.append(str(r))
    return "\n".join(parts).strip()

# ============================================================
# LOAD TRAINING DATA
# ============================================================
def make_examples(split):
    examples=[]
    for lang in os.listdir(split):
        ldir=os.path.join(split,lang)
        if not os.path.isdir(ldir): continue
        dlg=os.path.join(ldir,"Dialogues")
        summ=os.path.join(ldir,"Summary_Text")
        qna=os.path.join(ldir,"QnA")

        # ---- Summaries ----
        if os.path.isdir(dlg) and os.path.isdir(summ):
            for fn in os.listdir(dlg):
                if not fn.endswith(".jsonl"): continue
                rows=safe_read_jsonl(os.path.join(dlg,fn))
                dialogue=join_dialogue(rows)
                if not dialogue: continue
                sfile=os.path.join(summ,fn.replace(".jsonl","_summary.txt"))
                if not os.path.exists(sfile): continue
                text=open(sfile,"r",encoding="utf-8").read().strip()
                if not text: continue
                prompt=f"Summarize the following doctor‚Äìpatient dialogue in English:\n{dialogue}\n\nSummary:"
                examples.append({"prompt":prompt,"target":text})

        # ---- QnA ----
        if os.path.isdir(qna):
            for fn in os.listdir(qna):
                if not fn.endswith(".json"): continue
                try: data=json.load(open(os.path.join(qna,fn),encoding="utf-8"))
                except: continue
                for qa in data.get("questions",[]):
                    q=qa.get("question","").strip()
                    a=qa.get("answer","").strip()
                    if not q or not a: continue
                    prompt=f"Answer the following question in the same language:\nQuestion: {q}\nAnswer:"
                    examples.append({"prompt":prompt,"target":a})
    tprint(f"Loaded {len(examples)} examples from {split}")
    return examples

# ============================================================
# TOKENIZER + SLIDING CHUNKING
# ============================================================
def build_tokenizer(path):
    tok=AutoTokenizer.from_pretrained(path, use_fast=True)
    if tok.pad_token is None: tok.pad_token=tok.eos_token
    return tok

def sliding_chunks(ids, size, overlap):
    if len(ids)<=size: return [ids]
    chunks=[]; step=size-overlap
    for i in range(0,len(ids),step):
        chunks.append(ids[i:i+size])
        if i+size>=len(ids): break
    return chunks

def map_fn(tokenizer):
    def _map(batch):
        out={"input_ids":[],"attention_mask":[],"labels":[]}
        for p,t in zip(batch["prompt"],batch["target"]):
            p_ids=tokenizer.encode(p,add_special_tokens=False)[-PROMPT_MAX:]
            t_ids=tokenizer.encode(" "+t,add_special_tokens=False)[:TARGET_MAX]
            full=p_ids+t_ids
            for ch in sliding_chunks(full,CTX_MAX,WINDOW_OVERLAP):
                att=[1]*len(ch)
                plen=min(len(ch),len(p_ids))
                labels=[-100]*plen+ch[plen:]
                out["input_ids"].append(ch)
                out["attention_mask"].append(att)
                out["labels"].append(labels)
        return out
    return _map

# ============================================================
# PAD COLLATOR
# ============================================================
class Collator:
    def __init__(self,tok): self.tok=tok
    def __call__(self,batch):
        m=max(len(x["input_ids"]) for x in batch)
        def pad(lst,val): return lst+[val]*(m-len(lst))
        ids=[pad(x["input_ids"],self.tok.pad_token_id) for x in batch]
        att=[pad(x["attention_mask"],0) for x in batch]
        lbl=[pad(x["labels"],-100) for x in batch]
        return {
            "input_ids":torch.tensor(ids),
            "attention_mask":torch.tensor(att),
            "labels":torch.tensor(lbl)
        }

# ============================================================
# MODEL + QLoRA
# ============================================================
def build_model():
    cfg=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )
    model=AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        quantization_config=cfg,
        torch_dtype=torch.float16
    )
    model.gradient_checkpointing_enable()
    model.config.use_cache=False
    if hasattr(model,"enable_input_require_grads"): model.enable_input_require_grads()
    lora=LoraConfig(
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        target_modules=["q_proj","v_proj"],
        task_type="CAUSAL_LM"
    )
    model=get_peft_model(model,lora)
    model.print_trainable_parameters()
    return model

# ============================================================
# MAIN TRAIN LOOP
# ============================================================
def main():
    torch.manual_seed(SEED)
    tok=build_tokenizer(BASE_MODEL)
    train_raw=make_examples(os.path.join(DATA_DIR,"train"))
    dev_raw=make_examples(os.path.join(DATA_DIR,"dev"))

    train=Dataset.from_list(train_raw).map(map_fn(tok),batched=True,num_proc=4,remove_columns=["prompt","target"])
    dev=Dataset.from_list(dev_raw).map(map_fn(tok),batched=True,num_proc=4,remove_columns=["prompt","target"])

    model=build_model()
    args=TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=BATCH_SIZE,
        gradient_accumulation_steps=GRAD_ACCUM,
        num_train_epochs=NUM_EPOCHS,
        learning_rate=LR,
        fp16=FP16,
        logging_steps=50,
        save_strategy="steps",
        save_steps=SAVE_STEPS,
        save_total_limit=3,
        evaluation_strategy="epoch",
        report_to="none",
        max_steps=28000
    )
    trainer=Trainer(
        model=model,
        args=args,
        train_dataset=train,
        eval_dataset=dev,
        data_collator=Collator(tok)
    )

    ckpt=None
    if os.path.isdir(OUTPUT_DIR):
        ckpts=[d for d in os.listdir(OUTPUT_DIR) if d.startswith("checkpoint-")]
        if ckpts:
            ckpt=os.path.join(OUTPUT_DIR,sorted(ckpts,key=lambda x:int(x.split('-')[-1]))[-1])

    if ckpt:
        tprint(f"Resuming from {ckpt}")
        try:
            state_path=os.path.join(ckpt,"trainer_state.json")
            if os.path.exists(state_path):
                with open(state_path,"r") as f:
                    state=json.load(f)
                step=state.get("global_step",0)
                tprint(f"üîÅ Detected resume step: {step}")
            else:
                tprint("‚ö†Ô∏è No trainer_state.json found ‚Äî resuming blindly.")
        except Exception as e:
            tprint(f"‚ö†Ô∏è Could not read step info: {e}")
        trainer.train(resume_from_checkpoint=ckpt)
    else:
        tprint("‚ú® Fresh run")
        trainer.train()

    tprint("üíæ Saving final model‚Ä¶")
    trainer.save_model(OUTPUT_DIR)
    tok.save_pretrained(OUTPUT_DIR)
    tprint("‚úÖ Training complete!")

if __name__=="__main__":
    main()
