import os, json, time, re
from pathlib import Path
from typing import Dict, Any
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from langdetect import detect, DetectorFactory

DetectorFactory.seed = 42
torch.set_grad_enabled(False)

# -----------------------
# PATHS
# -----------------------
BASE_MODEL   = "/workspace/data/KZ_2117574/Qwen2.5-1.5B-Instruct"  # or your local path
ADAPTER_DIR  = "/workspace/data/KZ_2117574/qwen15b_lora_multilingual"
TEST_DIR     = "/workspace/data/KZ_2117574/test_data_release/test_data_release"
OUTPUT_DIR   = "/workspace/data/KZ_2117574/SharedTask_Inference_Output_qwen"

# -----------------------
# RUNTIME
# -----------------------
MAX_INPUT_TOKENS       = 32000         # Qwen2.5 supports 32k
MAX_NEW_TOKENS_SUMMARY = 512
MAX_NEW_TOKENS_JSON    = 120
MAX_NEW_TOKENS_QNA     = 200
BATCH_SIZE             = 2
TEMPERATURE            = 0.0           # deterministic for consistency

LANG_HINTS = {
    "English": "English", "Hindi": "Hindi", "Gujarati": "Gujarati",
    "Bangla": "Bangla", "Assamese": "Assamese", "Kannada": "Kannada",
    "Marathi": "Marathi", "Tamil": "Tamil", "Telugu": "Telugu", "Dogri": "Dogri"
}

# -----------------------
# PROMPTS
# -----------------------
SYSTEM_SUMMARY = (
    "You are a professional clinical summarization assistant.\n"
    "Write a fluent English summary ONLY (no other language, no bullets), focusing on diagnosis, key symptoms, investigations and management.\n"
    "Write 6‚Äì10 sentences. End your summary with the exact token <<END>>."
)

SYSTEM_JSON = (
    "You are a clinical information extraction assistant.\n"
    "Answer in English only. Be concise. If the information is not present, answer exactly 'N/A'.\n"
    "Do not add explanations. Follow the length limit requested."
)

SYSTEM_QNA = (
    "You are a multilingual clinical assistant. Answer in the SAME LANGUAGE as the user's question.\n"
    "Be concise, factual, and helpful."
)

# Each entry: (question, type, max_words, is_list)
JSON_FIELDS: Dict[str, tuple] = {
    "patient_identifiers": ("Identify the patient by name or ID if mentioned.", "str", 12, False),
    "demographics.age": ("What is the patient's age?", "str", 6, False),
    "demographics.sex": ("What is the patient's sex?", "str", 3, False),
    "visit.date_time": ("When did the visit occur? (date/time if mentioned)", "str", 10, False),
    "visit.type": ("What type of visit was it? (in-person/telemedicine etc.)", "str", 8, False),
    "chief_complaint": ("What is the chief complaint?", "str", 14, False),
    "onset_duration": ("How long have symptoms been present?", "str", 10, False),
    "symptom_description": ("Describe main symptoms in short.", "str", 20, False),
    "aggravating_factors": ("What aggravates symptoms?", "str", 14, False),
    "relieving_factors": ("What relieves symptoms?", "str", 14, False),
    "associated_symptoms": ("List associated symptoms; use a semicolon between items (‚â§5).", "list", 30, True),
    "past_medical_history": ("Summarize relevant past medical history.", "str", 16, False),
    "past_surgical_history": ("Summarize relevant past surgical history.", "str", 16, False),
    "family_history": ("Summarize family medical history.", "str", 16, False),
    "current_medications": ("List current medications; use a semicolon between items (‚â§5).", "list", 24, True),
    "allergies": ("List allergies (or N/A).", "str", 10, False),
    "social_history": ("Social history (tobacco/alcohol/other) short.", "str", 18, False),
    "functional_status": ("Summarize functional status briefly.", "str", 16, False),
    "vital_signs": ("List any vitals or key physical signs briefly.", "str", 18, False),
    "examination_findings": ("Summarize examination findings briefly.", "str", 18, False),
    "investigations": ("List investigations performed/planned; semicolon-separated (‚â§6).", "list", 30, True),
    "assessment_primary_diagnosis": ("What is the main working diagnosis?", "str", 14, False),
    "differential_diagnoses": ("List 2‚Äì4 differentials; semicolon-separated.", "list", 16, True),
    "management_plan": ("Summarize management plan in one sentence.", "str", 22, False),
    "tests_referrals_planned": ("List tests/referrals planned; semicolon-separated (‚â§6).", "list", 30, True),
    "follow_up_plan": ("Summarize follow-up plan briefly.", "str", 16, False),
    "chronology_response_to_treatment": ("Describe response/progress if mentioned.", "str", 18, False),
    "patient_concerns_preferences_consent": ("Summarize patient concerns or consent briefly.", "str", 18, False),
    "safety_issues_red_flags": ("List any red flags briefly.", "str", 18, False),
    "coding_terms": ("Give 1‚Äì3 likely codes/terms if possible; else N/A.", "str", 16, False),
}

# -----------------------
# UTILS
# -----------------------
def tprint(msg): print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def safe_read_jsonl(path: Path):
    rows=[]
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            s=line.strip()
            if s:
                try: rows.append(json.loads(s))
                except: continue
    return rows

def write_json(path: Path, obj: Any):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def write_text(path: Path, text: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text.strip() + "\n")

def detect_lang(text: str) -> str:
    try:
        return detect(text)
    except Exception:
        return "unknown"

def clip_tokens(tokenizer, text, max_tokens):
    ids = tokenizer.encode(text, add_special_tokens=False)
    if len(ids) <= max_tokens:
        return text
    ids = ids[-max_tokens:]
    return tokenizer.decode(ids, skip_special_tokens=True)

def shorten(text: str, max_words: int, max_chars: int = 180) -> str:
    text = re.sub(r"\s+", " ", text.strip())
    words = text.split()
    if len(words) > max_words:
        text = " ".join(words[:max_words])
    if len(text) > max_chars:
        text = text[:max_chars].rstrip(" .,;:-")
    return text

def parse_semicolon_list(s: str, max_items: int = 6):
    if s.strip().upper() == "N/A": return []
    parts = [p.strip(" ,;.-") for p in s.split(";")]
    parts = [p for p in parts if p]
    return parts[:max_items]

# -----------------------
# CHAT
# -----------------------
def build_messages(system_prompt, user_prompt):
    return [
        {"role":"system","content":system_prompt},
        {"role":"user","content":user_prompt}
    ]

def chat_generate(model, tokenizer, messages, max_new_tokens):
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=TEMPERATURE,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    gen = outputs[0][len(inputs.input_ids[0]):]
    return tokenizer.decode(gen, skip_special_tokens=True).strip()

# -----------------------
# MODEL LOADING
# -----------------------
def find_latest_adapter(path):
    p = Path(path)
    cands = [d for d in p.iterdir() if d.is_dir() and d.name.startswith("adapter_step_")]
    if not cands:
        return path
    return sorted(cands, key=lambda x: int(x.name.split("_")[-1]))[-1]

def load_model():
    tprint("üîª Loading tokenizer...")
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    tprint("üîª Loading base model...")
    base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        quantization_config=bnb,
        torch_dtype=torch.float16
    )

    adapter = find_latest_adapter(ADAPTER_DIR)
    tprint(f"üß≠ Using adapter: {adapter}")
    model = PeftModel.from_pretrained(base, adapter)
    model.eval()
    return model, tok

# -----------------------
# MAIN
# -----------------------
def main():
    model, tokenizer = load_model()
    langs = [p for p in Path(TEST_DIR).iterdir() if p.is_dir()]
    tprint(f"üåç Found {len(langs)} languages: {[p.name for p in langs]}")

    for lang_dir in langs:
        lang = lang_dir.name
        lang_hint = LANG_HINTS.get(lang, "the same language")
        out_lang = Path(OUTPUT_DIR) / lang
        dlg_dir = lang_dir / "Dialogues"
        qna_dir = lang_dir / "QnA"

        tprint(f"üóÇ Language: {lang}")

        # ---- SUMMARIES + JSON (concise) ----
        if dlg_dir.exists():
            files = sorted(dlg_dir.glob("*.jsonl"))
            tprint(f"  ‚Ä¢ Dialogues: {len(files)}")

            for f in tqdm(files, desc=f"{lang} dialogues"):
                out_sum_txt  = out_lang / "Summary_Text"  / f"{f.stem}_summary.txt"
                out_sum_json = out_lang / "Summary_Json"  / f"{f.stem}_summary.json"
                if out_sum_txt.exists() and out_sum_json.exists():
                    continue

                rows = safe_read_jsonl(f)
                dialogue = " ".join(
                    str(r.get("dialogue","")) if isinstance(r,dict) else str(r)
                    for r in rows
                )
                dialogue_clip = clip_tokens(tokenizer, dialogue, MAX_INPUT_TOKENS)

                # Summary with sentinel end
                messages = build_messages(
                    SYSTEM_SUMMARY,
                    f"Dialogue:\n{dialogue_clip}\n\nWrite the summary and end with <<END>>."
                )
                summary_raw = chat_generate(model, tokenizer, messages, MAX_NEW_TOKENS_SUMMARY)
                # cut at sentinel
                end_pos = summary_raw.find("<<END>>")
                summary = summary_raw[:end_pos].strip() if end_pos != -1 else summary_raw.strip()
                # cleanup language leakage
                if detect_lang(summary) != "en":
                    # one retry enforcing English
                    messages = build_messages(
                        SYSTEM_SUMMARY,
                        f"Dialogue:\n{dialogue_clip}\n\nWrite the summary ONLY in English. End with <<END>>."
                    )
                    summary_raw = chat_generate(model, tokenizer, messages, MAX_NEW_TOKENS_SUMMARY)
                    end_pos = summary_raw.find("<<END>>")
                    summary = summary_raw[:end_pos].strip() if end_pos != -1 else summary_raw.strip()

                write_text(out_sum_txt, summary)

                # Fill JSON field-by-field (concise)
                flat: Dict[str, Any] = {}
                for field, (q, typ, max_words, is_list) in JSON_FIELDS.items():
                    user_q = (
                        f"Summary (English):\n{summary}\n\n"
                        f"Dialogue (for reference):\n{dialogue_clip}\n\n"
                        f"Question:\n{q}\n\n"
                        f"Answer in English, ‚â§{max_words} words. "
                        f"{'Return a semicolon-separated list.' if is_list else 'Return a short phrase.'} "
                        f"If unknown, answer exactly N/A."
                    )
                    msg = build_messages(SYSTEM_JSON, user_q)
                    ans = chat_generate(model, tokenizer, msg, MAX_NEW_TOKENS_JSON)
                    # enforce English & brevity
                    if detect_lang(ans) != "en":
                        msg = build_messages(SYSTEM_JSON, user_q + "\n\nAnswer strictly in English.")
                        ans = chat_generate(model, tokenizer, msg, MAX_NEW_TOKENS_JSON)
                    ans = shorten(ans, max_words=max_words, max_chars=180)

                    if is_list:
                        flat[field] = parse_semicolon_list(ans, max_items=6)
                    else:
                        flat[field] = ans if ans else "N/A"

                # expand dotted keys to nested dict
                nested = {}
                for k, v in flat.items():
                    parts = k.split(".")
                    cur = nested
                    for p in parts[:-1]:
                        if p not in cur or not isinstance(cur[p], dict):
                            cur[p] = {}
                        cur = cur[p]
                    cur[parts[-1]] = v

                write_json(out_sum_json, nested)

        # ---- QnA (keep language) ----
        if qna_dir.exists():
            qfiles = sorted(qna_dir.glob("*.json"))
            tprint(f"  ‚Ä¢ QnA files: {len(qfiles)}")
            for qf in tqdm(qfiles, desc=f"{lang} QnA"):
                try:
                    data = json.load(open(qf, encoding="utf-8"))
                except Exception:
                    tprint(f"    - Skipped malformed {qf.name}")
                    continue

                qs = [q.get("question","").strip() for q in data.get("questions",[]) if q.get("question","").strip()]
                if not qs:
                    continue

                answers = []
                for q in qs:
                    msg = build_messages(SYSTEM_QNA, f"Question ({lang_hint}): {q}")
                    ans = chat_generate(model, tokenizer, msg, MAX_NEW_TOKENS_QNA)
                    det = detect_lang(ans)
                    # if wrong language, reprompt strictly
                    if lang_hint.lower() not in ("english", det.lower()):
                        msg = build_messages(SYSTEM_QNA, f"Answer strictly in {lang_hint}:\nQuestion: {q}")
                        ans = chat_generate(model, tokenizer, msg, MAX_NEW_TOKENS_QNA)
                    answers.append(ans.strip())

                out_payload = {"questions":[{"question":q,"answer":a} for q,a in zip(qs,answers)]}
                write_json((Path(OUTPUT_DIR)/lang/"QnA"/f"{qf.stem}_answers.json"), out_payload)

        torch.cuda.empty_cache()

    tprint(f"‚úÖ Inference complete! Results saved to {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
