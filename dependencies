import os, json, sys, inspect, traceback, torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# ============================================================
# ACCELERATE SHIM (for unwrap_model compatibility)
# ============================================================
def accelerate_compat_shim():
    try:
        import accelerate
        sig = inspect.signature(accelerate.Accelerator.unwrap_model)
        if "keep_torch_compile" not in sig.parameters:
            _orig = accelerate.Accelerator.unwrap_model
            def _shim(self, model, *a, **kw):
                kw.pop("keep_torch_compile", None)
                return _orig(self, model, *a, **kw)
            accelerate.Accelerator.unwrap_model = _shim
            print("[shim] unwrap_model patched for Accelerate.")
    except Exception as e:
        print("[shim] Could not apply accelerate shim:", e)
        traceback.print_exc()

accelerate_compat_shim()

# ============================================================
# CONFIGURATION
# ============================================================
DATA_DIR   = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set"
BASE_MODEL = "Qwen/Qwen1.5-1.5B-Instruct"
OUTPUT_DIR = "/workspace/data/KZ_2117574/qwen15b_lora_multilingual"

NUM_EPOCHS   = 1
SAVE_STEPS   = 1000
BATCH_SIZE   = 1
GRAD_ACCUM   = 16
LR           = 2e-4
EVAL_STEPS   = 1000
MAX_LEN_FALLBACK = 8192   # Qwen supports 8k
SEED = 42
DEBUG_PREVIEW = True   # ✅ preview a few examples before training

# ============================================================
# UTILITIES
# ============================================================
def read_jsonl(p):
    rows = []
    with open(p, encoding="utf-8") as f:
        for l in f:
            if l.strip():
                try:
                    rows.append(json.loads(l))
                except:
                    pass
    return rows

# ============================================================
# DATA PREPARATION (with full safety)
# ============================================================
def make_examples(root):
    examples = []
    for lang in os.listdir(root):
        lang_path = os.path.join(root, lang)
        if not os.path.isdir(lang_path):
            continue

        dlg_dir = os.path.join(lang_path, "Dialogues")
        sum_dir = os.path.join(lang_path, "Summary_Text")
        qna_dir = os.path.join(lang_path, "QnA")

        # --- Summarization ---
        if os.path.isdir(dlg_dir) and os.path.isdir(sum_dir):
            for fn in os.listdir(dlg_dir):
                if not fn.endswith(".jsonl"):
                    continue
                dlg_path = os.path.join(dlg_dir, fn)
                dialogues = []

                for x in read_jsonl(dlg_path):
                    try:
                        val = x.get("dialogue", "") if isinstance(x, dict) else str(x)
                        if isinstance(val, list):
                            val = " ".join(map(str, val))
                        dialogues.append(str(val))
                    except Exception:
                        continue

                dialogue_text = "\n".join(map(str, dialogues)).strip()
                if not dialogue_text:
                    continue

                sum_path = os.path.join(sum_dir, fn.replace(".jsonl", "_summary.txt"))
                if os.path.exists(sum_path):
                    try:
                        with open(sum_path, "r", encoding="utf-8") as f:
                            summary = f.read().strip()
                        if summary:
                            prompt = (
                                "Summarize the following doctor–patient dialogue in English:\n"
                                f"{str(dialogue_text)}\nSummary:"
                            )
                            examples.append({"text": prompt, "labels": str(summary)})
                    except Exception:
                        continue

        # --- QnA ---
        if os.path.isdir(qna_dir):
            for fn in os.listdir(qna_dir):
                if not fn.endswith(".json"):
                    continue
                try:
                    with open(os.path.join(qna_dir, fn), "r", encoding="utf-8") as f:
                        data = json.load(f)
                except Exception:
                    continue
                if not isinstance(data, dict):
                    continue

                qs = data.get("questions", [])
                if not isinstance(qs, list):
                    continue

                for qa in qs:
                    if not isinstance(qa, dict):
                        continue
                    q = qa.get("question", "")
                    a = qa.get("answer", "")
                    if isinstance(q, list):
                        q = " ".join(map(str, q))
                    if isinstance(a, list):
                        a = " ".join(map(str, a))
                    if not q or not a:
                        continue
                    prompt = (
                        "Answer the following medical question clearly in the same language:\n"
                        f"Question: {str(q)}\nAnswer:"
                    )
                    examples.append({"text": prompt, "labels": str(a)})

    print(f"✅ Loaded {len(examples)} examples from {root}")
    if DEBUG_PREVIEW and len(examples) > 0:
        print("\n🔍 Preview example:")
        for k, v in examples[0].items():
            print(f"  {k}: {v[:250]}...\n")
    return examples

# ============================================================
# LOAD DATASETS
# ============================================================
train_data = make_examples(os.path.join(DATA_DIR, "train"))
dev_data   = make_examples(os.path.join(DATA_DIR, "dev"))

# ============================================================
# MODEL + TOKENIZER + LoRA CONFIG
# ============================================================
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

MAX_LEN = getattr(tokenizer, "model_max_length", MAX_LEN_FALLBACK)
if MAX_LEN > MAX_LEN_FALLBACK:
    MAX_LEN = MAX_LEN_FALLBACK
print(f"🧠 Using context length: {MAX_LEN}")

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    quantization_config=bnb_cfg
)

lora_cfg = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

# ============================================================
# TOKENIZATION (safe & context-balanced)
# ============================================================
def _keep_both(ids, maxlen):
    if len(ids) <= maxlen:
        return ids
    h = int(maxlen * 0.6)
    t = maxlen - h
    return ids[:h] + ids[-t:]

def build_ids(prompt, target, maxlen):
    p_ids = tokenizer.encode(prompt, add_special_tokens=False)
    t_ids = tokenizer.encode(target, add_special_tokens=False)
    if len(p_ids) + len(t_ids) > maxlen:
        p_ids = _keep_both(p_ids, int(maxlen * 0.7))
    if len(p_ids) + len(t_ids) > maxlen:
        t_ids = t_ids[: maxlen - len(p_ids)]
    ids = p_ids + t_ids
    labels = [-100] * len(p_ids) + t_ids[:]
    return {"input_ids": ids, "attention_mask": [1] * len(ids), "labels": labels}

def tok_fn(ex):
    out = {"input_ids": [], "attention_mask": [], "labels": []}
    for p, t in zip(ex["text"], ex["labels"]):
        d = build_ids(str(p), str(t), MAX_LEN)
        for k in out:
            out[k].append(d[k])
    return out

train_ds = Dataset.from_list(train_data).map(
    tok_fn, batched=True, num_proc=4, remove_columns=["text", "labels"]
)
dev_ds = Dataset.from_list(dev_data).map(
    tok_fn, batched=True, num_proc=4, remove_columns=["text", "labels"]
)

# ============================================================
# COLLATOR
# ============================================================
def collate(batch):
    maxlen = max(len(x["input_ids"]) for x in batch)
    pad = tokenizer.pad_token_id
    def pad_seq(x, filler): return x + [filler] * (maxlen - len(x))
    return {
        "input_ids": torch.tensor([pad_seq(x["input_ids"], pad) for x in batch]),
        "attention_mask": torch.tensor([pad_seq(x["attention_mask"], 0) for x in batch]),
        "labels": torch.tensor([pad_seq(x["labels"], -100) for x in batch]),
    }

# ============================================================
# TRAINING CONFIG
# ============================================================
from transformers import TrainerCallback
class SaveAdapter(TrainerCallback):
    def __init__(self, out_dir, steps=1000):
        self.out_dir = out_dir
        self.steps = steps
        os.makedirs(out_dir, exist_ok=True)
    def on_step_end(self, args, state, control, **kw):
        if state.global_step % self.steps == 0 and state.global_step > 0:
            ck = f"{self.out_dir}/adapter_step_{state.global_step}"
            kw["model"].save_pretrained(ck)
            print(f"[callback] Saved adapter checkpoint at {ck}")

args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    num_train_epochs=NUM_EPOCHS,
    learning_rate=LR,
    evaluation_strategy="steps",
    eval_steps=EVAL_STEPS,
    save_strategy="steps",
    save_steps=SAVE_STEPS,
    save_total_limit=3,
    fp16=True,
    gradient_checkpointing=True,
    optim="paged_adamw_8bit",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    logging_steps=50,
    report_to="none",
    dataloader_num_workers=4,
    seed=SEED,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    data_collator=collate,
    callbacks=[SaveAdapter(OUTPUT_DIR, SAVE_STEPS)],
)

# ============================================================
# TRAINING (with checkpoint resume)
# ============================================================
latest = None
if os.path.isdir(OUTPUT_DIR):
    ckpts = [
        os.path.join(OUTPUT_DIR, x)
        for x in os.listdir(OUTPUT_DIR)
        if x.startswith("checkpoint-")
    ]
    if ckpts:
        latest = sorted(ckpts, key=lambda x: int(x.split("-")[-1]))[-1]

if latest:
    print(f"🔄 Resuming from {latest}")
    trainer.train(resume_from_checkpoint=latest)
else:
    print("✨ Starting fresh run...")
    trainer.train()

trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("✅ Training complete. Model saved to:", OUTPUT_DIR)
