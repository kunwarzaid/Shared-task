&A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:
- tokenization_indictrans.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.

'A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:
- configuration_indictrans.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.

2`torch_dtype` is deprecated! Use `dtype` instead!

"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-en-1B:
- modeling_indictrans.py
. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.

sThe model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.

([18:49:11] Loading Qwen-2.5-7B-Instruct

Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]
Fetching 4 files:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [02:00<06:01, 120.55s/it]
Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:00<00:00, 30.14s/it] 

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:09,  3.01s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:06<00:07,  3.55s/it]
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.76s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.38s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.44s/it]

hTraceback (most recent call last):
  File "/workspace/multi_summ/indictrans2.py", line 333, in <module>

\    run_pipeline()
  File "/workspace/multi_summ/indictrans2.py", line 261, in run_pipeline

q    print(translate_to_english(
  File "/workspace/multi_summ/indictrans2.py", line 145, in translate_to_english

ï¿½    inputs = tok(
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py", line 3073, in __call__

ï¿½    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py", line 3183, in _call_one

ï¿½    return self.encode_plus(
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py", line 3258, in encode_plus

ï¿½    return self._encode_plus(
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py", line 800, in _encode_plus

ï¿½    first_ids = get_input_ids(text)
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py", line 767, in get_input_ids

ï¿½    tokens = self.tokenize(text, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py", line 697, in tokenize

    tokenized_text.extend(self._tokenize(token))
  File "/root/.cache/huggingface/modules/transformers_modules/ai4bharat/indictrans2_hyphen_indic_hyphen_en_hyphen_1B/ac3daf0ecd37be3b6957764a9179ab2b07fa9d6a/tokenization_indictrans.py", line 201, in _src_tokenize

ï¿½    assert src_lang in LANGUAGE_TAGS, f"Invalid source language tag: {src_lang}"
AssertionError: Invalid source language tag: <mr>

removing /jobs/285371


Code
import os
import json
import time
import random
from pathlib import Path
from tqdm import tqdm

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    BitsAndBytesConfig
)
from langdetect import DetectorFactory
from huggingface_hub import login

# ============================================================
# LOGIN (OPTIONAL â€“ REQUIRED ONLY FOR GATED MODELS)
# ============================================================

def huggingface_login():
    token = ""   # <-- put token here if required
    if token:
        login(token=token)
        print("Successfully logged into Hugging Face!")

huggingface_login()

# ============================================================
# GLOBAL SETTINGS
# ============================================================

DetectorFactory.seed = 42
torch.set_grad_enabled(False)

print("Torch:", torch.__version__)
print("CUDA:", torch.version.cuda)

# ============================================================
# CONFIGURATION
# ============================================================

INDICTRANS_MODEL = "ai4bharat/indictrans2-indic-en-1B"
QWEN_MODEL = "Qwen/Qwen2.5-7B-Instruct"

USE_4BIT = True

TEST_DIR = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/test"
TRAIN_SPLIT_DIR = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/train_split"
OUTPUT_DIR = "/workspace/data/KZ_2117574/EACL/IndicTrans2_Qwen_pipeline"

TARGET_LANGS = ["Hindi", "Marathi", "Bangla"]

LANG_CODE_MAP = {
    "Hindi": "hi",
    "Marathi": "mr",
    "Bangla": "bn"
}

# ========== ZERO vs FEW SHOT ==========
FEW_SHOT_K = 0        # 0 = zero-shot, >0 = few-shot
FEW_SHOT_SEED = 42

# ========== LIMITS ==========
MAX_TEST_EXAMPLES = 100
MAX_NATIVE_CHARS = 8000
TARGET_DIALOGUE_TOKENS = 2048
EXAMPLE_DIALOGUE_TOKENS = 512
MAX_NEW_TOKENS = 512

SYSTEM_PROMPT = (
    "You are a clinical summarization assistant. "
    "Summarize the doctorâ€“patient dialogue faithfully. "
    "Do not hallucinate. End with <<END>>."
)

# ============================================================
# UTILS
# ============================================================

def tprint(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def safe_read_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            try:
                rows.append(json.loads(line))
            except:
                continue
    return rows

def write_text(path, text):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text.strip() + "\n")

def clip_chars(text, max_chars):
    return text if len(text) <= max_chars else text[-max_chars:]

def clip_tokens(tok, text, max_tokens):
    ids = tok.encode(text, add_special_tokens=False)
    if len(ids) <= max_tokens:
        return text
    return tok.decode(ids[-max_tokens:], skip_special_tokens=True)

def build_messages(system_prompt, user_prompt):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

# ============================================================
# INDIC TRANS2 (FIXED)
# ============================================================

def load_indictrans():
    tprint("Loading IndicTrans2")
    tok = AutoTokenizer.from_pretrained(
        INDICTRANS_MODEL,
        trust_remote_code=True
    )
    model = AutoModelForSeq2SeqLM.from_pretrained(
        INDICTRANS_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    model.eval()
    return model, tok

def translate_to_english(model, tok, text, src_lang):
    """
    IndicTrans2 REQUIRES explicit language token in the text.
    """

    # ðŸ”´ CRITICAL: prepend language tag
    text = f"<{src_lang}> {text}"

    tok.src_lang = src_lang
    tok.tgt_lang = "en"

    inputs = tok(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=2048
    ).to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=512,
            num_beams=4,
            early_stopping=True
        )

    return tok.decode(out[0], skip_special_tokens=True)

# ============================================================
# QWEN SUMMARIZER (FIXED)
# ============================================================

def load_qwen():
    tprint("Loading Qwen-2.5-7B-Instruct")

    tok = AutoTokenizer.from_pretrained(QWEN_MODEL, use_fast=True)
    tok.pad_token = tok.eos_token

    quant_cfg = BitsAndBytesConfig(
        load_in_4bit=USE_4BIT,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    model = AutoModelForCausalLM.from_pretrained(
        QWEN_MODEL,
        device_map="auto",
        quantization_config=quant_cfg,
        torch_dtype=torch.float16,
        trust_remote_code=True
    )

    # CRITICAL FIX
    model.tie_weights()
    model.eval()

    return model, tok

def summarize_qwen(model, tok, messages):
    prompt = tok.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tok([prompt], return_tensors="pt").to(model.device)

    with torch.no_grad():
        out = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            pad_token_id=tok.eos_token_id
        )

    gen = out[0][len(inputs.input_ids[0]):]
    return tok.decode(gen, skip_special_tokens=True)

# ============================================================
# FEW-SHOT EXAMPLES
# ============================================================

def collect_few_shot(train_root, lang, k, seed):
    if k == 0:
        return []

    rng = random.Random(seed)
    dlg_dir = Path(train_root) / lang / "Dialogues"
    sum_dir = Path(train_root) / lang / "Summary_Text"

    if not dlg_dir.exists():
        return []

    files = list(dlg_dir.glob("*.jsonl"))
    rng.shuffle(files)

    examples = []
    for f in files:
        sf = sum_dir / f"{f.stem}_summary.txt"
        if not sf.exists():
            continue

        dialogue = " ".join(
            r.get("dialogue", "")
            for r in safe_read_jsonl(f)
        ).strip()

        summary = sf.read_text(encoding="utf-8").replace("<<END>>", "").strip()

        if dialogue and summary:
            examples.append((dialogue, summary))

        if len(examples) == k:
            break

    return examples

# ============================================================
# MAIN PIPELINE
# ============================================================


    
def run_pipeline():
    trans_model, trans_tok = load_indictrans()
    qwen_model, qwen_tok = load_qwen()

    print(translate_to_english(
    trans_model,
    trans_tok,
    "à¤†à¤ªà¤²à¤¾ à¤˜à¤¸à¤¾ à¤¦à¥à¤–à¤¤à¥‹ à¤†à¤¹à¥‡",
    "mr"))
    
    for lang_dir in Path(TEST_DIR).iterdir():
        if lang_dir.name not in TARGET_LANGS:
            continue

        tprint(f"Processing language: {lang_dir.name}")
        src_lang = LANG_CODE_MAP[lang_dir.name]

        dlg_dir = lang_dir / "Dialogues"
        out_dir = Path(OUTPUT_DIR) / (
            "few_shot" if FEW_SHOT_K > 0 else "zero_shot"
        ) / lang_dir.name / "Summary_Text"

        files = sorted(dlg_dir.glob("*.jsonl"))[:MAX_TEST_EXAMPLES]

        few_shot_examples = collect_few_shot(
            TRAIN_SPLIT_DIR, lang_dir.name, FEW_SHOT_K, FEW_SHOT_SEED
        )

        for f in tqdm(files, desc=lang_dir.name):
            out_file = out_dir / f"{f.stem}_summary.txt"
            if out_file.exists():
                continue

            dialogue_native = " ".join(
                r.get("dialogue", "")
                for r in safe_read_jsonl(f)
            )

            dialogue_native = clip_chars(dialogue_native, MAX_NATIVE_CHARS)

            dialogue_en = translate_to_english(
                trans_model, trans_tok, dialogue_native, src_lang
            )

            dialogue_en = clip_tokens(
                qwen_tok, dialogue_en, TARGET_DIALOGUE_TOKENS
            )

            prefix = ""
            for i, (d, s) in enumerate(few_shot_examples, 1):
                d = clip_tokens(qwen_tok, d, EXAMPLE_DIALOGUE_TOKENS)
                prefix += f"Example {i}:\nDialogue:\n{d}\n\nSummary:\n{s}\n---\n"

            user_prompt = (
                f"{prefix}\nDialogue:\n{dialogue_en}\n\n"
                "Write a clinical English summary and end with <<END>>."
            )

            summary_raw = summarize_qwen(
                qwen_model,
                qwen_tok,
                build_messages(SYSTEM_PROMPT, user_prompt)
            )

            summary = summary_raw.split("<<END>>")[0].strip()
            write_text(out_file, summary)

        torch.cuda.empty_cache()

    tprint("Pipeline complete.")

# ============================================================
# ENTRY
# ============================================================

if __name__ == "__main__":
    run_pipeline()
