import os, json, time
from pathlib import Path
from tqdm import tqdm

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langdetect import detect, DetectorFactory

# ============================================================
# SETUP
# ============================================================

DetectorFactory.seed = 42
torch.set_grad_enabled(False)

# ============================================================
# CONFIG
# ============================================================

BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"

TEST_DIR = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/test"
OUTPUT_DIR = "/workspace/data/KZ_2117574/EACL/Qwen_2.5_7B_Instruct_split/zeroshot"

TARGET_LANGS = ["Bangla", "English", "Hindi", "Marathi"]

# Zero-shot is defined by FEW_SHOT_K = 0
FEW_SHOT_K = 0

USE_4BIT = True

# Practical full context for Qwen 2.5 7B on 32GB GPU
MAX_TOTAL_INPUT_TOKENS = 16384

# You decided to use this
MAX_NEW_TOKENS_SUMMARY = 1024

# IMPORTANT: Plain text only (NO special tokens)
SYSTEM_SUMMARY = (
    "You are a clinical summarization assistant. "
    "Read a doctorâ€“patient dialogue and write a fluent English clinical summary. "
    "Focus on symptoms, diagnosis, investigations, management plan, "
    "supportive care, and follow-up. "
    "Do not hallucinate new diagnoses or tests. "
    "End your summary with the token <<END>>."
)

# ============================================================
# UTILS
# ============================================================

def tprint(msg: str):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def safe_read_jsonl(path: Path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            try:
                rows.append(json.loads(line.strip()))
            except Exception:
                continue
    return rows

def write_text(path: Path, text: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(text.strip() + "\n", encoding="utf-8")

def clip_tokens(tokenizer, text: str, max_tokens: int) -> str:
    ids = tokenizer.encode(text, add_special_tokens=False)
    if len(ids) <= max_tokens:
        return text
    return tokenizer.decode(ids[-max_tokens:], skip_special_tokens=True)

def build_messages(system_prompt: str, user_prompt: str):
    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

def chat_generate(model, tokenizer, messages):
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS_SUMMARY,
            do_sample=False,
            num_beams=1,
            pad_token_id=tokenizer.eos_token_id
        )

    gen_ids = output[0][inputs.input_ids.shape[-1]:]
    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

# ============================================================
# MODEL LOADING
# ============================================================

def load_model():
    tprint("Loading tokenizer and model")

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
    tokenizer.pad_token = tokenizer.eos_token

    quant_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        quantization_config=quant_cfg,
        dtype=torch.float16
    )

    model.eval()
    return model, tokenizer

# ============================================================
# MAIN PIPELINE
# ============================================================

def run_summary_only():
    model, tokenizer = load_model()

    langs = [
        p for p in Path(TEST_DIR).iterdir()
        if p.is_dir() and p.name in TARGET_LANGS
    ]

    for lang_dir in langs:
        lang = lang_dir.name
        tprint(f"Processing language: {lang}")

        dlg_dir = lang_dir / "Dialogues"
        out_dir = Path(OUTPUT_DIR) / lang / "Summary_Text"

        files = sorted(dlg_dir.glob("*.jsonl"))[:100]

        for f in tqdm(files, desc=f"{lang}"):
            out_file = out_dir / f"{f.stem}_summary.txt"
            if out_file.exists():
                continue

            rows = safe_read_jsonl(f)
            dialogue = " ".join(
                r.get("dialogue", "") if isinstance(r, dict) else str(r)
                for r in rows
            )

            # Dialogue-first budgeting
            dialogue_clip = clip_tokens(
                tokenizer,
                dialogue,
                MAX_TOTAL_INPUT_TOKENS - MAX_NEW_TOKENS_SUMMARY - 512
            )

            user_prompt = (
                f"Dialogue:\n{dialogue_clip}\n\n"
                "Write an English clinical summary and end with <<END>>."
            )

            messages = build_messages(SYSTEM_SUMMARY, user_prompt)
            summary_raw = chat_generate(model, tokenizer, messages)

            end_pos = summary_raw.find("<<END>>")
            summary = summary_raw[:end_pos].strip() if end_pos != -1 else summary_raw.strip()

            # Safety check: enforce English
            if detect(summary) != "en":
                messages = build_messages(
                    SYSTEM_SUMMARY,
                    f"Dialogue:\n{dialogue_clip}\n\n"
                    "Write ONLY an English clinical summary. End with <<END>>."
                )
                summary_raw = chat_generate(model, tokenizer, messages)
                end_pos = summary_raw.find("<<END>>")
                summary = summary_raw[:end_pos].strip() if end_pos != -1 else summary_raw.strip()

            write_text(out_file, summary)

        torch.cuda.empty_cache()

    tprint("Inference complete.")

# ============================================================
# ENTRY POINT
# ============================================================

if __name__ == "__main__":
    run_summary_only()
