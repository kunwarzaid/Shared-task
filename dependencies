AI Safety for Everyone
Reference: https://arxiv.org/pdf/2502.09288
What is AI Safety for Everyone

The paper argues that current debate about â€œAI safetyâ€ is too narrowly focused on existential risks from future, very-advanced AI systems. 
arXiv
+1

Through a systematic review of peer-reviewed literature (383 papers), the authors show that there is already a large body of concrete safety research addressing immediate and practical issues in existing AI systems. 
arXiv
+1

They propose an â€œepistemically inclusive and pluralisticâ€ conception of AI safety â€” one that recognizes not just long-term existential risk, but also present-day harms, and allows researchers from different backgrounds (engineering, ethics, ML, governance) to meaningfully contribute. 
arXiv
+1

ğŸ” What the Review Found â€” Key Risk Types & Safety Work

The literature covers a wide range of safety concerns across the AI lifecycle: design, development, deployment, operation, and even decommissioning. 
arXiv

Key themes and topics include:

Robustness & Reliability: Ensuring AI models (classifiers, neural nets, etc.) work reliably under noisy data, out-of-distribution inputs, adversarial perturbations. 
arXiv
+1

Safe Reinforcement Learning / Safe Control: For agent-based systems, research considers control under uncertainty, stability, constraint-satisfaction, and safe behavior under dynamic/uncertain environments. 
arXiv

Adversarial Attacks & Defenses: Work on adversarial robustness, defenses against malicious inputs or perturbations, security-oriented safety. 
arXiv
+1

Human & Societal Risks: Issues of trust, accountability, fairness, misuse â€” treating AI safety as part of broader technological and social safety, not just technical â€œbugs.â€ 
arXiv
+1

Bridging Short-Term and Long-Term Safety: Many safety problems (e.g. robustness, misuse, reliability) persist across time horizons; treating them separately (as â€œnear-term vs long-termâ€) may be misleading. 
arXiv

The authors show that these are not fringe problems â€” they constitute a core part of AI safety research today. 
Semantic Scholar
+1

âœ… Why This Matters â€” Implications of â€œAI Safety for Everyoneâ€

Broadens the definition of AI safety: Safety isnâ€™t only about â€œfuture superintelligent AI,â€ but also about making current systems safer, fairer, reliable, and trustworthy.

Inclusivity for researchers: By defining safety broadly, researchers working on adversarial robustness, fairness, reliability, or governance get recognized as doing â€œreal AI safety.â€ This encourages cross-disciplinary involvement.

Practical relevance now: Many applications â€” healthcare, finance, autonomous systems â€” deploy AI today. So focusing only on speculative future risks ignores real, existing harms or failures.

Better alignment with established safety engineering traditions: The paper argues that AI safety should reconnect with the larger history of technological and systems safety (physical engineering, software systems, etc.), and benefit from existing methods, practices, and governance models. 
arXiv
+1

Guide for policy, regulation, and governance: A broader view helps create regulation, certification, and governance frameworks that deal with real risks now â€” not just remote existential scenarios.
