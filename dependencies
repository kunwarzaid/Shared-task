import os, json, sys, inspect, traceback, torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# ============================================================
# ACCELERATE COMPATIBILITY SHIM
# ============================================================
def accelerate_compat_shim():
    try:
        import accelerate
        sig = inspect.signature(accelerate.Accelerator.unwrap_model)
        if "keep_torch_compile" not in sig.parameters:
            _orig = accelerate.Accelerator.unwrap_model
            def _shim(self, model, *a, **kw):
                kw.pop("keep_torch_compile", None)
                return _orig(self, model, *a, **kw)
            accelerate.Accelerator.unwrap_model = _shim
            print("[shim] unwrap_model patched for Accelerate.")
    except Exception as e:
        print("[shim] Could not apply accelerate shim:", e)
        traceback.print_exc()
accelerate_compat_shim()

# ============================================================
# CONFIG
# ============================================================
DATA_DIR   = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set"
BASE_MODEL = "Qwen/Qwen1.5-1.5B-Instruct"
OUTPUT_DIR = "/workspace/data/KZ_2117574/qwen15b_lora_multilingual"

NUM_EPOCHS   = 1
SAVE_STEPS   = 1000
BATCH_SIZE   = 1
GRAD_ACCUM   = 16
LR           = 2e-4
EVAL_STEPS   = 1000
MAX_LEN_FALLBACK = 8192   # Qwen supports 8k
SEED = 42

# ============================================================
# DATA HELPERS
# ============================================================
def read_jsonl(p):
    rows=[]
    with open(p,encoding="utf-8") as f:
        for l in f:
            if l.strip():
                try: rows.append(json.loads(l))
                except: pass
    return rows

def make_examples(root):
    ex=[]
    for lang in os.listdir(root):
        lang_dir=os.path.join(root,lang)
        if not os.path.isdir(lang_dir): continue
        dlg=os.path.join(lang_dir,"Dialogues")
        summ=os.path.join(lang_dir,"Summary_Text")
        qna=os.path.join(lang_dir,"QnA")

        # summaries
        if os.path.isdir(dlg) and os.path.isdir(summ):
            for fn in os.listdir(dlg):
                if not fn.endswith(".jsonl"): continue
                dlg_path=os.path.join(dlg,fn)
                lines=[(x.get("dialogue","") if isinstance(x,dict) else str(x))
                       for x in read_jsonl(dlg_path)]
                txt="\n".join(lines).strip()
                sum_path=os.path.join(summ,fn.replace(".jsonl","_summary.txt"))
                if txt and os.path.exists(sum_path):
                    with open(sum_path,encoding="utf-8") as f: summ_txt=f.read().strip()
                    if summ_txt:
                        prompt=f"Summarize the following doctorâ€“patient dialogue in English:\n{txt}\nSummary:"
                        ex.append({"text":prompt,"labels":summ_txt})
        # qna
        if os.path.isdir(qna):
            for fn in os.listdir(qna):
                if not fn.endswith(".json"): continue
                with open(os.path.join(qna,fn),encoding="utf-8") as f:
                    data=json.load(f)
                for qa in data.get("questions",[]):
                    q,a=qa.get("question",""),qa.get("answer","")
                    if q and a:
                        prompt=f"Answer the following medical question clearly in the same language:\nQuestion: {q}\nAnswer:"
                        ex.append({"text":prompt,"labels":a})
    print(f"âœ… Loaded {len(ex)} examples from {root}")
    return ex

train_data=make_examples(os.path.join(DATA_DIR,"train"))
dev_data  =make_examples(os.path.join(DATA_DIR,"dev"))

# ============================================================
# TOKENIZER + MODEL
# ============================================================
bnb_cfg=BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
tokenizer=AutoTokenizer.from_pretrained(BASE_MODEL,use_fast=True)
if tokenizer.pad_token is None: tokenizer.pad_token=tokenizer.eos_token
MAX_LEN=getattr(tokenizer,"model_max_length",MAX_LEN_FALLBACK)
if MAX_LEN>MAX_LEN_FALLBACK: MAX_LEN=MAX_LEN_FALLBACK
print(f"ðŸ§  Context length: {MAX_LEN}")

model=AutoModelForCausalLM.from_pretrained(BASE_MODEL,
        device_map="auto",quantization_config=bnb_cfg)

lora_cfg=LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj","v_proj","k_proj","o_proj"],
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)
model=get_peft_model(model,lora_cfg)
model.print_trainable_parameters()

# ============================================================
# TOKENIZATION (keeps both ends if long)
# ============================================================
def _keep_both(ids,maxlen):
    if len(ids)<=maxlen: return ids
    h=int(maxlen*0.6); t=maxlen-h
    return ids[:h]+ids[-t:]

def build_ids(prompt,target,maxlen):
    p_ids=tokenizer.encode(prompt,add_special_tokens=False)
    t_ids=tokenizer.encode(target,add_special_tokens=False)
    if len(p_ids)+len(t_ids)>maxlen:
        p_ids=_keep_both(p_ids,int(maxlen*0.7))
    if len(p_ids)+len(t_ids)>maxlen:
        t_ids=t_ids[:maxlen-len(p_ids)]
    ids=p_ids+t_ids
    labels=[-100]*len(p_ids)+t_ids[:]
    return {"input_ids":ids,"attention_mask":[1]*len(ids),"labels":labels}

def tok_fn(ex):
    out={"input_ids":[],"attention_mask":[],"labels":[]}
    for p,t in zip(ex["text"],ex["labels"]):
        d=build_ids(str(p),str(t),MAX_LEN)
        for k in out: out[k].append(d[k])
    return out

train_ds=Dataset.from_list(train_data).map(tok_fn,batched=True,num_proc=4,remove_columns=["text","labels"])
dev_ds  =Dataset.from_list(dev_data).map(tok_fn,batched=True,num_proc=4,remove_columns=["text","labels"])

# ============================================================
# COLLATOR
# ============================================================
def collate(batch):
    maxlen=max(len(x["input_ids"]) for x in batch)
    pad=tokenizer.pad_token_id
    def pad_seq(x,filler): return x+[filler]*(maxlen-len(x))
    return {
        "input_ids":torch.tensor([pad_seq(x["input_ids"],pad) for x in batch]),
        "attention_mask":torch.tensor([pad_seq(x["attention_mask"],0) for x in batch]),
        "labels":torch.tensor([pad_seq(x["labels"],-100) for x in batch])
    }

# ============================================================
# TRAINING ARGS
# ============================================================
from transformers import TrainerCallback
class SaveAdapter(TrainerCallback):
    def __init__(self,d,steps=1000): self.d,self.s=d,steps; os.makedirs(d,exist_ok=True)
    def on_step_end(self,args,state,control,**kw):
        if state.global_step%self.s==0 and state.global_step>0:
            ck=f"{self.d}/adapter_step_{state.global_step}"
            kw["model"].save_pretrained(ck)
            print(f"[callback] saved adapter {ck}")

args=TrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    num_train_epochs=NUM_EPOCHS,
    learning_rate=LR,
    evaluation_strategy="steps",
    eval_steps=EVAL_STEPS,
    save_strategy="steps",
    save_steps=SAVE_STEPS,
    save_total_limit=3,
    fp16=True,
    gradient_checkpointing=True,
    optim="paged_adamw_8bit",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    logging_steps=50,
    report_to="none",
    dataloader_num_workers=4,
    seed=SEED
)

trainer=Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    data_collator=collate,
    callbacks=[SaveAdapter(OUTPUT_DIR,SAVE_STEPS)]
)

# ============================================================
# RESUME LOGIC
# ============================================================
latest=None
if os.path.isdir(OUTPUT_DIR):
    ck=[os.path.join(OUTPUT_DIR,x) for x in os.listdir(OUTPUT_DIR)
        if x.startswith("checkpoint-")]
    if ck:
        latest=sorted(ck,key=lambda x:int(x.split("-")[-1]))[-1]
if latest:
    print(f"ðŸ”„ Resuming from {latest}")
    trainer.train(resume_from_checkpoint=latest)
else:
    print("âœ¨ Starting fresh run")
    trainer.train()

trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("âœ… Training complete, model saved to",OUTPUT_DIR)
