• Medical LLMs achieve near-clinician accuracy, but their reasoning is opaque and unverifiable  
• Clinicians must trust *how* a diagnosis was reached, not just the final answer  
• LLMs often show unjustified confidence, inconsistent explanations, and hallucinated certainty  
• Small prompt changes can yield different diagnoses, revealing unstable reasoning  
• Existing safety systems moderate outputs but do not expose the internal reasoning process  
• In medicine, hidden errors are more dangerous than visible uncertainty  
