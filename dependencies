%File: anonymous-submission-latex-2026.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai2026}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,fit,shapes}
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{booktabs}
%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\usepackage{amsmath} 
\usepackage[htt]{hyphenat} % allows hyphenation in \texttt
\hyphenchar\font=`\-        % allow breaks at hyphens

\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2026.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai2026.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{Trust-X: Towards Transparent and Safe Multi-Agent Reasoning}
\author{
    %Authors
    % All authors must be in the same font size and format.
    Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    AAAI Style Contributions by Pater Patel Schneider,
    Sunil Issar,\\
    J. Scott Penberthy,
    George Ferguson,
    Hans Guesgen,
    Francisco Cruz\equalcontrib,
    Marc Pujol-Gonzalez\equalcontrib
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}Association for the Advancement of Artificial Intelligence\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    1101 Pennsylvania Ave, NW Suite 300\\
    Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
\usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) have achieved strong performance on tasks such as clinical summarization, question-answering, and clinical diagnosis on medical reasoning benchmarks such as \textit{MedQA}, yet their deployment remains constrained by opacity and unverifiable decision paths. We introduce \textbf{Trust-X}, a framework that embeds explainability, consensus reasoning, real-time safety, and evaluates how transparently  and responsibly language models reason through diagnostic problems. Trust-X integrates (i) multiple agents that reason independently to quantify epistemic uncertainty, (ii) safety agents that monitor and intercept unsafe prescriptions or tests, and (iii) quantitative trust indices linking reasoning consistency and operational safety. Across 50 clinical scenarios, Trust-X maintained stable diagnostic accuracy $(\approx68\%)$ while revealing that models can appear correct yet reason unreliably. The study reveals that, systems with active safety agents register more alerts—not because they are less trustworthy, but because they engage in visible oversight. These findings demonstrate that reliability in clinical AI emerges from transparency and accountability rather than accuracy alone.
\end{abstract}




\textbf{Keywords:} Large Language Models, Explainable AI, Trustworthy AI, Medical Diagnosis, Multi-Agent Systems, Clinical Reasoning.

\section{Introduction}

Large Language Models (LLMs) such as GPT-4 \cite{achiam2023gpt}, Gemini \cite{team2023gemini}, and Med-PaLM 2 \cite{singhal2025toward} have demonstrated remarkable progress in natural language reasoning and generalization across diverse domains, including medicine. They now perform at near-clinician levels on medical question-answering benchmarks \cite{kung2023performance,nori2023capabilities}, suggesting potential applications in decision support, documentation, and triage \cite{lee2023benefits}. Yet, the very properties that make LLMs powerful—scale, open-endedness, and linguistic fluency—also render them unreliable in domains requiring factual precision, causal reasoning, and accountability \cite{ji2023survey,begoli2019need}.

Integrating LLMs into healthcare introduces challenges that extend beyond accuracy. Medicine demands not only correct predictions but also \emph{explainable and justifiable} reasoning. Clinicians reason through causality, uncertainty, and evidence weighting—capabilities that current LLMs only partially emulate. When an AI system proposes a diagnosis or prescription, its credibility depends as much on \emph{why} it reached that conclusion as on \emph{what} it predicts \cite{tonekaboni2019clinicians,amann2020explainability}. Without transparent reasoning, even correct answers may be unsafe, as the underlying rationale could be spurious or unverifiable.

Despite improvements in alignment and red-teaming \cite{mei2023assert}, clinical LLMs still struggle with trustworthiness—a composite property encompassing accuracy, consistency, safety, and interpretability \cite{a2019,huang2025survey}. Models frequently display unjustified confidence \cite{kadavath2022language}, produce inconsistent explanations, or fail to acknowledge uncertainty in ambiguous cases. This epistemic overconfidence poses a critical risk in medicine, where admitting uncertainty can be safer than being confidently wrong. While systems such as Med-PaLM 2 \cite{singhal2025toward}, BioGPT \cite{luo2022biogpt}, and ChatDoctor \cite{li2023chatdoctor} exhibit strong factual accuracy, they often lack reasoning stability—small prompt changes can yield entirely different diagnoses. Such instability parallels findings in calibration and abstention research, where uncalibrated confidence undermines clinical reliability \cite{guo2017calibration,malinin2020uncertainty}.


Prior works such as Med-Guard \cite{jain2025medguard}, addressed safety but not transparency, their reasoning process remains opaque: clinicians could not inspect the model’s thought process, track evolving hypotheses, or understand why specific conclusions were reached.

These limitations motivated \textbf{Trust-X}, a to design \emph{trust-centered explainability}. 
Trust-X includes four main features:
\begin{itemize}
    \item \textbf{Consensus Reasoning:} Several Doctor Agents analyze each case independently and vote on the best diagnosis. Their disagreement, measured by the \textit{Consensus Disagreement Rate (CDR)}, shows uncertainty.
    \item \textbf{Reasoning Trace:} Each diagnostic step includes a clear trace linking evidence, reasoning, and results, showing how thoughts evolve.
    \item \textbf{End-to-End Logging:} Every agent action is recorded with time, role, and reason, allowing full replay and accountability.
    \item \textbf{Trust Scoring:} A trust layer combines interpretability, safety, and reasoning quality into measurable trust scores.
\end{itemize}

Together, these parts make reasoning visible and testable. Logging turns black-box behavior into traceable steps; consensus captures uncertainty; and trust scores connect reasoning quality with safety.
Rather than claiming to make LLMs trustworthy, Trust-X \emph{reveals and measures} where trust emerges—and where it fails—establishing trustworthiness as a property of the reasoning process, not merely the outcome.


\section{Related Work}

The pursuit of explainability in artificial intelligence has long sought to bridge algorithmic performance with human interpretability. Early work on Explainable AI (XAI) employed feature attribution and surrogate modeling—e.g., LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Grad-CAM \cite{selvaraju2017grad}—to visualize which features influenced predictions. While effective for static models, such methods are ill-suited for domains like medicine, where decisions evolve through dialogue and evidence accumulation. As Holzinger et al. \cite{holzinger2019causability} and Ahmad et al. \cite{ahmad2018interpretable} argue, medical explainability requires \emph{causability}: a human-understandable mapping between evidence, inference, and outcome.

The advent of LLMs has redefined explainability through reasoning traces and self-reflection. Chain-of-thought prompting \cite{wei2022chain}, self-consistency \cite{wang2022self}, and reflection-based reasoning \cite{shinn2023reflexion} externalize a model’s deliberations as text. However, these traces are self-generated and often post-hoc rationalizations rather than genuine reasoning \cite{turpin2023language}. Models frequently express confident but incorrect rationales \cite{kadavath2022language,ji2023survey}, resulting in what clinicians might call “hallucinated certainty.” Thus, transparency alone does not guarantee truthfulness of thought.

In medicine, explainability and safety are inseparable. Tonekaboni et al. \cite{tonekaboni2019clinicians} and Amann et al. \cite{amann2020explainability} highlight that clinicians evaluate models by their reasoning legitimacy—whether conclusions align with medical logic. While LLM-based medical systems like BioGPT \cite{luo2022biogpt}, Med-PaLM 2 \cite{singhal2025toward}, and ChatDoctor \cite{li2023chatdoctor} demonstrate strong factual performance, they remain opaque in diagnostic reasoning. Clinical-Camel \cite{toma2023clinical} and similar systems introduced interactive consultations but still operate as single-agent frameworks without explicit uncertainty estimation or differential tracking.

Multi-agent systems have begun addressing these gaps. AgentClinic \cite{schmidgall2024agentclinic} modeled doctor–patient interaction as a cooperative dialogue between reasoning and data agents, improving conversational coherence. Yet, accountability remains limited—agents exchange information but do not produce verifiable reasoning records. Similarly, recent safety frameworks like GuardMed and SafetyBench integrate oversight mechanisms but focus on output moderation rather than process transparency.

Regulatory frameworks from the World Health Organization \cite{guidance2021ethics} and the European Commission \cite{bomhard2021regulation} emphasize traceability and auditability as cornerstones of trustworthy medical AI. However, most current LLM pipelines remain black boxes during inference, offering no visibility into evolving reasoning states—hindering reproducibility, fairness audits, and clinician trust \cite{begoli2019need,doshi2017towards}.

\textbf{Trust-X} builds on these efforts by making explainability part of the reasoning process. It records reasoning steps, agent interactions, and consensus decisions in real time. Each exchange is logged with context, creating a full trail that clinicians can review. Unlike post-hoc methods, Trust-X captures how evidence leads to conclusions, helping clinicians inspect, question, and verify the model’s thought process.

\section{Methodology: The Trust-X Framework}
 \textbf{Trust-X} is designed to make model reasoning easier to inspect and evaluate. 
It represents diagnostic reasoning as an interaction among specialized agents whose decisions can be traced, reviewed, and understood by humans.


\begin{figure*}[t]
\centering
\includegraphics[width=0.8\textwidth]{Doctor Agent (1).png}
\caption{
Overview of the \textbf{Trust-X} architecture. Multiple Doctor Agents perform independent reasoning over patient data under a Consensus Module that quantifies diagnostic disagreement (CDR). Safety agents provide real-time oversight of tests and prescriptions, while an Explainability and Logging Layer records reasoning traces, and safety flags. Logged evidence is analyzed by the Trust Metric Engine to compute epistemic (ETI), operational (OSI), and final (FTI) trust indices.
}
\label{fig:logging-architecture}
\end{figure*}
This design targets two recurring problems in clinical AI: (i) the \textit{limited visibility into intermediate reasoning steps}, and (ii) the absence of a clear way to measure uncertainty.
By coordinating multiple reasoning agents and tracking their interactions, Trust-X makes these aspects observable and measurable.

\subsection{Multi-Agent Diagnostic Simulation}

Trust-X models realistic clinical encounters as iterative dialogues among specialized agents:

\begin{itemize}
    \item \textbf{Doctor Agent:} Conducts hypothesis-driven reasoning, asks questions, orders investigations, and outputs structured conclusions of the form \texttt{DIAGNOSIS READY: <diagnosis>}.
    \item \textbf{Patient Agent:} Interacts with doctor-agent, and provides consistent, factual responses..
    \item \textbf{Measurement Agent:} Provides diagnostic tests, returning results or indicating unavailable data.
    \item \textbf{Safety Agent:} Monitors proposed actions (tests and prescriptions) for risks or contraindications.
\end{itemize}

Each consultation proceeds in rounds of doctor–patient exchanges until a stable diagnosis and treatment plan emerge. This decomposition enables independent auditing of reasoning quality, safety behavior, and epistemic stability—critical for understanding how LLMs behave under clinical uncertainty.

\subsection{Reasoning Traces and the Differential Diagnosis Lifecycle}

Each Doctor Agent produces structured reasoning encapsulated in \texttt{<thinking\_process>} tags, including:
\begin{itemize}
    \item intermediate hypotheses and their supporting evidence,
    \item motivations for diagnostic test orders, and
    \item discarded hypotheses with rationales.
\end{itemize}

As the dialogue unfolds, each reasoning step is recorded, forming a timeline of evolving hypotheses. This \textbf{Differential Diagnosis Lifecycle (DDxL)} mirrors clinical reasoning, where hypotheses are refined as new evidence emerges, enabling quantitative analysis of reasoning patterns such as redundancy, coverage, and internal coherence.

\subsection{Safety and Prescription Oversight}

Safety checks in \textbf{Trust-X} operate in real time rather than after generation.  
Two agents handle this supervision:
\begin{itemize}
    \item \textbf{Test Safety Agent:} reviews each diagnostic test before it is ordered, flagging redundant or high-risk procedures (for example, unnecessary imaging).
    \item \textbf{Prescription Safety Agent:} examines proposed medications for contraindications or drug–drug interactions (DDIs) and labels them as \textit{SAFE}, \textit{CAUTION}, or \textit{UNSAFE}.
\end{itemize}

These agents influence the reasoning process directly, not just its outputs.  
We measure their activity through the \textit{Test Alert Rate} and the \textit{Unsafe Prescription Rate}.  
Safety labels were assigned using DrugBank \cite{wishart2018drugbank} as a reference for DDIs.

\subsection{Explainability and Logging Layer}

Every message between agents—questions, test orders, safety warnings, and reasoning updates—is stored in a shared logging layer.  
Each record contains:
\begin{enumerate}
    \item the agent’s role,
    \item a timestamp and case context,
    \item the current reasoning trace, and
    \item relevant metadata such as confidence, disagreement, and safety flags.
\end{enumerate}

These logs can be replayed to trace how a diagnosis emerged, inspect reasoning errors, or audit safety behavior.  
This continuous record provides the level of traceability expected in regulated clinical AI systems.

\subsection{Consensus Reasoning and Epistemic Uncertainty}

To represent uncertainty in diagnostic reasoning, we introduce a simple approach called \textbf{consensus reasoning}.  
Instead of depending on a single Doctor Agent, \textit{Trust-X} runs $K$ independent agents that each analyze the same clinical case.  
Their diagnoses $\{d_{i1}, \dots, d_{iK}\}$ are then combined by majority voting.  
We propose the \textbf{Consensus Disagreement Rate (CDR)} as a direct way to measure how much the agents disagree:

\[
\mathrm{CDR}_i = 1 - \frac{\max_{d \in D_i} f_i(d)}{K},
\]

where $f_i(d)$ is the number of agents predicting diagnosis $d$ for case $i$.  
Here, CDR ranges from 0 (full agreement) to 1 (complete disagreement).  
All experiments used $K=3$ agents.

While disagreement-based metrics, the specific CDR definition used here is newly proposed for multi-agent LLM settings.  
It provides an intuitive measure of epistemic uncertainty: higher CDR values mean that agents reached different conclusions, while lower values indicate convergence and stable reasoning.  

---

\subsection{Trust Metrics: From Transparency to Quantification}

To move beyond accuracy alone, we propose a set of three related metrics that together describe how much a system can be trusted:
\begin{itemize}
    \item the \textbf{Epistemic Trust Index (ETI)} — reasoning reliability,
    \item the \textbf{Operational Safety Index (OSI)} — safe and cautious behavior, and
    \item the \textbf{Final Trust Index (FTI)} — overall balance between reasoning quality and safety.
\end{itemize}

These indices were developed in this work to make reasoning and safety measurable in a consistent way.

\paragraph{Epistemic Trust Index (ETI).}
We propose the Epistemic Trust Index (ETI) to combine three core aspects of reasoning quality:  
accuracy, agreement among agents, and internal reasoning–diagnosis alignment:
\[
\mathrm{ETI} = 0.4 \times \mathrm{Accuracy} + 0.3 \times (1 - \mathrm{CDR}) + 0.3 \times \mathrm{RDC}.
\]
The chosen weights give slightly more importance to correctness while rewarding stable and coherent reasoning.  
Ablation studies (Table~\ref{tab:eti_weight_sweep}) show that the relative rankings of configurations remain consistent even when these weights are varied, supporting the stability of the formulation.

\paragraph{Reasoning–Diagnosis Consistency (RDC).}
We also propose the Reasoning–Diagnosis Consistency (RDC) metric to measure how well an agent’s explanation aligns with its final answer.  
RDC is calculated as the cosine similarity between the sentence embeddings of the reasoning text ($t_i$) and the final diagnosis ($d_i$):
\[
\mathrm{RDC}_i = \cos(E(t_i), E(d_i)).
\]
We use the \texttt{SentenceTransformer(all-MiniLM-L6-v2)} model for embedding computation \cite{reimers2019sentencebert}.  
RDC does not judge medical correctness; it only captures whether the reasoning and conclusion are semantically consistent.  
In this study, we introduce RDC as a lightweight way to quantify reasoning coherence in the absence of gold-standard expert annotations.

\paragraph{Operational Safety Index (OSI).}
Operational Safety Index (OSI) quantifies how safely the system behaves when safety supervision is active. we define OSI to be zero whenever safety monitoring is disabled. This ensures that operational trust cannot be achieved through the absence of supervision.

When safety agents are active, OSI decreases as unsafe actions increase, reflecting the proportion of risky behavior detected during reasoning.  
Unsafe behavior includes two types of safety events:
\begin{enumerate}
    \item \textbf{Unsafe prescriptions (UnsafeRx\%)} — drug violating contraindication or drug-drug interaction (DDI) rules.
    \item \textbf{Test safety alerts (TestAlerts\%)} — redundant or high-risk diagnostic tests flagged by the Test Safety Agent.
\end{enumerate}
Because both types of safety failures have comparable clinical importance, we assign them equal weight in the metric:
\[
\mathrm{OSI} = 100 - 0.5(\text{UnsafeRx\%} + \text{TestAlerts\%}).
\]

This linear penalty design provides an interpretable score on a 0–100 scale, where higher values represent safer system behavior under active supervision.  
Thus, OSI distinguishes between systems that are genuinely safe because they detect and avoid risks, and those that only appear safe because no checks were performed.

\paragraph{Final Trust Index (FTI).}


Finally, we propose the Final Trust Index (FTI) as a composite measure that integrates reasoning quality and safety performance:
\[
\mathrm{FTI} = 0.5 \times \mathrm{ETI} + 0.5 \times \mathrm{OSI}.
\]
Because OSI equals zero when safety is disabled, FTI automatically downweights systems that do not include active safety reasoning.  
This way, high trust values correspond only to configurations that are both transparent and risk-aware.  
FTI thus reflects overall trustworthiness based on reasoning integrity and safe behavior rather than raw accuracy alone.




\begin{table*}[t]
\centering
\caption{ETI weight-sweep sensitivity showing stable rankings across weight variations. 
B = Baseline, S = Safety, C = Consensus, T = Trust.}
\label{tab:eti_weight_sweep}
%\small
\begin{tabular}{lcccc}
\toprule
\textbf{Weights (A/CDR/RDC)} & \textbf{ETI (B/S/C/T)} & \textbf{Rank$_\text{ETI}$} & \textbf{FTI (B/S/C/T)} & \textbf{Rank$_\text{FTI}$} \\
\midrule
0.50/0.25/0.25 & 76.9/75.3/69.9/70.0 & B$>$S$>$C$>$T & 39.2/74.6/35.0/74.5 & S$>$T$>$B$>$C \\
0.45/0.30/0.25 & 78.4/76.9/70.0/70.0 & B$>$S$>$C$>$T & 39.2/75.4/35.0/74.5 & S$>$T$>$B$>$C \\
0.40/0.30/0.30 & 78.5/76.7/70.3/70.2 & B$>$S$>$C$>$T & 39.2/75.4/35.2/74.6 & S$>$T$>$B$>$C \\
0.35/0.35/0.30 & 80.0/78.3/70.4/70.2 & B$>$S$>$C$>$T & 39.2/76.2/35.2/74.6 & S$>$T$>$B$>$C \\
0.30/0.35/0.35 & 80.0/78.2/70.7/70.4 & B$>$S$>$C$>$T & 39.2/76.1/35.4/74.7 & S$>$T$>$B$>$C \\
\bottomrule
\end{tabular}
\end{table*}




\begin{table*}[t]
\centering
\caption{Component ablations for ETI. Removing terms changes rankings, confirming that accuracy, consensus stability, and reasoning coherence all contribute to trust estimation.}
\label{tab:eti_component_ablate}
%\small
%\setlength{\tabcolsep}{4pt} % default is 6pt
%\renewcommand{\arraystretch}{0.9} % default is 1.0

\begin{tabular}{lcccc}
\toprule
\textbf{Variant} & \textbf{Rank$_\text{ETI}$} & \textbf{Rank$_\text{FTI}$} & \textbf{ETI$_\text{mean}$} & \textbf{FTI$_\text{mean}$} \\
\midrule
Default (0.4/0.3/0.3) & B$>$S$>$C$>$T & S$>$T$>$B$>$C & 73.9 & 56.1 \\
No RDC term & B$>$S$>$T$>$C & S$>$T$>$B$>$C & 78.4 & 57.3 \\
No CDR term & C$>$T$>$B$>$S & S$>$T$>$B$>$C & 69.5 & 53.9 \\
Accuracy-only & B$>$T$>$S$>$C & S$>$T$>$B$>$C & 68.5 & 52.8 \\
\bottomrule
\end{tabular}
\end{table*}



\subsection{Algorithmic Computation}
To make metric derivation reproducible and verifiable, Algorithm~\ref{alg:trustmetrics} outlines how the three trust indices—Epistemic (ETI), Operational (OSI), and Final (FTI)—are computed within our evaluation pipeline.  
It translates the equations into explicit procedural steps, showing how per-case values for accuracy, consensus disagreement, reasoning–diagnosis consistency, and safety rates are aggregated into system-level trust scores.  
The algorithm also specifies the conditional handling of safety-disabled configurations (where $\mathrm{OSI}=0$), ensuring transparency in how different setups are treated during evaluation.


\begin{algorithm}[H]
\caption{Computation of Trust Metrics. 
This algorithm formalizes the derivation of Epistemic (ETI), Operational (OSI), and Final (FTI) Trust Indices.}
\label{alg:trustmetrics}
\textbf{Input:} For each case $i$: $K$ doctor predictions $\{d_{i1},\dots,d_{iK}\}$, ground truth $g_i$, reasoning trace $t_i$, safety logs\\
\textbf{Parameters:} Encoder $E(\cdot)$ (e.g., \texttt{all-MiniLM-L6-v2}); weights $(w_A,w_C,w_R)=(0.4,0.3,0.3)$; $K{=}3$\\
\textbf{Output:} System-level metrics: ETI, OSI, FTI
\begin{algorithmic}[1]
\STATE Initialize running sums: $\bar{A}=\bar{C}=\bar{R}=\bar{U}=\bar{T}=0$
\STATE Let $N$ be the number of cases
\FOR{$i=1$ to $N$}
    \STATE Count per-diagnosis frequency: $n_{id}=\#\{k : d_{ik}=d\}$
    \STATE $c^\star = \max_d n_{id}$;\quad $\hat{d}_i = \arg\max_d n_{id}$ \COMMENT{majority vote}
    \STATE Normalize embeddings: $u_i=E(t_i)/\|E(t_i)\|$;\quad $v_i=E(\hat{d}_i)/\|E(\hat{d}_i)\|$
    \STATE $\mathrm{Acc}_i = 100 \times \mathbb{1}[\hat{d}_i=g_i]$
    \STATE $\mathrm{CDR}_i = 100 \times \left(1 - \frac{c^\star}{K}\right)$ \COMMENT{0=full consensus, 100=complete disagreement}
    \STATE $\mathrm{RDC}_i = 50 \times (1 + u_i^\top v_i)$ \COMMENT{cosine similarity scaled to [0,100]}
    \STATE From safety logs: $\mathrm{UnsafeRx\%}_i$, $\mathrm{TestAlert\%}_i$
    \STATE Accumulate: $\bar{A}{+}{=}\mathrm{Acc}_i$;\ $\bar{C}{+}{=}\mathrm{CDR}_i$;\ $\bar{R}{+}{=}\mathrm{RDC}_i$;\ 
    $\bar{U}{+}{=}\mathrm{UnsafeRx\%}_i$;\ $\bar{T}{+}{=}\mathrm{TestAlert\%}_i$
\ENDFOR
\STATE Normalize to means (0--100): $\bar{A}/N,\ \bar{C}/N,\ \bar{R}/N,\ \bar{U}/N,\ \bar{T}/N$
\STATE $\mathrm{ETI} = w_A\,\bar{A} + w_C\,(100{-}\bar{C}) + w_R\,\bar{R}$
\IF{ safety agents inactive }
    \STATE $\mathrm{OSI} = 0$
\ELSE
    \STATE $\mathrm{OSI} = 100 - \tfrac{1}{2}(\bar{U} + \bar{T})$
\ENDIF
\STATE $\mathrm{FTI} = 0.5 \times (\mathrm{ETI} + \mathrm{OSI})$
\STATE \textbf{return} (ETI, OSI, FTI)
\end{algorithmic}
\end{algorithm}







.

\section{Experimental Setup}
\label{sec:setup}

\subsection{Dataset and Task}
\paragraph{AgentClinic–MedQA corpus.}
We evaluated Trust-X on 50 diagnostic scenarios drawn from the open-source \textit{MedQA} benchmark \cite{jin2021disease}, adapted into the \textit{AgentClinic} interactive format.  
Each scenario includes structured patient information—symptoms, demographics, and test findings—paired with a verified ground-truth diagnosis from the original MedQA dataset.  
The AgentClinic wrapper provides standardized dialogue templates for multi-agent reasoning and safety validation.  
Cases were randomly selected from the MedQA test split to ensure diversity in disease category and diagnostic complexity.

\paragraph{Sample Size Justification.}
Each of the four configurations was tested on 50 independent clinical cases (200 runs in total).  
This setup represents a focused, pilot-scale evaluation aimed at analyzing reasoning behavior rather than establishing aggregate benchmark scores.  
Resource limitations—particularly inference cost and access to proprietary models—prevented a larger-scale study, so we prioritized depth of reasoning trace analysis and safety behavior inspection.  
The intent was to determine whether current LLM-based systems can demonstrate trustworthy reasoning under realistic clinical supervision, not to report definitive accuracy statistics.

\subsection{Model Configuration}
Each agent type was assigned a foundation model in the reasoning workflow.  
The \textbf{Doctor} and \textbf{Prescription} agents used \textit{Gemini 2.5 Pro} \cite{gemini2024} for its strong analytical and generative capabilities,  
while the \textbf{Patient}, \textbf{Measurement}, and \textbf{Safety} agents used the lightweight \textit{Gemini Flash} model \cite{geminiFlash2024}.  
Dialogues were limited to 20 conversational turns per case to approximate realistic clinical pacing.  
Semantic similarity and reasoning coherence were computed using embeddings from the \texttt{SentenceTransformer(all-MiniLM-L6-v2)} model \cite{reimers2019sentencebert}.

\paragraph{Doctor Agent Replicas.}
The $K$ Doctor Agents are \emph{architecturally identical} and share the same underlying model weights. 
They are not role-specialized or trained differently. 
Epistemic diversity arises solely from independent stochastic decoding (controlled temperature sampling) and independent interaction trajectories with the Patient and Measurement agents.
Each Doctor Agent:
\begin{itemize}
    \item receives the same initial case description,
    \item reasons independently without access to other agents’ intermediate states, and
    \item produces a complete diagnostic reasoning trace and final diagnosis.
\end{itemize}
This design mirrors clinical practice, where multiple physicians independently assess the same patient before consensus.
\paragraph{Consensus Diversity.}
Differences among Doctor Agents therefore reflect \emph{model uncertainty} rather than engineered specialization.
This allows diagnostic disagreement to serve as a direct, behavior-based proxy for epistemic uncertainty, quantified through the Consensus Disagreement Rate (CDR).



\subsection{System Variants}
We evaluated four configurations representing increasing levels of reasoning supervision:
\begin{itemize}
    \item \textbf{Baseline:} Doctor–Patient–Measurement agents only.
    \item \textbf{Safety:} adds safety supervision, but consensus disabled.
    \item \textbf{Consensus:} enables multi-doctor reasoning, safety disabled.
    \item \textbf{Trust (Full):} all agents active with both safety and consensus.
\end{itemize}



% ------------------------------------------------------------------
\section{Results and Discussion}
\label{sec:results}

\subsection{Metric Validation and Reliability}

The proposed metrics were evaluated on 50 simulated clinical cases spanning diagnostic, testing, and prescribing tasks.  
All statistical analyses were performed at the case level ($n{=}50$); system-level comparisons are presented descriptively to illustrate relative behavior rather than to support inferential claims.

\paragraph{Convergent Validity.}
RDC showed positive association with evidence coverage ($r{=}0.56$) and negative association with redundancy ratio ($r{=}-0.62$), suggesting that higher semantic coherence coincides with more complete and less repetitive reasoning.  
This supports the interpretation of RDC as a proxy for reasoning clarity and focus.

\paragraph{Discriminant Validity.}
Case-level correlation between ETI and OSI was weak ($r{=}0.18$), indicating that epistemic reliability (how the model reasons) and operational prudence (how it acts) represent distinct behavioral dimensions.  
This separation is desirable: reasoning quality should not automatically imply behavioral safety, and vice versa.

\paragraph{Ablation Robustness and Weight Justification.}
As shown in Table~\ref{tab:eti_weight_sweep}, the ranking of systems remained stable across a wide range of ETI weight settings, indicating that the metric is not overly sensitive to coefficient choice.  
Further, the component ablation study (Table~\ref{tab:eti_component_ablate}) revealed that removing either CDR or RDC disrupted system ordering and reduced discriminability, confirming that each term contributes essential information.  
Overall, these results suggest that the weighting scheme is both empirically stable and interpretable from a clinical reasoning standpoint.

\subsection{Quantitative Outcomes}

\begin{table*}[t]
\centering
\caption{Performance comparison across Trust-X configurations under the revised OSI formulation.}
\label{tab:results_summary}
%\small
%\setlength{\tabcolsep}{4pt} % default is 6pt
%\renewcommand{\arraystretch}{0.9} % default is 1.0

\begin{tabular}{lccccccc}
\toprule
System & Acc. & CDR & RDC & UnsafeRx & ETI & OSI & FTI \\
\midrule
Baseline  & 69.0 & 0.0 & 69.5 & 0.0 & 78.5 & 0.0 & 39.2 \\
Safety    & 68.0 & 0.0 & 65.1 & 40.0 & 76.8 & 74.0 & 75.4 \\
Consensus & 68.0 & 30.0 & 73.8 & 0.0 & 70.3 & 0.0 & 35.2 \\
Trust     & 69.0 & 30.0 & 72.0 & 30.0 & 70.2 & 79.0 & 74.6 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{fig5_trust_radar.png}
\caption{Multidimensional trust radar: accuracy, RDC, inverted UnsafeRx, and evidence coverage.}
\label{fig:trust_radar}
\end{figure}

Across all configurations, diagnostic accuracy remained stable (68–69\%), confirming that the observed variations in trust metrics reflect differences in reasoning design rather than raw model performance.
Systems without active safety agents (\textbf{Baseline}, \textbf{Consensus}) received $\mathrm{OSI}=0$, which naturally reduced their Final Trust Index (FTI) despite strong accuracy, ensuring that operational trust can only arise when safety monitoring is actually enabled.  
Trust indices were computed using Algorithm~\ref{alg:trustmetrics}, providing consistent treatment across all systems.  

By contrast, the \textbf{Safety} and \textbf{Trust} configurations—both operating with real-time safety feedback—achieved higher FTI values (mean $\approx75.4$).  
These results suggest that genuine oversight and risk-aware reasoning contribute more to trust than predictive accuracy alone.  

\subsection{Trust–Accuracy Relationship}

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{fig2_trust_accuracy_tradeoff.png}
\caption{Trust--Accuracy trade-off. Bubble size represents UnsafeRx\%, and color encodes reasoning coherence (RDC).}
\label{fig:bubble_tradeoff}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{fig3_reasoning_quality.png}
\caption{Reasoning quality metrics: trace length, redundancy ratio, and evidence coverage.}
\label{fig:reasoning_quality}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{fig4_safety_vs_explainability.png}
\caption{Safety vs.\ explainability: test alerts, DDI detections, and RDC coherence.}
\label{fig:safety_explainability}
\end{figure}

\begin{figure}[t][h]
\centering
\includegraphics[width=0.47\textwidth]{fig1_performance_trust.png}
\caption{Performance and trust metrics across Trust-X configurations.}
\label{fig:perf_trust}
\end{figure}

Figure~\ref{fig:bubble_tradeoff} shows how diagnostic accuracy and overall trust interact.  
While all systems reached similar accuracy levels, their trust scores diverged considerably.  
\textbf{Baseline} and \textbf{Consensus} occupy the lower region of the trust axis (FTI 35–39), consistent with their lack of active safety mechanisms.  
In contrast, \textbf{Safety} and \textbf{Trust} attained substantially higher FTI $(\approx 75)$ by detecting and mitigating unsafe actions.  
This pattern highlights that \textit{trustworthiness is expressed through behavior and reasoning, not inferred solely from accuracy metrics}.

\subsection{Reasoning Quality Metrics}
Reasoning-trace statistics (Figure~\ref{fig:reasoning_quality}) show different strengths for each configuration.
The \textbf{Safety} and \textbf{Trust} setups produced the longest reasoning chains (about 300–350 tokens), since both include extra steps for checking and verifying decisions.
The \textbf{Consensus} setup created shorter reasoning traces but achieved the highest coherence (RDC = 73.8) and strong evidence coverage.
However, it also showed more repetition because multiple agents discussed similar points.
Overall, consensus reasoning supports thoughtful and consistent discussion, while safety supervision adds careful review and caution, even if it makes the reasoning longer.

\subsection{Safety and Explainability}
Figure~\ref{fig:safety_explainability} examines how safety monitoring affects reasoning coherence.  
Both \textbf{Safety} and \textbf{Trust} modes generated frequent safety alerts and DDI detections, confirming that oversight mechanisms were active.  
While isolated safety supervision reduced coherence (RDC $\approx65$) due to interruptions in reasoning flow, the \textbf{Trust} configuration recovered much of that coherence through its consensus process.  
This suggests that deliberative multi-agent reasoning can absorb safety feedback constructively, yielding reasoning that is both cautious and comprehensible.

\subsection{Multidimensional Trust Profile}
The radar plot in Figure~\ref{fig:trust_radar} integrates multiple trust dimensions: accuracy, reasoning coherence, evidence coverage, and safety (inverted UnsafeRx and TestAlerts).  
\textbf{Baseline} and \textbf{Consensus} exhibit narrow, safety-deficient profiles, whereas \textbf{Safety} and \textbf{Trust} produce more balanced distributions.  
Among them, the \textbf{Trust} setup shows the most symmetrical profile, combining interpretive stability with active safety oversight.  
This multidimensional view reinforces the central idea behind Trust-X: that trustworthy reasoning arises from the interaction of coherence, accountability, and safety—not from any single metric.

\subsection{Qualitative Reasoning Behavior}
Manual inspection of 50 reasoning traces revealed distinct behavioral styles:
\begin{itemize}
    \item \textbf{Baseline:} fluent but overconfident reasoning with no explicit safety awareness.
    \item \textbf{Safety:} cautious and self-corrective, often interrupted by safety alerts.
    \item \textbf{Consensus:} reflective, uncertainty-aware reasoning through cross-agent deliberation.
    \item \textbf{Trust:} most balanced—combining verification, reflection, and consensus into a cohesive diagnostic flow.
\end{itemize}
These qualitative patterns correspond closely to the quantitative trust profiles, illustrating how safety and consensus jointly shape reasoning transparency.
To complement the aggregate trust metrics, Table~\ref{tab:illustrative_cases} presents two representative reasoning examples from the Trust-X evaluation set. 
These cases illustrate how the proposed framework distinguishes between epistemically coherent yet unsafe reasoning and genuinely trustworthy, safety-aware decision-making.
In \textbf{Case 1}, the model correctly identifies \emph{Myasthenia Gravis}, integrates supporting test evidence, and issues a safety-qualified diagnostic plan under supervision. 
In contrast, \textbf{Case 2} reveals a coherent but unsafe reasoning chain: despite recognizing potential gastrointestinal obstruction, the model recommends no intervention and fails to escalate the case for emergency evaluation. 
This qualitative contrast reinforces the quantitative results—showing that high linguistic coherence (RDC) does not guarantee operational prudence (OSI), and that visible safety supervision is essential for trustworthy reasoning.

\begin{table*}[t]
\centering
\caption{Illustrative reasoning examples from the Trust-X evaluation set. Case 1 demonstrates accurate and safety-aware reasoning; Case 2 reveals unsafe reasoning despite superficially coherent logic.}
\label{tab:illustrative_cases}
\small
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{p{0.13\textwidth} p{0.39\textwidth} p{0.39\textwidth}}
\toprule
\textbf{Category} & \textbf{Case 1 – Correct Reasoning with Active Safety} & \textbf{Case 2 – Unsafe Reasoning under Diagnostic Uncertainty} \\
\midrule
\textbf{Patient Demographics} & 35-year-old female presenting with double vision, difficulty climbing stairs, and upper-limb weakness. & 8-month-old boy with fussiness, decreased feeding, and abdominal distension. \\

\textbf{Objective} & Assess and diagnose neuromuscular disorder. & Diagnose cause of feeding difficulty and abdominal distension. \\

\textbf{History} & One-month history of diplopia and proximal weakness that worsens with exertion and improves with rest. No significant past history. & Several days of irritability and poor feeding, perceived weight gain despite low intake. No fever, vomiting, or diarrhea. \\

\textbf{Physical Examination} & Ptosis on sustained upward gaze; reduced upper-limb strength; normal tone, reflexes, and sensation. & Distended, tympanitic abdomen; crying intensifies on left lower quadrant palpation. \\

\textbf{Investigations} & Positive acetylcholine receptor antibodies; EMG shows decremental response; chest CT normal (no thymoma). & Abdominal X-ray: dilated bowel loops, gasless rectum; Barium enema: transition zone suggesting Hirschsprung disease. \\

\textbf{Ground Truth Diagnosis} & Myasthenia Gravis & Hirschsprung Disease \\

\textbf{Model Reasoning (excerpt)} &
\textit{“The patient’s fluctuating weakness and fatigable ocular symptoms strongly suggest a neuromuscular junction disorder. The top differential is Myasthenia Gravis, supported by antibody positivity and EMG findings. Next step: Edrophonium challenge to confirm, followed by pyridostigmine initiation under monitoring.”} &
\textit{“The initial presentation is non-specific (fussiness, poor feeding) but has a red flag (perceived weight gain despite poor feeding). Asking about vomiting helps distinguish gastroenteritis, pyloric stenosis, or bowel obstruction (e.g., intussusception). The character of the vomit would narrow the differential.”} \\

\textbf{Model Output} & Diagnosis: Myasthenia Gravis; \newline Prescription: Edrophonium 10 mg IV (diagnostic). & Diagnosis: Intussusception (incorrect); \newline Prescription: “No medication appropriate.” \\

\textbf{Safety Assessment} & SAFE WITH CAUTION — requires cardiac monitoring due to risk of bradycardia or bronchospasm. & \textcolor{red}{UNSAFE} — failed to escalate for urgent evaluation despite obstructive findings. \\

\textbf{Trust Metrics} & CDR: 0.0; RDC: 79.3 & CDR: 0.0; RDC: 62.5 \\

\bottomrule
\end{tabular}
\end{table*}


\subsection{Discussion}
The revised trust metrics address prior concerns about score inflation and metric confounding.  
By assigning $\mathrm{OSI}=0$ in configurations without safety agents, we prevent systems from appearing “safe by omission.”  
\textbf{Baseline} and \textbf{Consensus} thus represent reasoning competence without operational vigilance, whereas \textbf{Safety} and \textbf{Trust} capture end-to-end accountability.  
In our experiments, apparent reductions in performance corresponded to more meaningful safety interventions—highlighting that responsible reasoning can appear less efficient but is more trustworthy.  

Overall, Trust-X shows that reliability in medical LLMs depends on the co-evolution of accuracy, explainability, and safety reasoning.  
Rather than treating trust as a static metric, the framework treats it as an outcome of transparent reasoning behavior.  
This supports our central position: \textit{trust cannot be inherited from accuracy—it must be earned through reasoning that can be inspected, verified, and explained.}

\section{Conclusion}
\label{sec:conclusion}
Large language models can appear diagnostically competent yet remain unreliable under clinical scrutiny.  
Trust-X embeds explainability, consensus, and safety directly into the reasoning process, transforming correctness into accountability.  
By revising OSI to zero when supervision is absent, we ensure that trust scores reflect genuine oversight rather than structural artifacts.  
Our results, based on 50 cases per configuration (200 simulations total), offer a diagnostic perspective rather than a benchmark-scale evaluation.  
The patterns observed were consistent across settings, suggesting that key aspects of trustworthy reasoning behavior are robust even within this limited scope.

\section{Ethical and Regulatory Considerations}
Trust-X is intended for research use only and is not a clinical decision-support system.  
All experiments used synthetic or publicly available benchmark data.  
The framework incorporates traceability, safety logging, and human-in-the-loop oversight.















\bibliography{aaai2026}

% Check whether the conference requires a reproducibility checklist to be included in the paper.
% If so, you can uncomment the following line and ajust the path to include it.
% \input{../../ReproducibilityChecklist/LaTeX/ReproducibilityChecklist.tex}

\end{document}
