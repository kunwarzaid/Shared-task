	2.  Lessons Learned from Evaluation of LLM based
BIBLIOGRAPHIC INFORMATION
Title	Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation
Author	Yicong Wu, Ting Chen, Irit Hochberg, Zhoujian Sun, Ruth Edry, Zhengxing Huang, Mor Peleg
Publication Date	15 Jul 2025
Abstract
Therapy recommendation for chronic patients with multimorbidity is challenging due to risks of treatment conflicts. Existing decision support systems face scalability limitations. Inspired by the way in which general practitioners (GP) manage multimorbidity patients, occasionally convening multidisciplinary team (MDT) collaboration, this study investigated the feasibility and value of using a Large Language Model (LLM)-based multi-agent system (MAS) for safer therapy recommendations. We designed a single agent and a MAS framework simulating MDT decision-making by enabling discussion among LLM agents to resolve medical conflicts. The systems were evaluated on therapy planning tasks for multimorbidity patients using benchmark cases. We compared MAS‚Äôs performance with single-agent approaches and realworld benchmarks. An important contribution of our study is the definition of evaluation metrics that go beyond the technical precision and recall and allow the inspection of clinical goals met and medication burden of the proposed advices to a gold standard benchmark. Our results show that with current LLMs, a single agent GP performs as well as MDTs. The best-scoring models provide correct recommendations that address all clinical goals, yet the advices are incomplete. Some models also present unnecessary medications, resulting in unnecessary conflicts between medication and conditions or drug-drug interactions.
Relevant Text
We propose a dynamically generated multi-agent framework that simulates real-world multidisciplinary expert consultations. The MAS replicates the multi-step workflow of MDTs as performed by clinicians in real-world settings, allowing LLMs to propose improved treatment plans by detecting and resolving conflicts. 
2. We apply the proposed MAS to clinical benchmark cases to assess its completeness and correctness using detailed qualitative research. Results demonstrated that, compared to single-agent systems, the MAS showed promise in reducing conflicts while maintaining an appropriate medication count, and it is more closely aligning with real-world expert judgment for multimorbidity patients.
 3. This study develop a new interpretable evaluation strategy. Unlike traditional aggregate metrics, we introduce specific improvement ratio that compares LLM-proposed treatment plans with original plans, focusing on conflict reduction and medication burden, similar to a hazard ratio.
 
To bridge this gap, we explore a novel application of LLM-based MAS collaboration in the domain of therapy recommendation. Unlike prior systems, our framework focuses on the resolution of conflicts, providing an interpretable, traceable workflow and validating final decisions using safety metrics. This design aims to enhance both the clinical transparency and the real-world utility of AIassisted medication planning, particularly in scenarios requiring multidisciplinary reasoning.
III. METHODS
A.	Core Conflict Resolution Workflow
Core Conflict Resolution Workflow Fig. 1a illustrates the conflict resolution workflow for a chronic patient with multimorbidity. Through prompt engineering, the LLM is assigned distinct roles to simulate interactions among multiple agents. The system operates with a primary role, the General Practitioner (GP), who oversees and coordinates the entire workflow. The process begins with the GP reviewing the patient‚Äôs clinical condition, including the chief complaint, medical history, physical findings, laboratory data, and initial prescription. Then the GP is asked to set clinical goals to manage the conditions and complaint, identify existing medications for the goals, and identify potential treatment conflicts, including DDIs and condition-specific contraindications that may increase patient risk: Where ùê∂ represents the detected conflict set, ùëÉ! is the patient‚Äôs clinical condition. Any identified conflicts are reported back to the GP, who evaluates the information and convenes a MDT when he thinks that some aspects of the patient's condition are outside his expertise, selecting specialists relevant to each conflict for resolution. During the conflict resolution phase, each specialist is assigned a subset of discussion points based on the identified conflicts. Some conflicts require input from multiple specialists, while others may be addressed by a single expert. For multi-expert conflicts, a collaborative forum is set up where each specialist presents their perspective, followed by rounds of peer evaluation. If disagreements arise, new arguments are proposed until a consensus is reached (please refer to Github for more details). The discussion proceeds in rounds ùëó ‚àà [1, ùëö]. The maximal number of rounds m was set empirically to 5, based on observations that most meaningful disagreements or suggestions occur within the first few exchanges. Beyond this round, agents tend to repeat previously stated positions. Additionally, longer discussions have risk in exceeding the model‚Äôs token limit. If a consensus is not reached within m rounds, a mediator agent is invoked to summarize the arguments and propose a consensus recommendation. In cases where only one expert is needed, the assigned specialist directly provides a recommendation. Upon resolution of all conflicts, the GP synthesizes the recommendations and finalizes the treatment plan, resulting in a revised prescription.
 

Searcher‚Äôs Comment
The cited non patent literature details a method for LLMs to detect medication conflicts and propose improved plan. This is similar to the use of LLM for monitoring test and prescription and maps with the Feature 1 (System and method for measuring the overall trustworthiness of medical large language models based on Consensus Reasoning and Epistemic Uncertainty, consistency between the reasoning and diagnosis of LLM, and monitoring of tests and prescriptions) of the proposed invention.  The cited prior art also computes DDI Ratio, Contraindication Ratio, and Medication Ratio,  to assess the size and direction of changes to DDI and medication number introduced by LLM-generated medication plans. However the method is different.
Inventor‚Äôs differentiating points / additional information is important to conclude relevance of the subject prior art to the proposed invention.
Inventor‚Äôs Comment
Feature 1 (System and method for measuring the overall trustworthiness of medical large language models based on Consensus Reasoning and Epistemic Uncertainty, consistency between the reasoning and diagnosis of LLM, and monitoring of tests and prescriptions): 
