# ===============================================
# ðŸ§ª Diagnostic Accuracy Benchmark (LLM Baselines)
# ChatDoctor-7B | Clinical-Camel-7B
# Using AgentClinic-MedQA 10 scenarios
# ===============================================

!pip install transformers accelerate bitsandbytes tqdm -q

import json, re, os, time
import torch, pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from google.colab import files

# ------------------------------------------------
# âš™ï¸ Configuration
# ------------------------------------------------
AGENTCLINIC_FILE = "agentclinic_medqa.jsonl"   # upload this if not already present
NUM_SCENARIOS = 10
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SAVE_DIR = "/content/diagnostic_comparison"
os.makedirs(SAVE_DIR, exist_ok=True)

MODELS = {
    "ChatDoctor-7B": "lxe/ChatDoctor-7B",
    "Clinical-Camel-7B": "ehsanaghaei/Clinical-Camel-7B"  # placeholder HF ID â€” replace if different
}

# ------------------------------------------------
# ðŸ§© Load AgentClinic scenarios
# ------------------------------------------------
if not os.path.exists(AGENTCLINIC_FILE):
    print("â¬†ï¸ Please upload your agentclinic_medqa.jsonl file now.")
    uploaded = files.upload()

scenarios = []
with open(AGENTCLINIC_FILE, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i >= NUM_SCENARIOS: break
        d = json.loads(line)
        diag = d.get("OSCE_Examination", {}).get("Correct_Diagnosis", "Unknown")
        actor = d.get("OSCE_Examination", {}).get("Patient_Actor", {})
        hist = actor.get("History", "")
        phys = actor.get("Physical_Examination_Findings", "")
        labs = d.get("OSCE_Examination", {}).get("Test_Results", {})
        summary = f"History: {hist}\nExam: {phys}\nLabs: {json.dumps(labs)}"
        scenarios.append({"id": i, "summary": summary, "gt": diag})

print(f"âœ… Loaded {len(scenarios)} AgentClinic scenarios.")

# ------------------------------------------------
# ðŸ§  Model inference utility
# ------------------------------------------------
def ask_model(model_name, model_id, summary):
    """Query model for diagnosis and return raw text output."""
    try:
        tok = AutoTokenizer.from_pretrained(model_id)
        mdl = AutoModelForCausalLM.from_pretrained(
            model_id, device_map="auto", load_in_8bit=True)
        pipe = pipeline(
            "text-generation", model=mdl, tokenizer=tok,
            device=0 if DEVICE=="cuda" else -1,
            max_new_tokens=100, temperature=0.1)
        prompt = (
            "You are a clinical reasoning expert.\n"
            "Given the following patient case, state the SINGLE most likely diagnosis.\n"
            "Provide only the diagnosis name, no explanation.\n"
            f"Case:\n{summary}\n\nDiagnosis:"
        )
        out = pipe(prompt, num_return_sequences=1)[0]["generated_text"]
        m = re.search(r"Diagnosis[:\- ]*(.*)", out, re.I)
        return m.group(1).split("\n")[0] if m else out.strip()
    except Exception as e:
        return f"Error: {e}"

# ------------------------------------------------
# âš–ï¸ Compare results (simple fuzzy match)
# ------------------------------------------------
def compare_diag(pred, gold):
    """Heuristic comparison for string match."""
    pred, gold = pred.lower(), gold.lower()
    return gold in pred or pred in gold or \
           len(set(pred.split()) & set(gold.split())) / len(set(gold.split())) > 0.5

# ------------------------------------------------
# ðŸš€ Run benchmark
# ------------------------------------------------
records = []
for model_name, model_id in MODELS.items():
    print(f"\n=== Evaluating {model_name} ===")
    for sc in tqdm(scenarios):
        pred = ask_model(model_name, model_id, sc["summary"])
        ok = compare_diag(pred, sc["gt"])
        records.append({
            "scenario": sc["id"],
            "model": model_name,
            "pred_diag": pred,
            "gold_diag": sc["gt"],
            "correct": ok
        })

# ------------------------------------------------
# ðŸ’¾ Save results
# ------------------------------------------------
df = pd.DataFrame(records)
df.to_csv(f"{SAVE_DIR}/diagnostic_comparison.csv", index=False)
summary = df.groupby("model")["correct"].mean().mul(100).round(2)
print("\n=== Diagnostic Accuracy Summary ===")
print(summary)
summary.to_csv(f"{SAVE_DIR}/diagnostic_summary.csv")

print(f"\nAll results saved in: {SAVE_DIR}")
