Large Language Models (LLMs) have recently demonstrated remarkable diagnostic and reasoning capabilities across medical benchmarks, yet their deployment in clinical decision support remains limited by issues of reliability, transparency, and trustworthiness. Prior work has focused on improving safety—for instance, MedGuard introduced multi-agent validation to reduce harmful or contraindicated recommendations. However, little is known about why medical LLMs reach particular conclusions, how consistently they reason across agents, or whether explainability correlates with diagnostic correctness.

In this paper, we extend the MedGuard framework to develop MedGuard-X, a trust-centered and explainable diagnostic simulation pipeline. MedGuard-X integrates (i) multi-LLM consensus reasoning to estimate epistemic uncertainty, (ii) explainable reasoning traces for each clinical action, and (iii) trustworthiness scoring through disagreement tracking and justification alignment. Experiments on 20 complex clinical scenarios demonstrate that while MedGuard-X increases transparency and reduces unsafe test and drug recommendations, diagnostic accuracy remains inconsistent. In many cases, LLMs exhibit confident but unjustified reasoning—highlighting that explainability does not imply reliability.

Our results emphasize the need for structured accountability mechanisms and hybrid symbolic-LLM architectures before LLMs can be considered trustworthy in real-world clinical environments.
