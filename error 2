While Large Language Models (LLMs) have shown impressive skill in medical diagnosis and reasoning on paper, their use in actual clinical settings to support doctors' decisions is still held back by significant concerns about their reliability, transparency, and overall trustworthiness.
Previous efforts have mainly focused on making these systems safer. One such system, MedGuard, for instance, used a multi-agent approach to validate AI suggestions, helping to weed out recommendations that could be harmful or were medically inappropriate. However, this still leaves critical questions unanswered. We have little insight into the reasoning process of a medical LLM, how consistently different AI agents approach the same problem, or whether an AI's ability to explain itself has any bearing on the accuracy of its diagnosis.
To investigate these gaps, we developed MedGuard-X, an extension of the original MedGuard framework. This new pipeline is built from the ground up to center on trust and explainability in diagnostic simulations. MedGuard-X functions by having multiple LLMs work on a problem to gauge consensus, generating clear reasoning chains for every step, and assigning a "trustworthiness score" based on how well the final justification aligns with the proposed actions.
Testing this system across 20 complex clinical situations revealed a mixed outcome. While MedGuard-X successfully improved transparency and significantly reduced unsafe recommendations for tests and drugs, its ability to make an accurate diagnosis remained inconsistent. We frequently observed the LLMs presenting flawed reasoning with a high degree of confidence, underscoring a critical insight: a system's ability to provide an explanation does not guarantee its reliability.
These results make it clear that before LLMs can be safely integrated into real-world medical environments, we must establish more structured methods for accountability. The path forward will likely involve creating hybrid systems that merge the capabilities of LLMs with the reliability of symbolic, rule-based AI.
