# --- Critical fix for QLoRA gradient issue ---
model.config.use_cache = False  # Required for training
try:
    if hasattr(model, "gradient_checkpointing_disable"):
        model.gradient_checkpointing_disable()
    print("✅ Gradient checkpointing disabled.")
except Exception as e:
    print("⚠️ Could not disable gradient checkpointing:", e)

model.train()  # ensure model is in training mode
