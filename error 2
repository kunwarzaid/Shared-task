import os
import json
import time
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig
from peft import PeftModel

# ==============================================================
# CONFIGURATION
# ==============================================================

BASE_MODEL = "/workspace/data/KZ_2117574/gemma_3_1b"
ADAPTER_DIR = "/workspace/data/KZ_2117574/less_para_gemma/run_gemma1b_lora"
EXTRACT_DIR = "/workspace/data/KZ_2117574/test_data_release"
OUTPUT_DIR = "/workspace/data/KZ_2117574/SharedTask_Inference_Output"

MAX_INPUT_TOKENS = 6144
MAX_NEW_TOKENS_SUMMARY = 300
MAX_NEW_TOKENS_JSON = 600
MAX_NEW_TOKENS_QNA = 200

# Sampling hyperparams (for text/QnA)
TEMPERATURE = 0.3
TOP_P = 0.9
REPETITION_PENALTY = 1.1

# ==============================================================
# TIMESTAMP LOGGER
# ==============================================================

def tprint(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

# ==============================================================
# HELPERS
# ==============================================================

def safe_read_jsonl(path: Path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rows.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return rows

def clip_tokens(tokenizer, text, max_tokens):
    ids = tokenizer.encode(text, add_special_tokens=False)
    if len(ids) <= max_tokens:
        return text
    ids = ids[-max_tokens:]
    return tokenizer.decode(ids, skip_special_tokens=True)

# ==============================================================
# GENERATION FUNCTIONS
# ==============================================================

def gen_text(model, tokenizer, prompt, max_new_tokens):
    """Sampling-based text generation (used for summary + QnA)."""
    inputs = tokenizer(prompt.strip(), return_tensors="pt", padding=True, truncation=True).to(model.device)
    gen_conf = GenerationConfig(
        temperature=TEMPERATURE,
        top_p=TOP_P,
        top_k=50,
        repetition_penalty=REPETITION_PENALTY,
        do_sample=True,
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
    )
    with torch.inference_mode():
        outputs = model.generate(**inputs, generation_config=gen_conf)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


def try_extract_json(text: str):
    """Try to cleanly extract valid JSON from model output."""
    text = text.strip()
    if "```" in text:
        for block in text.split("```"):
            block = block.strip().removeprefix("json").strip()
            try:
                return json.loads(block)
            except Exception:
                pass
    try:
        return json.loads(text)
    except Exception:
        start, end = text.find("{"), text.rfind("}")
        if start != -1 and end != -1 and end > start:
            try:
                return json.loads(text[start:end+1])
            except Exception:
                pass
    return None


def gen_json(model, tokenizer, prompt, max_new_tokens):
    """Deterministic JSON generation for structured outputs."""
    inputs = tokenizer(prompt.strip(), return_tensors="pt", padding=True, truncation=True).to(model.device)
    gen_conf = GenerationConfig(
        temperature=0.0,
        do_sample=False,
        top_p=1.0,
        top_k=0,
        repetition_penalty=1.0,
        max_new_tokens=max_new_tokens,
        pad_token_id=tokenizer.eos_token_id,
    )
    with torch.inference_mode():
        outputs = model.generate(**inputs, generation_config=gen_conf)
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    obj = try_extract_json(text)
    if obj:
        return obj

    # Retry with stricter prompt
    strict_prompt = prompt + "\n\nReturn ONLY valid JSON."
    inputs = tokenizer(strict_prompt.strip(), return_tensors="pt", truncation=True, padding=True).to(model.device)
    with torch.inference_mode():
        outputs = model.generate(**inputs, generation_config=gen_conf)
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    obj2 = try_extract_json(text)
    return obj2 or {}

def write_json(path: Path, obj):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def write_text(path: Path, text):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text.strip() + "\n")

# ==============================================================
# PROMPTS AND SCHEMA
# ==============================================================

JSON_TEMPLATE = {
  "patient_identifiers": None,
  "demographics": {"age": None, "sex": None},
  "visit": {"date_time": None, "type": None},
  "chief_complaint": None,
  "onset_duration": None,
  "symptom_description": None,
  "aggravating_factors": None,
  "relieving_factors": None,
  "associated_symptoms": [],
  "past_medical_history": None,
  "past_surgical_history": None,
  "family_history": None,
  "current_medications": [],
  "allergies": None,
  "social_history": [],
  "functional_status": None,
  "vital_signs": None,
  "examination_findings": None,
  "investigations": [],
  "assessment_primary_diagnosis": None,
  "differential_diagnoses": [],
  "management_plan": None,
  "tests_referrals_planned": [],
  "follow_up_plan": None,
  "chronology_response_to_treatment": None,
  "patient_concerns_preferences_consent": None,
  "safety_issues_red_flags": None,
  "coding_terms": None,
  "conversation_metadata": {"timestamps": [], "speaker_labels": []}
}

LANG_HINTS = {
    "English": "English",
    "Hindi": "Hindi",
    "Gujarati": "Gujarati",
    "Bangla": "Bangla",
    "Assamese": "Assamese",
    "Kannada": "Kannada",
    "Marathi": "Marathi",
    "Tamil": "Tamil",
    "Telugu": "Telugu",
}

def build_summary_text_prompt(dialogue_text):
    return (
        "Summarize the following doctor‚Äìpatient dialogue in English.\n\n"
        f"Dialogue:\n{dialogue_text}\n\nSummary:"
    )

def build_summary_json_prompt(dialogue_text):
    schema = json.dumps(JSON_TEMPLATE, indent=2)
    return (
        "From the following doctor‚Äìpatient dialogue, extract structured clinical information.\n"
        "Return ONLY valid JSON matching this schema:\n"
        f"{schema}\n\nDialogue:\n{dialogue_text}\n\nJSON:"
    )

def build_qna_prompt(question, lang):
    return (
        f"Answer the following patient question in {lang}. "
        f"Be concise and factual.\n\nQuestion: {question}\nAnswer:"
    )

# ==============================================================
# INFERENCE PIPELINE
# ==============================================================

def run_inference():
    tprint("üöÄ Starting V100/A100-optimized inference")

    # Load model efficiently (4-bit quantization)
    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",
    )

    tprint("üîª Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    tprint("üîª Loading base model (fp16, 4-bit)...")
    base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        quantization_config=bnb_cfg,
        torch_dtype=torch.float16,
    )
    model = PeftModel.from_pretrained(base, ADAPTER_DIR)
    model.eval()

    tprint("‚úÖ Model loaded successfully.")

    langs = [p for p in Path(EXTRACT_DIR).iterdir() if p.is_dir()]
    tprint(f"üåç Found {len(langs)} languages: {[p.name for p in langs]}")

    for lang_dir in langs:
        lang = lang_dir.name
        lang_hint = LANG_HINTS.get(lang, "the same language")
        out_lang = Path(OUTPUT_DIR) / lang
        dlg_dir = lang_dir / "Dialogues"
        qna_dir = lang_dir / "QnA"

        tprint(f"üóÇ Processing language: {lang}")

        # Summaries
        if dlg_dir.exists():
            dlg_files = list(dlg_dir.glob("*.jsonl"))
            tprint(f"üìÑ Dialogues: {len(dlg_files)}")
            for dlg_file in dlg_files:
                try:
                    rows = safe_read_jsonl(dlg_file)
                    dialogue = " ".join(
                        str(r.get("dialogue", "")) if isinstance(r, dict) else str(r)
                        for r in rows
                    )
                    dialogue = clip_tokens(tokenizer, dialogue, MAX_INPUT_TOKENS)
                    text_prompt = build_summary_text_prompt(dialogue)
                    json_prompt = build_summary_json_prompt(dialogue)

                    text_out = gen_text(model, tokenizer, text_prompt, MAX_NEW_TOKENS_SUMMARY)
                    json_out = gen_json(model, tokenizer, json_prompt, MAX_NEW_TOKENS_JSON) or JSON_TEMPLATE

                    write_text(out_lang / "Summary_Text" / f"{dlg_file.stem}_summary.txt", text_out)
                    write_json(out_lang / "Summary_Json" / f"{dlg_file.stem}_summary.json", json_out)

                except Exception as e:
                    print(f"‚ö†Ô∏è Skipped {dlg_file.name}: {e}")

        # QnA
        if qna_dir.exists():
            for qna_file in qna_dir.glob("*.json"):
                try:
                    data = json.load(open(qna_file, encoding="utf-8"))
                    out_qs = {"questions": []}
                    for qa in data.get("questions", []):
                        q = qa.get("question", "").strip()
                        if not q:
                            continue
                        prompt = build_qna_prompt(q, lang_hint)
                        ans = gen_text(model, tokenizer, prompt, MAX_NEW_TOKENS_QNA)
                        ans = ans.replace("Answer:", "").strip()
                        out_qs["questions"].append({"question": q, "answer": ans})
                    write_json(out_lang / "QnA" / f"{qna_file.stem}_answers.json", out_qs)
                except Exception as e:
                    print(f"‚ö†Ô∏è Skipped {qna_file.name}: {e}")

    tprint(f"‚úÖ Inference complete! Results saved to {OUTPUT_DIR}")

# ==============================================================
# MAIN
# ==============================================================

if __name__ == "__main__":
    run_inference()
