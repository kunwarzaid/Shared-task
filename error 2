import os, json, time, random, re
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from indic_transliteration import sanscript
from indic_transliteration.sanscript import transliterate

# =========================
# PATHS
# =========================
BASE_MODEL   = "/workspace/data/KZ_2117574/gemma_3_1b"
ADAPTER_DIR  = "/workspace/data/KZ_2117574/less_para_gemma/run_gemma1b_lora"
TRAIN_DIR    = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/train"
TEST_DIR     = "/workspace/data/KZ_2117574/test_data_release/test_data_release"
OUTPUT_DIR   = "/workspace/data/KZ_2117574/SharedTask_Inference_Output_romanized"

# =========================
# CONSTANTS
# =========================
SEED = 42
FEWSHOT_PER_LANG = 1
MODEL_MAX = 4096
CHUNK_SIZE = 1000
MAX_NEW_TOKENS_SUMMARY = 300
MAX_NEW_TOKENS_JSON = 600
MAX_NEW_TOKENS_QNA = 200

TEMPERATURE_JSON = 0.3
TOP_P_JSON = 0.95
TEMPERATURE_QNA = 0.7
TOP_P_QNA = 0.9
REP_PENALTY = 1.07

LANG_HINTS = {
    "English": "English", "Hindi": "Hindi", "Gujarati": "Gujarati", "Bangla": "Bangla",
    "Assamese": "Assamese", "Kannada": "Kannada", "Marathi": "Marathi",
    "Tamil": "Tamil", "Telugu": "Telugu", "Dogri": "Dogri"
}

# ‚úÖ Corrected: Assamese uses Bengali script
SANSCRIPT_MAP = {
    "Hindi": sanscript.DEVANAGARI,
    "Gujarati": sanscript.GUJARATI,
    "Bangla": sanscript.BENGALI,
    "Assamese": sanscript.BENGALI,
    "Tamil": sanscript.TAMIL,
    "Telugu": sanscript.TELUGU,
    "Kannada": sanscript.KANNADA,
    "Marathi": sanscript.DEVANAGARI,
    "Dogri": sanscript.DEVANAGARI,
}

JSON_TEMPLATE = {
  "patient_identifiers": None, "demographics": {"age": None, "sex": None},
  "visit": {"date_time": None, "type": None},
  "chief_complaint": None, "onset_duration": None, "symptom_description": None,
  "aggravating_factors": None, "relieving_factors": None,
  "associated_symptoms": [], "past_medical_history": None,
  "past_surgical_history": None, "family_history": None,
  "current_medications": [], "allergies": None, "social_history": [],
  "functional_status": None, "vital_signs": None, "examination_findings": None,
  "investigations": [], "assessment_primary_diagnosis": None,
  "differential_diagnoses": [], "management_plan": None,
  "tests_referrals_planned": [], "follow_up_plan": None,
  "chronology_response_to_treatment": None,
  "patient_concerns_preferences_consent": None,
  "safety_issues_red_flags": None, "coding_terms": None,
  "conversation_metadata": {"timestamps": [], "speaker_labels": []}
}

# =========================
# HELPERS
# =========================
def tprint(msg):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def safe_read_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            s = line.strip()
            if not s: continue
            try: rows.append(json.loads(s))
            except json.JSONDecodeError: continue
    return rows

# --- Transliteration helpers ---
def romanize_text(text, lang):
    if lang not in SANSCRIPT_MAP: return text
    try:
        return transliterate(text, SANSCRIPT_MAP[lang], sanscript.ITRANS)
    except Exception as e:
        print(f"‚ö†Ô∏è Romanisation failed for {lang}: {e}")
        return text

def deromanize_text(text, lang):
    if lang not in SANSCRIPT_MAP: return text
    try:
        return transliterate(text, sanscript.ITRANS, SANSCRIPT_MAP[lang])
    except Exception as e:
        print(f"‚ö†Ô∏è De-romanisation failed for {lang}: {e}")
        return text

def join_dialogue_jsonl(rows):
    return "\n".join(str(r.get("dialogue", "")) if isinstance(r, dict) else str(r) for r in rows)

def chunk_text_by_tokens(tokenizer, text, chunk_size):
    ids = tokenizer.encode(text, add_special_tokens=False)
    chunks = []
    for i in range(0, len(ids), chunk_size):
        sub = tokenizer.decode(ids[i:i + chunk_size], skip_special_tokens=True)
        chunks.append(sub)
    return chunks

def clean_answer(ans):
    ans = re.sub(r"^Answer\s*[:\-]*\s*", "", ans.strip(), flags=re.IGNORECASE)
    return ans.strip()

def try_extract_json(text):
    text = text.strip()
    candidates = re.findall(r"\{.*\}", text, flags=re.DOTALL)
    for c in candidates:
        try:
            return json.loads(c)
        except Exception:
            continue
    return None

def write_json(path, obj):
    path.parent.mkdir(parents=True, exist_ok=True)
    json.dump(obj, open(path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)

def write_text(path, text):
    path.parent.mkdir(parents=True, exist_ok=True)
    open(path, "w", encoding="utf-8").write(text.strip() + "\n")

# =========================
# PROMPTS
# =========================
def build_summary_prompt(dialogue):
    return (
        "You are a clinical summarization assistant.\n"
        "Summarize the following doctor‚Äìpatient dialogue clearly and concisely in English.\n"
        "Focus on diagnosis, symptoms, and management.\nReturn only the summary text.\n\n"
        f"### Dialogue:\n{dialogue}\n\n### Summary:"
    )

def build_json_prompt(dialogue):
    schema = json.dumps(JSON_TEMPLATE, indent=2, ensure_ascii=False)
    return (
        "You are a clinical assistant.\n"
        "Extract structured information from the dialogue below.\n"
        "Populate the JSON schema accurately. Use null or [] if not mentioned.\n"
        "Return ONLY valid JSON.\n\n"
        f"### Schema:\n{schema}\n\n### Dialogue:\n{dialogue}\n\n### JSON:"
    )

def build_qna_prompt(question, lang_hint):
    return (
        f"You are a medical assistant answering patient questions in {lang_hint}.\n"
        "Answer clearly and concisely.\n"
        f"### Question: {question}\n### Answer:"
    )

# =========================
# GENERATION
# =========================
def gen_text(model, tokenizer, prompt, max_new_tokens, temp=0.7, top_p=0.9):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=MODEL_MAX).to(model.device)
    with torch.inference_mode():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temp,
            top_p=top_p,
            repetition_penalty=REP_PENALTY,
            pad_token_id=tokenizer.eos_token_id,
        )
    return tokenizer.decode(out[0], skip_special_tokens=True).strip()

def gen_json(model, tokenizer, prompt, max_new_tokens):
    txt = gen_text(model, tokenizer, prompt, max_new_tokens, temp=TEMPERATURE_JSON, top_p=TOP_P_JSON)
    obj = try_extract_json(txt)
    if obj: return obj
    txt2 = gen_text(model, tokenizer, prompt + "\nReturn ONLY valid JSON.", max_new_tokens)
    obj2 = try_extract_json(txt2)
    return obj2 or JSON_TEMPLATE

# =========================
# MAIN PIPELINE
# =========================
def main():
    random.seed(SEED)
    torch.set_grad_enabled(False)
    bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type="nf4")

    tprint("üöÄ Loading model and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL, device_map="auto", quantization_config=bnb_cfg, torch_dtype=torch.float16
    )
    model = PeftModel.from_pretrained(base, ADAPTER_DIR)
    model.eval()
    tprint("‚úÖ Model loaded successfully.")

    langs = [p for p in Path(TEST_DIR).iterdir() if p.is_dir()]
    tprint(f"Found {len(langs)} languages: {[p.name for p in langs]}")

    for lang_dir in langs:
        lang = lang_dir.name
        lang_hint = LANG_HINTS.get(lang, "the same language")
        out_lang = Path(OUTPUT_DIR) / lang
        dlg_dir = lang_dir / "Dialogues"
        qna_dir = lang_dir / "QnA"

        tprint(f"üóÇ Processing: {lang}")

        # === Summaries ===
        if dlg_dir.exists():
            for p in sorted(dlg_dir.glob("*.jsonl")):
                try:
                    rows = safe_read_jsonl(p)
                    dialogue = join_dialogue_jsonl(rows)
                    roman_dialogue = romanize_text(dialogue, lang)
                    chunks = chunk_text_by_tokens(tokenizer, roman_dialogue, CHUNK_SIZE)

                    summaries = []
                    for chunk in chunks:
                        chunk_prompt = build_summary_prompt(chunk)
                        out = gen_text(model, tokenizer, chunk_prompt, MAX_NEW_TOKENS_SUMMARY)
                        summaries.append(out)

                    full_summary_en = " ".join(summaries)
                    json_prompt = build_json_prompt(full_summary_en)
                    json_out = gen_json(model, tokenizer, json_prompt, MAX_NEW_TOKENS_JSON)

                    # De-romanize summary text
                    summary_native = deromanize_text(full_summary_en, lang)
                    for k, v in json_out.items():
                        if isinstance(v, str):
                            json_out[k] = deromanize_text(v, lang)

                    write_text(out_lang / "Summary_Text" / f"{p.stem}_summary.txt", summary_native)
                    write_json(out_lang / "Summary_Json" / f"{p.stem}_summary.json", json_out)
                except Exception as e:
                    tprint(f"‚ö†Ô∏è Skipped {p.name}: {e}")

        # === QnA ===
        if qna_dir.exists():
            for p in sorted(qna_dir.glob("*.json")):
                try:
                    data = json.load(open(p, encoding="utf-8"))
                    out = {"questions": []}
                    for qa in data.get("questions", []):
                        q = qa.get("question", "").strip()
                        if not q:
                            continue
                        roman_q = romanize_text(q, lang)
                        prompt = build_qna_prompt(roman_q, lang_hint)
                        ans_rom = gen_text(model, tokenizer, prompt, MAX_NEW_TOKENS_QNA,
                                           temp=TEMPERATURE_QNA, top_p=TOP_P_QNA)
                        ans_native = deromanize_text(clean_answer(ans_rom), lang)
                        out["questions"].append({"question": q, "answer": ans_native})
                    write_json(out_lang / "QnA" / f"{p.stem}_answers.json", out)
                except Exception as e:
                    tprint(f"‚ö†Ô∏è Skipped {p.name}: {e}")

    tprint(f"‚úÖ Inference complete. Results saved to {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
