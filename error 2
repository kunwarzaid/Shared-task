import os, json, time, random
from pathlib import Path
from typing import List, Dict, Tuple

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel

# =========================
# PATHS (edit if needed)
# =========================
BASE_MODEL   = "/workspace/data/KZ_2117574/gemma_3_1b"
ADAPTER_DIR  = "/workspace/data/KZ_2117574/less_para_gemma/run_gemma1b_lora"
TRAIN_DIR    = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/train"
TEST_DIR     = "/workspace/data/KZ_2117574/test_data_release"
OUTPUT_DIR   = "/workspace/data/KZ_2117574/SharedTask_Inference_Output"

# =========================
# RUNTIME / GENERATION
# =========================
SEED = 42
random.seed(SEED)

# keep budgets modest so few-shot fits comfortably on V100
MODEL_MAX = 4096            # safe window for input
FEWSHOT_PER_LANG = 2        # using 2 works best for speed/quality
RESERVE_FOR_TASK = 1536     # keep for the *test* dialogue itself

# new tokens
MAX_NEW_TOKENS_SUMMARY = 220
MAX_NEW_TOKENS_JSON    = 380
MAX_NEW_TOKENS_QNA     = 140

# decoding
QNA_TEMP = 0.7
QNA_TOPP = 0.9
REP_PEN  = 1.07

LANG_HINTS = {
    "English": "English", "Hindi": "Hindi", "Gujarati": "Gujarati", "Bangla": "Bangla",
    "Assamese": "Assamese", "Kannada": "Kannada", "Marathi": "Marathi",
    "Tamil": "Tamil", "Telugu": "Telugu", "Dogri": "Dogri"
}

JSON_TEMPLATE = {
  "patient_identifiers": None,
  "demographics": {"age": None, "sex": None},
  "visit": {"date_time": None, "type": None},
  "chief_complaint": None,
  "onset_duration": None,
  "symptom_description": None,
  "aggravating_factors": None,
  "relieving_factors": None,
  "associated_symptoms": [],
  "past_medical_history": None,
  "past_surgical_history": None,
  "family_history": None,
  "current_medications": [],
  "allergies": None,
  "social_history": [],
  "functional_status": None,
  "vital_signs": None,
  "examination_findings": None,
  "investigations": [],
  "assessment_primary_diagnosis": None,
  "differential_diagnoses": [],
  "management_plan": None,
  "tests_referrals_planned": [],
  "follow_up_plan": None,
  "chronology_response_to_treatment": None,
  "patient_concerns_preferences_consent": None,
  "safety_issues_red_flags": None,
  "coding_terms": None,
  "conversation_metadata": {"timestamps": [], "speaker_labels": []}
}

# -------------------------
def tprint(msg:str):
    print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def safe_read_jsonl(path: Path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            s = line.strip()
            if not s: continue
            try:
                rows.append(json.loads(s))
            except json.JSONDecodeError:
                continue
    return rows

def read_text(path: Path) -> str:
    try:
        return Path(path).read_text(encoding="utf-8", errors="replace").strip()
    except:
        return ""

def clip_to_tokens(tok, text: str, max_tok: int) -> str:
    ids = tok.encode(text, add_special_tokens=False)
    if len(ids) <= max_tok: return text
    ids = ids[-max_tok:]  # keep tail (most recent turns)
    return tok.decode(ids, skip_special_tokens=True)

def join_dialogue_jsonl(rows: List[dict]) -> str:
    parts = []
    for r in rows:
        if isinstance(r, dict):
            d = r.get("dialogue", "")
        else:
            d = str(r)
        if isinstance(d, list):
            d = " ".join(map(str, d))
        parts.append(str(d))
    return "\n".join(parts).strip()

# -------------------------
# Few-shot utilities
# -------------------------
def sample_fewshot_training(lang_dir: Path, tok, budget_tokens: int) -> Tuple[str, str, str]:
    """
    Returns 3 few-shot blocks as strings (summary, json, qna).
    JSON few-shot is optional (we'll give 1 schema-only example + possibly one light example
    by mirroring a few fields from the textual summary). To stay safe & fast, we do schema-only.
    """
    dlg_dir = lang_dir / "Dialogues"
    sum_dir = lang_dir / "Summary_Text"
    qna_dir = lang_dir / "QnA"

    fs_summary_blocks = []
    fs_json_blocks    = []
    fs_qna_blocks     = []

    # ---- Summaries few-shot ----
    if dlg_dir.exists() and sum_dir.exists():
        files = sorted([p for p in dlg_dir.glob("*.jsonl")])
        # deterministic small sampling
        random.Random(SEED).shuffle(files)
        for p in files[:FEWSHOT_PER_LANG]:
            rows = safe_read_jsonl(p)
            dial = join_dialogue_jsonl(rows)
            sum_path = sum_dir / f"{p.stem}_summary.txt"
            summ = read_text(sum_path)
            if dial and summ:
                dial = clip_to_tokens(tok, dial, min(800, budget_tokens//FEWSHOT_PER_LANG))
                summ = summ.strip()
                fs_summary_blocks.append(
                    f"Example:\nDialogue:\n{dial}\n\nSummary:\n{summ}\n"
                )

    # ---- JSON few-shot (schema hint only to avoid token bloat) ----
    schema = json.dumps(JSON_TEMPLATE, indent=2, ensure_ascii=False)
    fs_json_blocks.append(
        "Example (format only):\nDialogue:\n<short clinical conversation>\n\nJSON:\n" + schema + "\n"
    )

    # ---- QnA few-shot ----
    if qna_dir.exists():
        files = sorted([p for p in qna_dir.glob("*.json")])
        random.Random(SEED).shuffle(files)
        taken = 0
        for p in files:
            try:
                data = json.loads(Path(p).read_text(encoding="utf-8"))
            except Exception:
                continue
            for qa in data.get("questions", []):
                q = str(qa.get("question","")).strip()
                a = str(qa.get("answer","")).strip()
                if q and a:
                    q = clip_to_tokens(tok, q, 150)
                    a = clip_to_tokens(tok, a, 200)
                    fs_qna_blocks.append(
                        f"Example:\nQuestion: {q}\nAnswer: {a}\n"
                    )
                    taken += 1
                    if taken >= FEWSHOT_PER_LANG:
                        break
            if taken >= FEWSHOT_PER_LANG:
                break

    return "\n".join(fs_summary_blocks).strip(), "\n".join(fs_json_blocks
