ï¿½{'eval_loss': 3.096264123916626, 'eval_runtime': 482.0975, 'eval_samples_per_second': 6.876, 'eval_steps_per_second': 6.876, 'epoch': 2.0}
{'train_runtime': 13764.8344, 'train_samples_per_second': 3.244, 'train_steps_per_second': 0.406, 'train_loss': 2.8212687675454213, 'epoch': 2.0}
ðŸ’¾ Saving fine-tuned adapter and tokenizer...
âœ… Training complete. Model saved at: /workspace/data/KZ_2117574/less_para_gemma/run_gemma1b_lora
