import os, json, time, random, torch, multiprocessing as mp
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ============================================================
# CONFIG
# ============================================================
BASE_MODEL   = "/workspace/data/KZ_2117574/gemma_3_1b"
TRAIN_DIR    = "/workspace/data/KZ_2117574/SharedTask_NLPAI4Health_Train&dev_set/train"
TEST_DIR     = "/workspace/data/KZ_2117574/test_data_release/test_data_release"
OUTPUT_DIR   = "/workspace/data/KZ_2117574/Baseline_Gemma_FewShot_Output"

SEED = 42
FEWSHOT_PER_LANG = 2
MODEL_MAX = 4096
RESERVE_FOR_TASK = 3072

MAX_NEW_TOKENS_SUMMARY = 300
MAX_NEW_TOKENS_JSON = 500
MAX_NEW_TOKENS_QNA = 200

TEMPERATURE_JSON = 0.3
TOP_P_JSON = 0.95
TEMPERATURE_QNA = 0.7
TOP_P_QNA = 0.9
REP_PENALTY = 1.07

LANG_HINTS = {
    "English": "English", "Hindi": "Hindi", "Gujarati": "Gujarati",
    "Bangla": "Bangla", "Assamese": "Assamese", "Kannada": "Kannada",
    "Marathi": "Marathi", "Tamil": "Tamil", "Telugu": "Telugu", "Dogri": "Dogri"
}

JSON_TEMPLATE = {
  "patient_identifiers": None,
  "demographics": {"age": None, "sex": None},
  "visit": {"date_time": None, "type": None},
  "chief_complaint": None,
  "onset_duration": None,
  "symptom_description": None,
  "aggravating_factors": None,
  "relieving_factors": None,
  "associated_symptoms": [],
  "past_medical_history": None,
  "past_surgical_history": None,
  "family_history": None,
  "current_medications": [],
  "allergies": None,
  "social_history": [],
  "functional_status": None,
  "vital_signs": None,
  "examination_findings": None,
  "investigations": [],
  "assessment_primary_diagnosis": None,
  "differential_diagnoses": [],
  "management_plan": None,
  "tests_referrals_planned": [],
  "follow_up_plan": None,
  "chronology_response_to_treatment": None,
  "patient_concerns_preferences_consent": None,
  "safety_issues_red_flags": None,
  "coding_terms": None,
  "conversation_metadata": {"timestamps": [], "speaker_labels": []}
}

# ============================================================
# HELPERS
# ============================================================
def tprint(msg): print(f"[{time.strftime('%H:%M:%S')}] {msg}", flush=True)

def safe_read_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8", errors="replace") as f:
        for line in f:
            s = line.strip()
            if not s:
                continue
            try:
                rows.append(json.loads(s))
            except json.JSONDecodeError:
                continue
    return rows

def read_text(path):
    try: return Path(path).read_text(encoding="utf-8", errors="replace").strip()
    except: return ""

def clip_to_tokens(tok, text, max_tok):
    ids = tok.encode(text, add_special_tokens=False)
    if len(ids) <= max_tok:
        return text
    ids = ids[-max_tok:]
    return tok.decode(ids, skip_special_tokens=True)

def join_dialogue_jsonl(rows):
    return "\n".join(str(r.get("dialogue", "")) if isinstance(r, dict) else str(r) for r in rows)

# ============================================================
# FEW-SHOT BUILDER
# ============================================================
def sample_fewshot_training(lang_dir, tok, budget_tokens):
    dlg_dir = lang_dir / "Dialogues"
    sum_dir = lang_dir / "Summary_Text"
    qna_dir = lang_dir / "QnA"
    fs_summary, fs_json, fs_qna = [], [], []

    if dlg_dir.exists() and sum_dir.exists():
        files = list(dlg_dir.glob("*.jsonl"))
        random.shuffle(files)
        for p in files[:FEWSHOT_PER_LANG]:
            rows = safe_read_jsonl(p)
            dial = join_dialogue_jsonl(rows)
            summ_path = sum_dir / f"{p.stem}_summary.txt"
            summ = read_text(summ_path)
            if dial and summ:
                dial = clip_to_tokens(tok, dial, min(800, budget_tokens // FEWSHOT_PER_LANG))
                fs_summary.append(f"Example:\nDialogue:\n{dial}\n\nSummary:\n{summ}\n")

    schema = json.dumps(JSON_TEMPLATE, indent=2, ensure_ascii=False)
    fs_json.append("Example:\nDialogue:\n<short dialogue>\n\nJSON:\n" + schema + "\n")

    if qna_dir.exists():
        files = list(qna_dir.glob("*.json"))
        random.shuffle(files)
        for p in files[:FEWSHOT_PER_LANG]:
            data = json.loads(Path(p).read_text(encoding="utf-8"))
            for qa in data.get("questions", []):
                q = qa.get("question", "").strip()
                a = qa.get("answer", "").strip()
                if q and a:
                    fs_qna.append(f"Example:\nQuestion: {q}\nAnswer: {a}\n")
    return "\n".join(fs_summary), "\n".join(fs_json), "\n".join(fs_qna)

# ============================================================
# PROMPTS
# ============================================================
def build_summary_text_prompt(fewshot, dialogue):
    head = "Below are examples followed by a new doctor-patient dialogue to summarize in English.\n"
    if fewshot: head += fewshot + "\n\n"
    return head + f"Now summarize the following dialogue clearly:\nDialogue:\n{dialogue}\n\nSummary:"

def build_summary_json_prompt(fewshot, dialogue):
    schema = json.dumps(JSON_TEMPLATE, indent=2, ensure_ascii=False)
    head = ("Extract structured clinical information from the dialogue below. "
            "Return ONLY valid JSON following this schema.\n")
    if fewshot: head += fewshot + "\n\n"
    return head + f"Schema:\n{schema}\n\nDialogue:\n{dialogue}\n\nJSON:"

def build_qna_prompt(fewshot, question, lang):
    head = f"Below are example patient questions and answers in {lang}.\n"
    if fewshot: head += fewshot + "\n\n"
    return head + f"Now answer this question in {lang}:\nQuestion: {question}\nAnswer:"

# ============================================================
# GENERATION
# ============================================================
def try_extract_json(text):
    text = text.strip()
    if "```" in text:
        for block in text.split("```"):
            b = block.strip()
            if b.lower().startswith("json"): b = b[4:].strip()
            try: return json.loads(b)
            except: continue
    try: return json.loads(text)
    except: pass
    start, end = text.find("{"), text.rfind("}")
    if start != -1 and end != -1:
        try: return json.loads(text[start:end+1])
        except: pass
    return JSON_TEMPLATE

def gen_text(model, tokenizer, prompt, max_new_tokens, do_sample=False, temperature=0.7, top_p=0.9):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=MODEL_MAX).to(model.device)
    with torch.inference_mode():
        out = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=do_sample,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=REP_PENALTY,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    input_len = inputs["input_ids"].shape[-1]
    gen_ids = out[0][input_len:]
    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

def gen_json(model, tokenizer, prompt):
    txt = gen_text(model, tokenizer, prompt, MAX_NEW_TOKENS_JSON, do_sample=True,
                   temperature=TEMPERATURE_JSON, top_p=TOP_P_JSON)
    obj = try_extract_json(txt)
    if not isinstance(obj, dict) or len(obj) < 3:
        strict_prompt = prompt + "\n\nReturn valid structured JSON only, matching schema."
        txt2 = gen_text(model, tokenizer, strict_prompt, MAX_NEW_TOKENS_JSON, do_sample=False)
        obj2 = try_extract_json(txt2)
        if isinstance(obj2, dict) and len(obj2) > 3:
            return obj2
        return JSON_TEMPLATE
    return obj

# ============================================================
# GPU WORKER
# ============================================================
def process_languages(rank, langs):
    torch.cuda.set_device(rank)
    device = f"cuda:{rank}"
    tprint(f"üß† [GPU {rank}] Loading base Gemma model...")

    bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_quant_type="nf4")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map={"": device},
                                                quantization_config=bnb_cfg, torch_dtype=torch.float16)
    model.eval()

    for lang_dir in langs:
        lang = lang_dir.name
        lang_hint = LANG_HINTS.get(lang, "the same language")
        out_lang = Path(OUTPUT_DIR) / lang
        dlg_dir = lang_dir / "Dialogues"
        qna_dir = lang_dir / "QnA"
        train_lang_dir = Path(TRAIN_DIR) / lang

        fs_sum, fs_json, fs_qna = ("", "", "")
        if train_lang_dir.exists():
            fs_sum, fs_json, fs_qna = sample_fewshot_training(train_lang_dir, tokenizer, 512)

        tprint(f"üîπ [GPU {rank}] Processing {lang}...")

        # Summaries
        if dlg_dir.exists():
            for p in sorted(dlg_dir.glob("*.jsonl")):
                rows = safe_read_jsonl(p)
                dial = clip_to_tokens(tokenizer, join_dialogue_jsonl(rows), RESERVE_FOR_TASK)
                text_prompt = build_summary_text_prompt(fs_sum, dial)
                json_prompt = build_summary_json_prompt(fs_json, dial)
                text_out = gen_text(model, tokenizer, text_prompt, MAX_NEW_TOKENS_SUMMARY)
                json_out = gen_json(model, tokenizer, json_prompt)
                (out_lang / "Summary_Text").mkdir(parents=True, exist_ok=True)
                (out_lang / "Summary_Json").mkdir(parents=True, exist_ok=True)
                Path(out_lang / "Summary_Text" / f"{p.stem}_summary.txt").write_text(text_out)
                Path(out_lang / "Summary_Json" / f"{p.stem}_summary.json").write_text(json.dumps(json_out, indent=2, ensure_ascii=False))

        # QnA
        if qna_dir.exists():
            for p in sorted(qna_dir.glob("*.json")):
                data = json.load(open(p, encoding="utf-8"))
                out = {"questions": []}
                for qa in data.get("questions", []):
                    q = qa.get("question", "").strip()
                    if not q: continue
                    prompt = build_qna_prompt(fs_qna, q, lang_hint)
                    ans = gen_text(model, tokenizer, prompt, MAX_NEW_TOKENS_QNA,
                                   do_sample=True, temperature=TEMPERATURE_QNA, top_p=TOP_P_QNA)
                    out["questions"].append({"question": q, "answer": ans})
                (out_lang / "QnA").mkdir(parents=True, exist_ok=True)
                Path(out_lang / "QnA" / f"{p.stem}_answers.json").write_text(json.dumps(out, indent=2, ensure_ascii=False))

    tprint(f"‚úÖ [GPU {rank}] Done with {len(langs)} languages!")

# ============================================================
# MAIN
# ============================================================
def main():
    torch.set_grad_enabled(False)
    random.seed(SEED)
    all_langs = [p for p in Path(TEST_DIR).iterdir() if p.is_dir()]
    ngpus = torch.cuda.device_count()
    tprint(f"üöÄ {ngpus} GPUs detected. Total languages: {len(all_langs)}")

    chunks = [all_langs[i::ngpus] for i in range(ngpus)]
    mp.set_start_method("spawn", force=True)
    procs = []

    for rank, lang_chunk in enumerate(chunks):
        p = mp.Process(target=process_languages, args=(rank, lang_chunk))
        p.start()
        procs.append(p)

    for p in procs:
        p.join()

    tprint(f"üèÅ All GPUs finished. Outputs saved to {OUTPUT_DIR}")

if __name__ == "__main__":
    main()
