model = get_peft_model(model, lora_cfg)
model.print_trainable_parameters()

# üîß Critical fix for 'does not require grad' issue
try:
    model.gradient_checkpointing_disable()
    model.config.use_cache = False
    print("‚úÖ Disabled gradient checkpointing and caching to ensure gradient flow.")
except Exception as e:
    print("‚ö†Ô∏è Could not disable gradient checkpointing:", e)
