Below is a clean, publication-ready Experiments section, written in the style and tone expected by an EACL reviewer.
I have made all assumptions explicit, added the fine-tuning details, and kept claims conservative and reproducible.

⸻

4 Experiments

4.1 Task Definition

We study multilingual clinical dialogue summarization, where the input is a doctor–patient conversation and the output is a plain-text English clinical summary.
Dialogues may be in English, Hindi, Marathi, or Bangla, while all reference summaries are written in English. The task requires both cross-lingual understanding and domain-specific summarization without hallucinating clinical facts.

⸻

4.2 Models Evaluated

We evaluate three large language models representative of recent instruction-tuned open-weight systems:
	•	LLaMA-2-13B-Chat
	•	Mistral-7B-Instruct
	•	Qwen2.5-7B-Instruct

These models were selected to cover different model families and sizes while remaining feasible for deployment on a single 32 GB GPU.

⸻

4.3 Inference Settings: Zero-Shot and Few-Shot

For all three models, we conduct both zero-shot and few-shot inference.

Zero-Shot Inference
In the zero-shot setting, the model is prompted only with a system instruction and the target dialogue. The prompt instructs the model to generate a concise English clinical summary, focusing on diagnosis, symptoms, investigations, treatment, and follow-up, without introducing unsupported medical claims.

Few-Shot Inference
In the few-shot setting, we prepend k = 3 in-context examples to the prompt. Each example consists of:
	•	a doctor–patient dialogue, and
	•	its corresponding gold English summary.

Few-shot examples are sampled from the training split and are language-matched to the test dialogue when possible. Example dialogues are truncated to ensure the total prompt remains within the model’s context window.
Few-shot prompting is applied uniformly across all three models to enable a fair comparison.

⸻

4.4 Fine-Tuning Setup

In addition to prompt-based inference, we fine-tune Qwen2.5-7B-Instruct for domain adaptation.

Training Data
We use a corpus of doctor–patient dialogues across four languages (English, Hindi, Marathi, Bangla). All target summaries are in English.
The original training data is split into train and test subsets, while a provided development set is retained for validation.

Fine-Tuning Method
We employ parameter-efficient fine-tuning (PEFT) using QLoRA:
	•	Base model: Qwen2.5-7B-Instruct
	•	Quantization: 4-bit NF4
	•	LoRA rank: 8
	•	LoRA target modules: query, key, value, and output projections
	•	Optimizer: AdamW
	•	Learning rate: 2 \times 10^{-4}
	•	Batch size: 1 with gradient accumulation
	•	Maximum sequence length: 1,536 tokens
	•	Training epochs: 1

Only LoRA adapter parameters are updated, keeping the base model frozen. This allows efficient training on a single NVIDIA V100 32 GB GPU.

⸻

4.5 Fine-Tuning Variants

We explore two fine-tuning variants of Qwen2.5-7B-Instruct.

4.5.1 Plain-Text Summary Fine-Tuning
In the first variant, the model is fine-tuned to generate plain-text English summaries directly from multilingual dialogues.
The training prompt explicitly instructs the model to produce an English summary regardless of the input language. At inference time, we perform zero-shot generation using the fine-tuned model.

4.5.2 Reasoning-Structured Fine-Tuning
In the second variant, we restructure the training targets to encourage implicit reasoning.
Intermediate reasoning markers present in the data are removed from the output, forcing the model to internalize reasoning while still producing a clean final summary.
This variant is also evaluated using zero-shot inference, allowing us to isolate the effect of output structuring during training.

⸻

4.6 Evaluation Protocol

We evaluate all models on the same held-out test set. Since gold summaries are available only for the train and dev portions, the test split is derived from the original training data to enable controlled evaluation.

We report results separately for:
	•	zero-shot prompting,
	•	few-shot prompting, and
	•	fine-tuned models.

Automatic evaluation metrics are complemented by qualitative analysis, focusing on factual correctness, clinical relevance, and linguistic fluency across languages.

⸻

4.7 Experimental Goals

Our experiments are designed to answer the following questions:
	1.	How do zero-shot and few-shot prompting compare across different model families?
	2.	Does domain-specific fine-tuning improve clinical summarization quality over prompting alone?
	3.	Does restructuring training outputs to encourage implicit reasoning lead to better summaries?

⸻

Reviewer-style closing remark

Overall, this experimental setup allows us to disentangle the effects of model choice, prompting strategy, and fine-tuning design on multilingual clinical dialogue summarization.

⸻

If you want, I can next:
	•	Add tables placeholders (“Table 1 shows…”)
	•	Write a Results section with expected trends
	•	Tighten this to fit a strict EACL page limit
	•	Or adapt the tone to ACL / EMNLP style instead
