# ============================================================
# OPTIONAL: Convert checkpoint to safetensors (for safety + resume)
# ============================================================
import os
import torch
from transformers import AutoModelForCausalLM
from peft import PeftModel

def convert_checkpoint_to_safetensors(src_path, dst_path):
    """
    Converts a model checkpoint to safetensors format safely.
    Only converts model weights â€” optimizer states are ignored (for PEFT fine-tuning).
    """
    os.makedirs(dst_path, exist_ok=True)

    print(f"ðŸ”„ Converting checkpoint from {src_path} to {dst_path} (safetensors format)...")

    # Load the model from the checkpoint
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        device_map="auto",
        quantization_config=bnb_cfg
    )
    model = PeftModel.from_pretrained(model, src_path)

    # Save it as safetensors
    model.save_pretrained(dst_path, safe_serialization=True)

    print(f"âœ… Safetensors checkpoint saved at: {dst_path}")
    return dst_path


# Example usage:
latest_ckpt = "/workspace/data/KZ_2117574/gemma1b_qlora_multilingual_finetune_fast/checkpoint-7000"
safe_ckpt = latest_ckpt + "-safe"

# Convert only if safetensors folder doesnâ€™t exist yet
if not os.path.exists(safe_ckpt):
    convert_checkpoint_to_safetensors(latest_ckpt, safe_ckpt)
else:
    print(f"âš¡ Using existing safetensors checkpoint: {safe_ckpt}")
