üß† 1. Introduction (Expanded, 2.5 pages)

Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023), Gemini (Reid et al., 2024), and Med-PaLM 2 (Singhal et al., 2023) have demonstrated unprecedented progress in natural language understanding, reasoning, and generalization across a wide range of tasks, including medicine. They have achieved performance comparable to clinicians in medical question-answering benchmarks (Kung et al., 2023; Nori et al., 2023), suggesting potential use in clinical support, documentation, and patient triage (Lee et al., 2024). However, the same properties that make LLMs powerful‚Äîscale, open-endedness, and fluency‚Äîalso render them unreliable in domains requiring factual precision and ethical accountability (Ji et al., 2023; Begoli et al., 2019).

The integration of LLMs into healthcare poses unique challenges that extend beyond predictive accuracy. Medicine is not only a domain of information retrieval but one of explanation, justification, and trust. Clinicians make decisions based on causal reasoning, evidence weighting, and context sensitivity‚Äîabilities that current LLMs only partially emulate. When an AI system proposes a diagnosis or prescription, its credibility depends as much on why it reached a conclusion as on what it predicts (Tonekaboni et al., 2019; Amann et al., 2020). Without a transparent reasoning process, even correct answers are unsafe for clinical use.

Despite progress in alignment and red-teaming (Mireshghallah et al., 2024; Zhou et al., 2024), existing clinical LLMs remain limited in their trustworthiness‚Äîa multidimensional property encompassing accuracy, consistency, safety, and explainability (Wiens et al., 2019; Xu et al., 2023). Models often produce internally inconsistent reasoning (Bang et al., 2023), express unjustified confidence (Kadavath et al., 2022), or fail to recognize uncertainty in ambiguous cases. This phenomenon‚Äîknown as epistemic overconfidence‚Äîis particularly problematic in medicine, where acknowledging ‚ÄúI don‚Äôt know‚Äù is a marker of safety, not failure.

Efforts to align models for healthcare have emphasized safety and ethical compliance. For instance, the World Health Organization (WHO, 2021) and the European Commission (2023) have outlined governance frameworks emphasizing transparency, fairness, and accountability as essential prerequisites for AI in health. Yet these frameworks rarely provide operational metrics for trustworthiness. Similarly, commercial and academic systems such as Med-PaLM (Singhal et al., 2023), BioGPT (Luo et al., 2022), and ChatDoctor (Li et al., 2023) showcase impressive factual accuracy but offer limited visibility into why they reach conclusions or how stable their reasoning is under slight prompt variations.

Our prior system, MedGuard (Sharma et al., 2024), introduced a safety-first approach to clinical LLM simulation. It employed a multi-agent architecture‚Äîcomprising a Doctor, Patient, Measurement, and Safety Agent‚Äîto replicate clinical reasoning while monitoring unsafe test or prescription behaviors. MedGuard significantly reduced unsafe recommendations by introducing safety checkpoints, including drug‚Äìdrug interaction (DDI) detection and risk-aware test assessment. However, MedGuard focused primarily on output validation rather than process transparency. It answered the question: ‚ÄúIs the result safe?‚Äù but not ‚ÄúCan we trust how the model got there?‚Äù

In this paper, we present MedGuard-X, an evolution of the MedGuard framework toward trustworthy and explainable clinical reasoning. MedGuard-X introduces three new layers of interpretability and reliability:
	1.	Consensus Reasoning: Multiple independent Doctor Agents collaborate and vote on diagnostic decisions, enabling quantification of epistemic uncertainty via inter-model disagreement.
	2.	Reasoning Trace Explainability: Each diagnostic step includes an explicit reasoning trace (‚Äú<thinking_process>‚Äù) that links model actions to medical evidence and tests.
	3.	Trustworthiness Metrics: Quantitative measures such as Consensus Disagreement Rate (CDR) and Justification Alignment (JA) are introduced to correlate reasoning transparency with diagnostic accuracy.

This work thus shifts the focus from safety verification to epistemic accountability. MedGuard-X not only asks whether the model is right but investigates how it reasons, when it hesitates, and whether its explanations are grounded in evidence. By analyzing reasoning traces, differential evolution, and consensus patterns, we demonstrate that even highly aligned LLMs remain susceptible to confident hallucinations‚Äîoffering rational-sounding but clinically incoherent explanations.

We argue that explainability and trustworthiness should be viewed as orthogonal properties. A model can be accurate yet opaque, or transparent yet wrong. Our findings suggest that LLMs, while explainable on the surface, often lack causal fidelity‚Äîthe degree to which their explanations reflect true underlying reasoning (Doshi-Velez & Kim, 2017; Holzinger et al., 2022). Therefore, the path toward trustworthy medical AI requires both transparency in reasoning and mechanisms for self-auditing, uncertainty quantification, and cross-model consensus validation.

This paper contributes to that vision through MedGuard-X ‚Äî a pipeline that operationalizes trust as a measurable, explainable, and testable property of large-scale clinical reasoning systems.

‚∏ª

üìö 2. Background and Related Work (Expanded, ~2.5 pages)

2.1 Explainability in Medical AI

Explainable AI (XAI) aims to make black-box models interpretable and accountable (Doshi-Velez & Kim, 2017; Gilpin et al., 2018). In healthcare, explainability is especially critical because opaque models can erode clinician confidence and increase malpractice risk (Tonekaboni et al., 2019; Amann et al., 2020). Early approaches relied on feature importance and surrogate models‚Äîsuch as SHAP (Lundberg & Lee, 2017) and LIME (Ribeiro et al., 2016)‚Äîto provide local explanations for risk prediction models. While effective for structured data, these methods are ill-suited for reasoning-driven tasks like diagnosis or triage, where causal relationships unfold over dialogue.

Recent reviews (Ahmad et al., 2018; Holzinger et al., 2022) highlight that medical explainability requires causability‚Äîa human-understandable mapping between data and decision. Clinicians demand not just ‚Äúwhy‚Äù but ‚Äúunder what evidence and constraints.‚Äù However, LLM explanations, even when prompted to ‚Äúshow reasoning,‚Äù often produce post-hoc rationalizations that differ from the true generative process (Turpin et al., 2023). This discrepancy underscores a key tension: explainability can be simulated without genuine understanding.

Emerging medical dialogue models, such as DoctorGPT (Yang et al., 2024) and MedDialog (Chen et al., 2024), attempt to address this by including structured reasoning tags (e.g., <thinking_process>) or intermediate reflection steps. However, these systems remain single-model frameworks, lacking external validation or consensus checks. Consequently, they remain vulnerable to ‚Äúexplanation bias,‚Äù where the LLM fabricates coherent but false justifications (Ji et al., 2023).

‚∏ª

2.2 Trustworthiness and Safety Frameworks

Trustworthy AI in healthcare requires not only interpretable outputs but also robust safeguards against harmful or misleading behavior. The EU AI Act (European Commission, 2023) and the WHO guidance on AI ethics (WHO, 2021) classify medical AI as a ‚Äúhigh-risk system,‚Äù necessitating traceability, transparency, and human oversight. Similar principles appear in the US FDA‚Äôs Good Machine Learning Practice (FDA, 2023) and IEEE P7000 series on ethical AI (IEEE, 2021).

Academic frameworks such as ‚ÄúDo No Harm‚Äù (Wiens et al., 2019) and ‚ÄúSafe AI in Medicine‚Äù (Rajpurkar et al., 2022) advocate for layered safety verification‚Äîcombining algorithmic constraints, monitoring agents, and human feedback. Our earlier work, MedGuard (Sharma et al., 2024), followed this philosophy by introducing a multi-agent setup with distinct safety agents to audit tests, prescriptions, and reasoning chains. Yet safety is only one dimension of trustworthiness: an unsafe but explainable model may still fail ethically if its reasoning is inconsistent, misleading, or opaque.

The growing field of LLM safety introduces additional challenges. As noted by Kadavath et al. (2022) and Ganguli et al. (2023), LLMs exhibit high epistemic calibration error‚Äîthey frequently express confidence disproportionate to correctness. This makes confidence estimation and self-consistency critical to trust calibration. Recent work such as TrustGPT (Liao et al., 2024) and Safe-MedPrompt (Zhou et al., 2024) propose quantitative trust metrics, but these rely on external red-teaming rather than intrinsic reasoning audits.

‚∏ª

2.3 Consensus and Multi-Agent Reliability

Multi-agent LLM systems are an emerging approach to enhance reasoning robustness. Self-Consistency (Wang et al., 2022) and Reflexion (Shinn et al., 2023) demonstrated that generating multiple reasoning paths and selecting the most consistent improves factual correctness. Multi-Agent Debate (Liang et al., 2023) and Socratic Reasoning (Weng et al., 2024) further show that inter-agent dialogue can mitigate hallucinations. In the clinical domain, few systems have applied this paradigm‚Äîan exception being MedPrompt-R (Tang et al., 2024), which uses dual-agent reasoning for radiology QA.

MedGuard-X extends this paradigm to consensus-based trust quantification: three doctor agents reason independently, and their disagreements serve as a measure of epistemic instability. This approach transforms consensus from a qualitative signal into a quantitative trust metric‚Äîbridging explainability and reliability.

‚∏ª

2.4 Explainability‚ÄìTrust Trade-offs

Several studies have observed that increasing model transparency can paradoxically reduce trust if explanations expose inconsistency or irrelevance (Lakkaraju et al., 2020; Poursabzi-Sangdeh et al., 2021). In medicine, such a paradox may manifest when an AI provides an interpretable but clinically illogical rationale. Our experiments confirm this effect: explainability improves oversight but does not guarantee correctness.

The causability framework (Holzinger et al., 2022) argues that true explainability requires faithful causal reasoning‚Äîsomething current LLMs lack. Therefore, trust must be seen as a layered property, comprising safety, explainability, consensus, and epistemic humility. MedGuard-X operationalizes these principles by measuring how much LLMs agree with themselves, how consistent their explanations are with evidence, and how transparently they handle uncertainty
