Code
import torch, bitsandbytes as bnb, transformers
print(torch.__version__)
print(torch.version.cuda)      # None = CPU build; must match your system CUDA
print(bnb.__version__)
print(transformers.__version__)

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

from huggingface_hub import login
def huggingface_login():
    """
    Logs into the Hugging Face Hub using the user's token.
    """
    # print("Please enter your Hugging Face token. You can generate one at: https://huggingface.co/settings/tokens")
    token = ''
    try:
        # Log in to the Hugging Face Hub
        login(token=token)
        print("Successfully logged into Hugging Face!")
    except Exception as e:
        print(f"Failed to log in: {e}")

huggingface_login()


MODEL_NAME = "meta-llama/Llama-2-13b"

# Verify CUDA availability
use_cuda = torch.cuda.is_available()
device = "cuda" if use_cuda else "cpu"
print(f"\nUsing device: {device}\n")

# 4-bit quantization config (requires bitsandbytes)
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,  # fp16 compute for speed/compat
)

print(f"Loading model: {MODEL_NAME}\n")
# device_map="auto" lets HF place weights across GPUs/CPU appropriately
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    quantization_config=bnb_cfg,
    trust_remote_code=True,
    low_cpu_mem_usage=True,
)

# Tokenizer (Qwen requires its tokenizer for generation)
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True,
)

print("\n--- Attention related modules ---\n")
for name, module in model.named_modules():
    lname = name.lower()
    if any(x in lname for x in ["attn", "proj", "q", "k", "v"]):
        print(name)


Error

Successfully logged into Hugging Face!

Using device: cuda

Loading model: meta-llama/Llama-2-13b


�    model = AutoModelForCausalLM.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained

�    return model_class.from_pretrained(
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 277, in _wrapper

�    return func(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 4900, in from_pretrained

�    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
  File "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py", line 1148, in _get_resolved_checkpoint_files

�    raise OSError(
OSError: meta-llama/Llama-2-13b does not appear to have a file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt or flax_model.msgpack.
