/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[7], line 1
----> 1 from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
      2 import os, json
      4 # Paths

File <frozen importlib._bootstrap>:1075, in _handle_fromlist(module, fromlist, import_, recursive)

File /usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:2317, in _LazyModule.__getattr__(self, name)
   2315 elif name in self._class_to_module:
   2316     try:
-> 2317         module = self._get_module(self._class_to_module[name])
   2318         value = getattr(module, name)
   2319     except (ModuleNotFoundError, RuntimeError) as e:

File /usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:2347, in _LazyModule._get_module(self, module_name)
   2345     return importlib.import_module("." + module_name, self.__name__)
   2346 except Exception as e:
-> 2347     raise e

File /usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:2345, in _LazyModule._get_module(self, module_name)
   2343 def _get_module(self, module_name: str):
   2344     try:
-> 2345         return importlib.import_module("." + module_name, self.__name__)
   2346     except Exception as e:
   2347         raise e

File /usr/lib/python3.10/importlib/__init__.py:126, in import_module(name, package)
    124             break
    125         level += 1
--> 126 return _bootstrap._gcd_import(name[level:], package, level)

File /usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py:64
     51 from .base import (
     52     ArgumentHandler,
     53     CsvPipelineDataFormat,
   (...)
     61     infer_framework_load_model,
     62 )
     63 from .depth_estimation import DepthEstimationPipeline
---> 64 from .document_question_answering import DocumentQuestionAnsweringPipeline
     65 from .feature_extraction import FeatureExtractionPipeline
     66 from .fill_mask import FillMaskPipeline

File /usr/local/lib/python3.10/dist-packages/transformers/pipelines/document_question_answering.py:30
     21 from ..utils import (
     22     ExplicitEnum,
     23     add_end_docstrings,
   (...)
     27     logging,
     28 )
     29 from .base import ChunkPipeline, build_pipeline_init_args
---> 30 from .question_answering import select_starts_ends
     33 if is_vision_available():
     34     from PIL import Image

File /usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:9
      5 from typing import TYPE_CHECKING, Optional, Union
      7 import numpy as np
----> 9 from ..data import SquadExample, SquadFeatures, squad_convert_examples_to_features
     10 from ..modelcard import ModelCard
     11 from ..tokenization_utils import PreTrainedTokenizer

File /usr/local/lib/python3.10/dist-packages/transformers/data/__init__.py:15
      1 # Copyright 2020 The HuggingFace Team. All rights reserved.
      2 #
      3 # Licensed under the Apache License, Version 2.0 (the "License");
   (...)
     12 # See the License for the specific language governing permissions and
     13 # limitations under the License.
---> 15 from .data_collator import (
     16     DataCollatorForLanguageModeling,
     17     DataCollatorForMultipleChoice,
     18     DataCollatorForPermutationLanguageModeling,
     19     DataCollatorForSeq2Seq,
     20     DataCollatorForSOP,
     21     DataCollatorForTokenClassification,
     22     DataCollatorForWholeWordMask,
     23     DataCollatorWithFlattening,
     24     DataCollatorWithPadding,
     25     DefaultDataCollator,
     26     default_data_collator,
     27 )
     28 from .metrics import glue_compute_metrics, xnli_compute_metrics
     29 from .processors import (
     30     DataProcessor,
     31     InputExample,
   (...)
     45     xnli_tasks_num_labels,
     46 )

File /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:763
    757             batch["decoder_input_ids"] = decoder_input_ids
    759         return batch
    762 @dataclass
--> 763 class DataCollatorForLanguageModeling(DataCollatorMixin):
    764     """
    765     Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they
    766     are not all of the same length.
   (...)
    823     </Tip>
    824     """
    826     tokenizer: PreTrainedTokenizerBase

File /usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:1233, in DataCollatorForLanguageModeling()
   1228     # The rest of the time (10% of the time) we keep the masked input tokens unchanged
   1229     return inputs, labels
   1231 @staticmethod
   1232 def _calc_word_ids_and_prob_mask(
-> 1233     offsets: np.ndarray[np.ndarray[tuple[int, int]]], special_tokens_mask: np.ndarray[np.ndarray[int]]
   1234 ) -> tuple[np.ndarray[np.ndarray[int]], np.ndarray[np.ndarray[int]]]:
   1235     """
   1236     Map tokens to word ids and create mask of tokens to not mask.
   1237     Tokens that are part of the same word will have the same word id and we will only
   1238     set a mask probability for the first token of each word.
   1239     """
   1241     token_starts = offsets[:, :, 0]

TypeError: Too few arguments for numpy.ndarray
