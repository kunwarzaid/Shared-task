\section{Introduction}

Large Language Models (LLMs) such as GPT-4 \cite{achiam2023gpt}, Gemini \cite{team2023gemini}, and Med-PaLM 2 \cite{singhal2025toward} have demonstrated unprecedented progress in natural language understanding, reasoning, and generalization across a wide range of tasks, including medicine. They have achieved performance comparable to clinicians on medical question-answering benchmarks \cite{kung2023performance,nori2023capabilities}, suggesting potential utility in clinical decision support, documentation, and patient triage \cite{lee2023benefits}. However, the very properties that make LLMs powerful---scale, open-endedness, and linguistic fluency---also make them unreliable in domains demanding factual precision, accountability, and causal reasoning \cite{ji2023survey,begoli2019need}. 

Integrating LLMs into healthcare introduces challenges that extend far beyond raw accuracy. Medicine is not only about prediction but also about explanation, justification, and trust. Clinicians reason based on causality, uncertainty, and evidence weighting---capabilities that current LLMs only partially emulate. When an AI model provides a diagnosis or recommends a treatment, its credibility depends as much on \emph{why} it reached that conclusion as on \emph{what} it predicts \cite{tonekaboni2019clinicians,amann2020explainability}. Without a transparent reasoning process, even correct answers are unsafe for clinical use because the underlying rationale may be spurious or unverifiable.

Despite the advances in alignment and red-teaming efforts \cite{mei2023assert}, clinical LLMs still exhibit fundamental limitations in trustworthiness---a composite property encompassing accuracy, consistency, safety, and interpretability \cite{a2019,huang2025survey}. Many models produce internally inconsistent explanations, display unjustified confidence \cite{kadavath2022language}, or fail to acknowledge uncertainty in ambiguous cases. This epistemic overconfidence poses a severe risk in medicine, where being ``unsure'' can be safer than being confidently wrong.

International regulatory bodies, including the World Health Organization \cite{guidance2021ethics} and the European Commission \cite{bomhard2021regulation}, have stressed the importance of transparency, fairness, and accountability in medical AI. Yet, such frameworks are often high-level, lacking concrete operational metrics for evaluating trustworthiness. Similarly, prominent medical LLMs such as Med-PaLM 2 \cite{singhal2025toward}, BioGPT \cite{luo2022biogpt}, and ChatDoctor \cite{li2023chatdoctor} achieve strong factual accuracy but offer limited insight into their reasoning stability or causal consistency. Minor variations in prompts or phrasing can yield significantly different diagnostic outcomes, challenging the reliability of such systems for clinical use.

Our previous system, \textbf{MedGuard}, addressed some of these safety concerns through a multi-agent simulation that mirrored real-world clinical reasoning. The system featured distinct Doctor, Patient, Measurement, and Safety Agents, allowing for continuous risk assessment during diagnosis and prescription. MedGuard successfully reduced unsafe outputs by embedding safety checkpoints such as drug--drug interaction (DDI) detection and risk-aware test evaluation. However, MedGuard primarily focused on verifying the \emph{outcomes} of reasoning rather than the \emph{processes} that led to them. It asked, “Is the result safe?” but not, “Can we trust how the model reached this result?”

In this paper, we introduce \textbf{MedGuard-X}, an evolution of the MedGuard framework designed to move beyond safety validation toward comprehensive \emph{trustworthy and explainable clinical reasoning}. MedGuard-X enhances transparency and reliability through three core innovations:
\begin{itemize}
    \item \textbf{Consensus Reasoning:} Multiple independent Doctor Agents collaborate on each case, voting on diagnostic decisions. The resulting \emph{Consensus Disagreement Rate (CDR)} quantifies epistemic uncertainty and reveals when LLMs diverge in their reasoning.
    \item \textbf{Reasoning Trace Explainability:} Each diagnostic step includes an explicit reasoning trace (\texttt{<thinking\_process>}) linking model actions to evidence, patient responses, and ordered tests. This mechanism allows reconstruction of the decision path from question to conclusion.
    \item \textbf{Trustworthiness Metrics:} We introduce measurable indicators such as \emph{Justification Alignment (JA)} and CDR to quantify how consistent reasoning transparency correlates with diagnostic accuracy.
\end{itemize}

This work thus reframes clinical AI evaluation around \emph{epistemic accountability}. MedGuard-X not only assesses whether a model is correct but also analyzes how it reasons, when it hesitates, and whether its justifications align with medical evidence. By analyzing reasoning traces, differential evolution, and consensus patterns, we show that even highly aligned medical LLMs remain prone to confident hallucinations---providing persuasive but clinically incoherent explanations. 

We argue that \emph{explainability} and \emph{trustworthiness} are orthogonal properties. A model can be accurate yet opaque, or transparent yet incorrect. Our findings support prior work suggesting that most LLMs, while capable of surface-level explanations, lack \emph{causal fidelity}---the degree to which explanations truthfully represent the model’s reasoning process \cite{doshi2017towards,holzinger2022toward}. Building trustworthy medical AI therefore requires not only reasoning transparency but also mechanisms for self-auditing, uncertainty quantification, and cross-agent consensus validation. MedGuard-X operationalizes these ideas by combining real-time safety analysis, structured reasoning traces, and explainability logs into a unified, auditable framework for clinical reasoning.

\section{Related Work}

Explainable AI (XAI) aims to make complex models interpretable and accountable \cite{doshi2017towards,gilpin2018explaining}. In healthcare, this goal is particularly critical: opaque decision systems can erode clinician confidence, increase legal liability, and obscure the rationale behind life-altering decisions \cite{tonekaboni2019clinicians,amann2020explainability}. Traditional explainability methods such as LIME \cite{ribeiro2016should} and SHAP \cite{lundberg2017unified} quantify local feature importance, while Grad-CAM \cite{selvaraju2017grad} visualizes activation saliency in neural networks. These techniques are useful for static, structured predictions but ill-suited for diagnostic reasoning tasks that unfold dynamically across multiple dialogue turns. Unlike image or risk models, medical dialogues require contextual reasoning, iterative hypothesis generation, and justification chaining—all of which demand process-level transparency rather than static attribution.

Recent literature on LLM explainability introduces reasoning transparency through techniques such as \emph{chain-of-thought prompting} \cite{wei2022chain} and \emph{self-reflection} \cite{kojima2022large}. While these methods provide insight into the model’s internal reasoning, their explanations remain self-generated and unverifiable. As highlighted by Turpin et al. \cite{turpin2023language}, models often fabricate plausible reasoning to justify conclusions, producing what has been termed “rationalization without understanding.” This challenge is especially acute in medicine, where incorrect reasoning can lead to unsafe or unethical outcomes even if the final answer is correct. Mireshghallah et al. \cite{mireshghallah2023privacy} and Zhou et al. \cite{zhou2023certified} argue that explanations must be auditable and grounded in verifiable process traces rather than purely textual justifications.

LLM-based clinical dialogue systems, such as ChatDoctor \cite{li2023chatdoctor}, Clinical-Camel \cite{wang2023clinicalcamel}, DoctorGPT \cite{yang2024doctorgpt}, and MedDialog \cite{chen2024meddialog}, have extended this paradigm by modeling interactive reasoning between a doctor and a patient. These systems emulate real consultations, allowing the model to query, hypothesize, and update diagnoses iteratively. However, they generally remain single-agent systems without explicit safety or explainability integration. While they can produce medically coherent conversations, their reasoning remains opaque and unaudited, making it difficult to identify when errors emerge or why a specific test or treatment was selected. AgentClinic \cite{ma2024agentclinic} partially addresses this by modeling doctor–patient interactions through agent collaboration, but it still lacks structured traceability and systematic uncertainty quantification.

Parallel efforts to ensure safety and alignment in clinical LLMs have emphasized risk mitigation, ethical compliance, and human oversight. Studies by Jiang et al. (2023) and Wu et al. (2024) demonstrate that medical LLMs can hallucinate unsafe procedures or produce contraindicated prescriptions. Frameworks such as GuardMed \cite{peters2023safeaimed}, AuditTrailAI \cite{kim2024audittrail}, and SafetyBench \cite{ghosh2024alignment} propose monitoring tools or red-teaming protocols but often focus on post-hoc safety evaluation rather than real-time safety reasoning. Regulatory bodies such as the FDA \cite{fda2023ai} and WHO \cite{guidance2021ethics} emphasize traceability and human-in-the-loop review, but few technical implementations provide actionable transparency into model decision pathways.

MedGuard-X bridges these gaps by embedding explainability, safety analysis, and uncertainty quantification directly into the reasoning pipeline. Every agent interaction—whether a question, test request, or prescription—is logged in a structured record containing timestamp, actor type, reasoning snippet, and decision rationale. This \emph{Explainability and Logging Layer} provides a chronological audit trail that reconstructs how diagnoses emerge from dialogue, tests, and safety deliberations. Unlike prior XAI approaches that rely on retrospective interpretations, MedGuard-X offers \emph{process-level explainability}: the ability to inspect, evaluate, and verify the reasoning process as it unfolds.

By combining multi-agent collaboration, safety-aware reasoning, and transparent logging, MedGuard-X advances the state of explainable clinical AI beyond both static interpretability and monolithic alignment. It redefines trustworthiness as an operational property—measurable, auditable, and empirically linked to reasoning coherence. In doing so, it demonstrates that safety and explainability are not competing goals but interdependent components of trustworthy clinical reasoning.
