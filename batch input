\documentclass[11pt]{article}
\usepackage[final]{acl} % camera-ready version

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{caption}
\usepackage{tikz}
\usepackage{float}
\usepackage{placeins} % added for \FloatBarrier
\usepackage{natbib}
\usepackage{url}

\renewcommand{\UrlFont}{\ttfamily\small}

% spacing tweaks
\setlength{\parskip}{4pt}
\setlength{\abovecaptionskip}{3pt}
\setlength{\belowcaptionskip}{3pt}
\setlength{\textfloatsep}{8pt plus 1pt minus 2pt}
\setlength{\floatsep}{8pt plus 1pt minus 2pt}
\setlength{\intextsep}{8pt plus 1pt minus 2pt}

\title{Multilingual Clinical Dialogue Summarization and Information Extraction with Qwen-1.5B LoRA}

\author{
\begin{tabular}{c}
\textbf{Kunwar Zaid} \quad
\textbf{Amit Sangroya} \quad
\textbf{Jyotsana Khatri} \\
TCS Research, New Delhi, India \\
\texttt{\{kunwar.zaid, amit.sangroya, jyotsana.khatri\}@tcs.com} \\
\end{tabular}
}

\begin{document}
\maketitle

\begin{abstract}
This paper describes our submission to the NLP-AI4Health 2025 Shared Task on multilingual clinical dialogue summarization and structured information extraction.
Our system is based on Qwen-1.5B Instruct fine-tuned with LoRA adapters for parameter-efficient adaptation.
The pipeline produces (i) concise English summaries, (ii) schema-aligned JSON outputs, and (iii) multilingual Q\&A responses.
The Qwen-based approach substantially improves summary fluency, factual completeness, and JSON field coverage while maintaining efficiency within constrained GPU resources.
\end{abstract}

\section{Introduction}
The Shared Task on multilingual clinical dialogue summarization challenges systems to process doctor–patient conversations across ten languages and output three modalities: summaries, structured records, and Q\&A responses.
We present a LoRA-adapted Qwen-1.5B pipeline optimized for factual summarization and schema-based information extraction, designed to handle multilingual inputs efficiently under limited hardware conditions.

\section{System Architecture and Approach}

\begin{figure}[htbp!]
\centering
\includegraphics[width=1.2\columnwidth]{latex/Multilingual Doctor-Patient Dialogue.png}
\caption{Overview of the multilingual summarization and extraction pipeline. The pipeline includes English summarization, structured information extraction, and multilingual Q\&A generation.}
\label{fig:struct}
\end{figure}

Figure~\ref{fig:struct} illustrates the modular inference design. Each dialogue passes through sequential stages: English summarization, structured field extraction, and multilingual Q\&A generation.

\subsection{Model Configuration}
We used Qwen-1.5B Instruct quantized to 4-bit NF4 precision via \texttt{BitsAndBytes} \citep{dettmers2023bitsandbytes}.
LoRA adapters \citep{hu2021lora} were trained with rank $r=8$, $\alpha=32$, dropout $0.05$, and target modules \texttt{q\_proj} and \texttt{v\_proj}.
Training used the AdamW optimizer ($2\times10^{-4}$ learning rate, cosine decay).
Gradient checkpointing and mixed precision allowed training within 60GB RAM and 32×V100 GPUs.

\paragraph{Training Details.}
Fine-tuning was conducted for \textbf{one epoch} due to strict time and hardware constraints. Despite this, validation showed rapid convergence, indicating effective domain adaptation. 

\subsection{Inference Pipeline}
Each language’s dialogues were processed independently with checkpoint resumption support.
The inference proceeds through:
\begin{enumerate}
    \item \textbf{Summary Generation:} Produce an English summary ending with sentinel token \texttt{<<END>>}.
    \item \textbf{Structured Extraction:} Populate each JSON field by querying the model separately.
    \item \textbf{Multilingual Q\&A:} Generate answers in the dialogue’s original language.
\end{enumerate}
Greedy decoding (\texttt{do\_sample=False}) ensures stable, deterministic outputs across runs.

\subsection{Prompt Design for Inference}
The system employs role-based prompts for consistent and interpretable outputs across all subtasks.  
Distinct templates were used for summary generation, structured JSON extraction, and multilingual Q\&A.  
Each follows a clear \textit{System–User} structure to improve controllability and ensure coherent task behavior.

% (prompts unchanged)
% -------------------------------------------------------------
% (keeping your entire section exactly the same)
% -------------------------------------------------------------

\subsection{Field-by-Field JSON Extraction}
Early experiments with single-shot JSON generation—where the model was prompted to fill the entire schema in one response—consistently failed to produce usable outputs.
Most fields were returned as \texttt{null} or empty strings, and the overall structure often violated JSON syntax.
This occurred because large language models tend to lose schema consistency across multiple nested fields when generating long structured outputs.

To address this issue, we adopted a field-by-field extraction strategy.
Each JSON field was reformulated as an independent \textit{question–answer} task, allowing the model to focus on one piece of information at a time.
For example:
\begin{quote}\small
Q: What is the patient’s chief complaint? \\
A: Persistent throat discomfort and hoarseness for two months.
\end{quote}

Once the model generated an answer for each field, a lightweight Python post-processing script automatically reconstructed the full JSON object.
Each field’s text response was inserted into its corresponding key, ensuring schema validity and non-null entries.
If the answer contained phrases such as “N/A,” “not mentioned,” or was empty, the script defaulted that field to \texttt{null}.

This modular approach improved the completeness and consistency of structured outputs, enabling selective regeneration of missing or low-confidence fields without re-running the entire inference pipeline.
By decoupling schema adherence from natural language reasoning, the system produced well-formed, information-rich JSON records across all ten languages.

\begin{table}[htbp!]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lp{0.3\textwidth}}
\toprule
\textbf{Field} & \textbf{Example Q-A Pair ($\leq$12 words)} \\
\midrule
Chief Complaint & Q: What is the patient’s chief complaint? \newline
A: Persistent throat discomfort and hoarseness for two months. \\
Past Medical History & Q: Summarize past medical history. \newline
A: No major illnesses reported previously. \\
Management Plan & Q: Summarize management plan. \newline
A: Schedule biopsy and CT scan; smoking cessation counselling. \\
\bottomrule
\end{tabular}
\caption{Example question–answer pairs used for field-level JSON extraction.}
\label{tab:jsonqa}
\end{table}

\section{Dataset and Preprocessing}
We used the multilingual clinical dialogue dataset provided by the organizers, covering ten languages: English, Hindi, Gujarati, Tamil, Telugu, Marathi, Kannada, Bangla, Assamese, and Dogri.
Dialogues were concatenated turn-wise and normalized for whitespace and encoding.
Native Indic scripts were retained to preserve token integrity for Qwen’s multilingual tokenizer \citep{bai2023qwen}.

\section{Experimental Setup and Results}
The system was evaluated on the official NLP-AI4Health 2025 multilingual clinical dialogue test set across three subtasks: (i) Question Answering (QnA), (ii) Text Summarization (Summary\_Text), and (iii) Key--Value Information Extraction (Summary\_KNV). Performance was assessed using task-appropriate metrics as specified by the organizers.

\subsection{Evaluation Metrics}
\begin{itemize}
    \item \textbf{QnA:} Evaluated using macro F1 score, measuring overlap between predicted and gold-standard answers.
    \item \textbf{Summarization:} Evaluated with both ROUGE-L (lexical overlap) and BERTScore-F1 (semantic similarity), capturing fluency and factual alignment.
    \item \textbf{Structured Extraction:} Evaluated using field-level F1 (KNV F1), reflecting accuracy of key--value pairs in the generated JSON schema.
\end{itemize}

\subsection{Quantitative Results}
\begin{figure}[htbp!]
\centering
\includegraphics[width=1.2\columnwidth]{latex/avg_per_task.png}
\caption{Average task-wise scores (F1, BERT-F1, COMET) across subtasks.}
\label{fig:taskwise}
\end{figure}

\FloatBarrier % ensure Table 2 stays near this figure

\begin{table*}[!htbp]
\centering
\small
\setlength{\tabcolsep}{7pt}
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{lcccc lcccc}
\toprule
\textbf{Language} & \textbf{QnA F1} & \textbf{ROUGE-L} & \textbf{BERT-F1} & \textbf{KNV F1} &
\textbf{Language} & \textbf{QnA F1} & \textbf{ROUGE-L} & \textbf{BERT-F1} & \textbf{KNV F1} \\
\midrule
Marathi  & 0.228 & 0.169 & 0.811 & 0.302 & Telugu   & 0.345 & 0.179 & 0.834 & 0.263 \\
Kannada  & 0.471 & 0.169 & 0.825 & 0.272 & Tamil    & 0.442 & 0.182 & 0.838 & 0.297 \\
Gujarati & 0.496 & 0.170 & 0.839 & 0.269 & Bangla   & 0.334 & 0.185 & 0.822 & 0.290 \\
English  & 0.674 & 0.191 & 0.835 & 0.335 & Hindi    & 0.618 & 0.176 & 0.836 & 0.344 \\
Assamese & 0.533 & 0.181 & 0.834 & 0.288 &          &       &       &       &      \\
\midrule
\textbf{Macro Avg.} & \textbf{0.460} & \textbf{0.178} & \textbf{0.830} & \textbf{0.296} & & & & & \\
\bottomrule
\end{tabular}
\caption{Official evaluation results across subtasks. QnA: macro F1. Summary\_Text: ROUGE-L and BERT-F1. Summary\_KNV: key–value F1.}
\label{tab:main-results}
\end{table*}

% Rest of your content unchanged (Result Interpretation, Discussion, Conclusion)

\bibliographystyle{acl_natbib}
\bibliography{custom}

\end{document}
