\section{Conclusion}
This work presented a multilingual clinical dialogue summarization and structured information extraction system built on Qwen-1.5B with parameter-efficient LoRA fine-tuning.  
The system was designed to operate under constrained computational resources while maintaining high factual precision and multilingual consistency across ten Indic and non-Indic languages.

Through modular task decomposition—summary generation, field-wise JSON extraction, and multilingual question answering—the approach demonstrated strong generalization across diverse scripts and linguistic structures.  
The role-based prompting framework ensured consistent output formats, while the field-by-field extraction strategy provided resilience against schema violations that typically hinder end-to-end structured generation.  

Quantitative evaluation confirmed the effectiveness of this design: summarization achieved high semantic alignment (BERT-F1 ≈ 0.83), QnA exhibited competitive factual accuracy (macro F1 = 0.46), and JSON extraction maintained structural validity with balanced key–value F1 (0.296).  
Despite limited fine-tuning time and single-epoch training, the model achieved robust multilingual behavior and stable inference quality.

Overall, these findings highlight that careful prompt engineering, modular reasoning, and lightweight adaptation techniques can deliver clinically interpretable, multilingual NLP systems that remain both efficient and reliable in low-resource and high-variance environments.
