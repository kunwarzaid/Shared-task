Our previous system, \textbf{MedGuard}, was designed with a singular mission: to make LLM-driven clinical reasoning safer. It introduced a multi-agent simulation—Doctor, Patient, Measurement, and Safety Agents—that collectively mirrored the diagnostic workflow of a real consultation. Each agent had a distinct responsibility: the Doctor asked and reasoned, the Patient responded based on case data, the Measurement Agent provided or denied tests, and the Safety Agent intervened when prescriptions or procedures appeared risky. MedGuard proved that even general-purpose LLMs could behave responsibly when embedded in a structured, safety-aware environment. The system successfully reduced unsafe test orders and prescriptions by introducing explicit checkpoints such as drug–drug interaction (DDI) analysis and risk-aware test assessment.

Yet as we evaluated MedGuard, an important realization emerged: while the system had become \emph{safer}, it was not necessarily \emph{more trustworthy}. It told us \emph{what} to avoid but not \emph{why} decisions were made. The reasoning remained opaque, the differential diagnosis invisible, and the internal deliberations lost between prompts. In practice, this meant that MedGuard could prevent an unsafe prescription—but clinicians could not see how the AI reached its final diagnosis or whether it had reasoned coherently along the way.

This realization motivated the development of \textbf{MedGuard-X}—a step from compliance toward \emph{understanding}. While MedGuard asked “Is the outcome safe?”, MedGuard-X asks “Can we trust the reasoning process that produced it?” To answer that question, MedGuard-X introduces four tightly integrated components that together transform the system from a reactive monitor into a transparent clinical reasoning companion:

\begin{itemize}
    \item \textbf{Consensus Reasoning:} Multiple Doctor Agents independently evaluate each case, compare hypotheses, and vote on the most likely diagnosis. The level of disagreement, measured as the \emph{Consensus Disagreement Rate (CDR)}, serves as a quantifiable indicator of epistemic uncertainty—an automated form of “second opinion.”
    \item \textbf{Reasoning Trace Explainability:} Every diagnostic step is recorded as a structured reasoning trace (\texttt{<thinking\_process>}) that links a model’s decision to explicit evidence, patient statements, and prior test results. These traces make visible how an LLM weighs evidence and evolves its hypothesis space.
    \item \textbf{End-to-End Logging \& Traceability:} All agent interactions—including who acted, why, and what evidence informed the decision—are logged in real time with timestamps and context hashes. These logs are not mere audit trails but narrative reconstructions of clinical reasoning, allowing post-hoc replay and causal debugging of failures.
    \item \textbf{Differential Diagnosis Lifecycle:} The Doctor Agent maintains an evolving ranked list of differential diagnoses (DDx) that changes with each new piece of evidence. Each DDx update—promotion, demotion, or exclusion—is logged, exposing the causal path from initial uncertainty to final confidence.
\end{itemize}

Together, these layers form the foundation of a system that does not only aim for correctness but for \emph{credibility}. Logging transforms reasoning into a visible object of study; consensus converts intuition into measurable uncertainty; and the explicit DDx ledger bridges human and machine reasoning by making the model’s “thought process” inspectable and comparable to clinical reasoning frameworks. 

In short, MedGuard-X tells a story of transition—from opaque correctness to transparent understanding, from static alignment to dynamic accountability. It reframes the question of trust in medical AI: not “Did it get the answer right?” but “Can we see why it believed that was the answer, and does that belief make sense in the language of medicine?”
MedGuard-X thus reframes safety not as a constraint but as an opportunity for understanding. By transforming reasoning into a traceable, auditable process—complete with consensus signals, differential updates, and timestamped logs—it brings interpretability and trust into the core of model behavior rather than treating them as external evaluation tasks. The next question, then, is how this approach aligns with and diverges from prior work on explainable and trustworthy AI in medicine. In the following section, we situate MedGuard-X within the broader landscape of research on explainable AI (XAI), clinical dialogue modeling, and multi-agent safety systems, highlighting how it unifies these strands into a coherent framework for transparent clinical reasoning.
\section{Related Work}

The pursuit of explainability in artificial intelligence (AI) has long aimed to bridge the gap between algorithmic performance and human understanding. Early approaches to Explainable AI (XAI) sought to make opaque models more interpretable through feature attribution and surrogate modeling. Techniques such as LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Grad-CAM \cite{selvaraju2017grad} offered local interpretability for vision and tabular models, helping users understand which features influenced predictions. However, such techniques remain fundamentally limited in sequential or reasoning-intensive domains like medicine, where decision-making evolves dynamically through dialogue and evidence accumulation. As Holzinger et al. \cite{holzinger2022toward} and Ahmad et al. \cite{ahmad2018interpretable} noted, true medical explainability requires \emph{causability}—a human-understandable mapping between evidence, inference, and outcome.

The rise of large language models (LLMs) has transformed the explainability landscape, introducing reasoning traces and self-reflection as first-class elements of interaction. Chain-of-thought prompting \cite{wei2022chain}, self-consistency \cite{wang2022self}, and reflection-based reasoning \cite{shinn2023reflexion} expose the model’s internal deliberations as natural language. Yet these traces are self-generated and unverifiable, often serving as post-hoc rationalizations rather than faithful representations of the model’s reasoning \cite{turpin2023language}. Studies such as Kadavath et al. \cite{kadavath2022language} and Ji et al. \cite{ji2023survey} further demonstrate that models can sound confident while being fundamentally wrong, producing what clinicians might call “hallucinated certainty.” Thus, while chain-of-thought increases transparency, it does not guarantee \emph{truthfulness of thought}.

In the medical domain, explainability and safety intersect more acutely than in any other. Tonekaboni et al. \cite{tonekaboni2019clinicians} and Amann et al. \cite{amann2020explainability} emphasized that clinicians evaluate not just model accuracy but reasoning legitimacy—whether an AI system can justify its choices under medical logic. LLM-based medical assistants such as BioGPT \cite{luo2022biogpt}, Med-PaLM 2 \cite{singhal2025toward}, and ChatDoctor \cite{li2023chatdoctor} have demonstrated high factual accuracy on standardized tests, yet remain largely opaque in their diagnostic reasoning. These systems excel at providing plausible answers but often fail to document how intermediate evidence or conflicting symptoms influence those answers. Clinical-Camel \cite{wang2023clinicalcamel} and DoctorGPT \cite{yang2024doctorgpt} introduced dialogue-based reasoning to simulate real consultations, allowing the model to query and respond iteratively. However, their reasoning still occurs within a single-agent paradigm—there is no mechanism to verify internal consistency, validate uncertainty, or audit the progression of a differential diagnosis.

Agent-based frameworks have begun to address these limitations by distributing cognitive roles among multiple specialized agents. AgentClinic \cite{ma2024agentclinic} modeled doctor–patient interaction as a cooperative dialogue between reasoning and information agents, showing improved coherence and completeness in medical conversations. Yet such frameworks stop short of embedding accountability: their agents converse but do not \emph{explain themselves} in a machine-auditable way. Similarly, multi-agent safety systems like GuardMed \cite{peters2023safeaimed} and SafetyBench \cite{ghosh2024alignment} introduced safety supervision and red-teaming strategies, but these focus on output control rather than process transparency.

Parallel efforts in AI governance and auditing have highlighted the same gap. The World Health Organization’s guidance on ethics and governance of AI in health \cite{guidance2021ethics} and the European Commission’s regulatory framework for trustworthy AI \cite{bomhard2021regulation} emphasize traceability, fairness, and accountability as foundational pillars. Yet in practice, most clinical LLM pipelines remain black boxes during inference, offering no continuous record of how decisions evolve. The lack of auditability undermines post-hoc analysis, reproducibility, and user trust—key concerns raised in the broader literature on algorithmic accountability \cite{begoli2019need, doshi2017towards}.

\textbf{MedGuard-X} is positioned at the intersection of these developments. It operationalizes explainability not as an afterthought but as a design principle, integrating real-time logging, explicit reasoning traces, and consensus-driven uncertainty estimation into the diagnostic process itself. Each agent’s interaction—whether a question, a test order, or a prescription—is recorded with a timestamp, reasoning snippet, and context hash, forming a verifiable audit trail of the reasoning flow. Unlike static XAI methods that reconstruct explanations after inference, MedGuard-X provides \emph{process-level transparency}: the ability to trace how evidence, hypotheses, and safety constraints interact over time. Its explicit differential-diagnosis ledger allows observers to see how initial uncertainty narrows into final conclusions, while the consensus mechanism quantifies epistemic disagreement among parallel doctor agents. In doing so, MedGuard-X transforms explainability from an interpretive task into an intrinsic property of system behavior.

Ultimately, this work builds upon and extends the emerging consensus that explainability and trustworthiness are inseparable in safety-critical AI. MedGuard-X contributes to this trajectory by demonstrating that logging, traceability, and consensus reasoning can make medical LLMs not only \emph{safer}, but also \emph{understandable}—a necessary step toward systems clinicians can interrogate, audit, and trust.
