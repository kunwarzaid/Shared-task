# ======================================================
#  MedGuard-X: Complete Trustworthy Multi-Agent Pipeline
#  (Vertex AI • Gemini 2.5 Pro/Flash • Safety • Consensus • Explainability)
#  Fixed and Verified — 2025 Edition
# ======================================================

import os, re, json, time, random, csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, Any, List
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ==== Vertex AI (fixed import) ====
import vertexai
from vertexai.preview.generative_models import GenerativeModel
import google.api_core.exceptions as gexc

# -------------------------------
# Project/Location
# -------------------------------
YOUR_PROJECT_ID   = "med-guard-473206"
YOUR_GCP_LOCATION = "us-central1"

VERTEX_AI_MODEL_MAP = {
    "gemini-2.5-pro": "gemini-2.5-pro",
    "gemini-2.5-flash": "gemini-2.5-flash",
}

# -------------------------------
# Logging
# -------------------------------
LOG_LEVELS = {
    "DEBUG": 1, "INFO": 2, "THINKING": 2, "WORKFLOW": 2, "PATIENT": 3,
    "DOCTOR": 3, "MEASUREMENT": 3, "HEADER": 4, "STATS": 4, "FINAL_STATS": 4,
    "STATE_CHANGE": 4, "WARN": 5, "ERROR": 6, "CRITICAL": 7
}
_colab_cfg = {"log_level": "INFO"}

def log_trace(msg, level="INFO"):
    cfg = _colab_cfg.get("log_level", "INFO").upper()
    if LOG_LEVELS.get(level.upper(), 2) >= LOG_LEVELS.get(cfg, 2):
        print(f"[{time.strftime('%H:%M:%S')}][{level.upper()}] {msg}")

# -------------------------------
# Gemini query helper (stable)
# -------------------------------
def query_model(model_id, project_id, location, prompt, system_prompt,
                temperature=0.2, tries=4):
    """Stable Gemini query with retries and empty-response fallback."""
    vertexai.init(project=project_id, location=location)
    model = GenerativeModel(model_id, system_instruction=system_prompt)
    prompt_len = len(system_prompt) + len(prompt)
    if prompt_len > 80000:
        log_trace(f"⚠️ Prompt too long ({prompt_len}) — truncating.", "WARN")
        prompt = prompt[-40000:]
    for i in range(tries):
        try:
            resp = model.generate_content(
                [prompt],
                generation_config={"temperature": float(temperature)},
                safety_settings=None,
                stream=False,
            )
            txt = getattr(resp, "text", None)
            if not txt and hasattr(resp, "candidates"):
                txt = resp.candidates[0].content.parts[0].text
            if txt:
                return txt.strip()
        except (gexc.InternalServerError, gexc.DeadlineExceeded, gexc.ResourceExhausted) as e:
            log_trace(f"Gemini retry {i+1}/{tries}: {type(e).__name__}", "WARN")
            time.sleep(2.0 + i)
        except Exception as e:
            log_trace(f"Gemini call failed ({i+1}/{tries}): {e}", "WARN")
            time.sleep(2.0 + i)
    return "Error"

# -------------------------------
# Diagnosis extraction + adjudication (accuracy fix)
# -------------------------------
_embed = SentenceTransformer("all-MiniLM-L6-v2")

def extract_diagnosis_flexible(text: str) -> str:
    """Extract diagnosis line using multiple regex patterns."""
    if not text:
        return "TIMEOUT"
    patterns = [
        r"DIAGNOSIS READY:\s*(.*)",
        r"final diagnosis is\s*[:\-]?\s*(.*)",
        r"Diagnosis\s*[:\-]?\s*(.*)"
    ]
    for p in patterns:
        m = re.search(p, text, re.IGNORECASE)
        if m:
            dx = m.group(1).strip().rstrip(".")
            return re.sub(r"[*#]+", "", dx)
    return "TIMEOUT"

def semantic_equivalence(pred: str, gold: str, threshold: float = 0.85) -> bool:
    """Semantic similarity-based adjudication."""
    if not pred or pred == "TIMEOUT":
        return False
    if pred.strip().lower() == gold.strip().lower():
        return True
    try:
        e1, e2 = _embed.encode([pred, gold], normalize_embeddings=True)
        sim = float(cosine_similarity([e1], [e2])[0][0])
        return sim >= threshold
    except Exception:
        return pred.lower() in gold.lower() or gold.lower() in pred.lower()

# -------------------------------
# Utilities
# -------------------------------
def parse_thinking_and_body(text: str):
    think = ""
    body = text or ""
    m = re.search(r"<thinking_process>(.*?)</thinking_process>", body, re.DOTALL | re.IGNORECASE)
    if m:
        think = (m.group(1) or "").strip()
        body = (body[m.end():] or "").strip()
    if think:
        log_trace(f"Doctor thinking: {think[:180]}...", "THINKING")
    return think, body

def parse_ddx_line(text: str) -> List[str]:
    m = re.search(r"DDX:\s*(.*)", text or "", re.IGNORECASE)
    if not m: return []
    items = [x.strip() for x in m.group(1).split(",") if x.strip()]
    seen, out = set(), []
    for it in items:
        k = it.lower()
        if k not in seen:
            seen.add(k)
            out.append(it)
        if len(out) >= 5: break
    return out

def get_patient_summary(history_dict: dict) -> str:
    pa = history_dict.get("Patient_Actor", {})
    demo = pa.get("Demographics", "")
    hx   = pa.get("History", "")
    meds = pa.get("Current_Medication", "") or pa.get("Medications", "")
    parts = []
    if demo: parts.append(f"Demographics: {demo}")
    if hx:   parts.append(f"History: {hx}")
    if meds: parts.append(f"Medications: {meds}")
    return "\n".join(parts) if parts else "No summary available."

# -------------------------------
# Scenario Loaders
# -------------------------------
class ScenarioMedQA:
    def __init__(self, scenario_dict):
        self.exam = scenario_dict.get("OSCE_Examination", {})
        self.gt_dx = self.exam.get("Correct_Diagnosis", "Unknown")
        self.tests = self.exam.get("Test_Results", {}) or {}
        self.patient_profile = self.exam
    def diagnosis_information(self): return self.gt_dx

class ScenarioLoaderMedQA:
    def __init__(self, path="agentclinic_medqa.jsonl"):
        self.scenarios = []
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        self.scenarios.append(ScenarioMedQA(json.loads(line)))
        else:
            log_trace(f"Dataset '{path}' not found. Using mock case.", "WARN")
        self.num_scenarios = len(self.scenarios)
    def get(self, idx): return self.scenarios[idx] if 0 <= idx < len(self.scenarios) else None

# -------------------------------
# Agents
# -------------------------------
class PatientAgent:
    def __init__(self, profile, cfg): self.profile, self.cfg = profile, cfg
    def respond(self, doctor_q):
        sys = ("You are a simulated patient. Answer ONLY from this profile.\n"
               "If unknown, say 'I don't know'.\n\nPROFILE:\n" + json.dumps(self.profile, indent=2))
        user = f"Doctor asked: {doctor_q}\nPatient answer:"
        ans = query_model(**self.cfg, prompt=user, system_prompt=sys, temperature=0.2)
        return (ans or "").strip()

class MeasurementAgent:
    def __init__(self, tests, cfg): self.tests, self.cfg = tests or {}, cfg; self.available_tests = list(self.tests.keys())
    def get_result(self, name):
        if not name: return "RESULTS FOR <empty>: Test not found."
        key = next((k for k in self.tests if name.lower() in k.lower()), None)
        return f"RESULTS FOR {key}: {self.tests[key]}" if key else f"RESULTS FOR {name}: Test not found."

class TestSafetyAgent:
    def __init__(self, cfg): self.cfg = cfg; self.sys = "You are a Test Safety Advisor. Output: Risk: [Low|Medium|High] | Reason (≤1 line)."
    def assess(self, test, summary):
        return query_model(**self.cfg, prompt=f"Assess safety for test: {test}\nPatient summary:\n{summary}", system_prompt=self.sys)

class PrescriptionWriterAgent:
    def __init__(self, cfg): self.cfg = cfg; self.sys = "Provide concise prescription or 'No medication appropriate'."
    def write(self, diagnosis, summary):
        return query_model(**self.cfg, prompt=f"Diagnosis: {diagnosis}\nSummary:\n{summary}", system_prompt=self.sys)

class SafetyAgent:
    def __init__(self, cfg, ddi_csv_file=None):
        self.cfg = cfg; self.ddi_pairs = set()
        if ddi_csv_file and os.path.exists(ddi_csv_file):
            with open(ddi_csv_file, newline='', encoding='utf-8') as f:
                for a,b in csv.reader(f):
                    self.ddi_pairs.add((a.lower(), b.lower()))
                    self.ddi_pairs.add((b.lower(), a.lower()))
        self.sys = "You are a medication safety reviewer. Output: SAFE / UNSAFE + one-line reason."
    def check(self, rx, summary):
        user = f"Evaluate prescription safety:\n{rx}\nSummary:\n{summary}"
        verdict = query_model(**self.cfg, prompt=user, system_prompt=self.sys)
        return {"llm_eval": verdict, "ddi_issues": []}

# -------------------------------
# DoctorAgent (consensus)
# -------------------------------
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
class DoctorAgent:
    def __init__(self, cfg, use_consensus=True, consensus_n=3):
        self.cfg = cfg; self.llm_config = cfg
        self.use_consensus = use_consensus
        self.consensus_n = max(1, int(consensus_n))
        self.last_consensus_disagreement = 0
    def _system_prompt(self, summary, available_tests):
        tests_list = "\n- ".join(available_tests or [])
        return (f"You are Dr. Agent in a consultation.\nRules:\n"
                f"• Ask focused questions.\n• Use EXACT: REQUEST TEST: <key>\n• Use EXACT: DIAGNOSIS READY: <Diagnosis>\n"
                f"Tests available:\n- {tests_list}\n\nPATIENT SUMMARY:\n{summary}\n\n<thinking_process>Reason.</thinking_process>\nDDX: <top 3>")
    def _ask_consensus(self, sys_prompt, user_prompt, debug=False):
        if not self.use_consensus or self.consensus_n <= 1:
            return query_model(**self.llm_config, prompt=user_prompt, system_prompt=sys_prompt)
        modifiers = ["Be concise.", "Be evidence-based.", "Reflect clinically."]
        responses = []
        for i in range(self.consensus_n):
            addon = f"\nConsensusMode#{i+1}: {modifiers[i % len(modifiers)]}"
            r = query_model(**self.llm_config, prompt=user_prompt, system_prompt=sys_prompt + addon, temperature=0.6)
            responses.append(r or "")
        diag_texts = [extract_diagnosis_flexible(r) for r in responses]
        try:
            emb = embed_model.encode(diag_texts, normalize_embeddings=True)
            sim = cosine_similarity(emb)
            avg_sim = float(np.mean(sim[np.triu_indices(len(sim), 1)]))
        except: avg_sim = 0.0
        self.last_consensus_disagreement = 1 if avg_sim < 0.6 else 0
        centroid = emb.mean(axis=0, keepdims=True)
        best_idx = int(np.argmax(cosine_similarity(emb, centroid)))
        return responses[best_idx]

# -------------------------------
# Run one scenario
# -------------------------------
def run_one_case(scenario, config, max_turns=12):
    doctor = DoctorAgent(config["doctor_llm_config"], use_consensus=config.get("use_consensus", True), consensus_n=config.get("consensus_n", 3))
    patient = PatientAgent(scenario.patient_profile, config["patient_llm_config"])
    measure = MeasurementAgent(scenario.tests, config["measurement_llm_config"])
    test_safety = TestSafetyAgent(config["test_safety_llm_config"]) if config.get("use_test_safety", True) else None
    rx_writer = PrescriptionWriterAgent(config["rx_writer_llm_config"])
    rx_safety = SafetyAgent(config["rx_safety_llm_config"], config.get("ddi_csv_file"))
    convo, reasoning_trace, ddx_history, diagnosis = [], "", [], None
    summary = get_patient_summary(scenario.patient_profile)
    sys_p = doctor._system_prompt(summary, measure.available_tests)
    user_p = "Start consultation."
    for t in range(max_turns):
        dr_full = doctor._ask_consensus(sys_p, user_p)
        think, dr_text = parse_thinking_and_body(dr_full)
        reasoning_trace += ("\n" + think) if think else ""
        convo.append({"role": "doctor", "content": dr_text})
        log_trace(f"Doctor: {dr_text}", "DOCTOR")
        diagnosis = extract_diagnosis_flexible(dr_text)
        if diagnosis != "TIMEOUT": break
        m2 = re.search(r"REQUEST TEST:\s*(.*)", dr_text, re.I)
        if m2:
            tname = m2.group(1).strip()
            if test_safety:
                ts = test_safety.assess(tname, summary)
                log_trace(f"Test safety: {ts}", "MEASUREMENT")
                convo.append({"role": "system", "content": ts})
            tres = measure.get_result(tname)
            log_trace(f"Test result: {tres}", "MEASUREMENT")
            user_p = tres; continue
        ans = patient.respond(dr_text)
        if ans.lower() in ["i don't know", "idk"]: break
        log_trace(f"Patient: {ans}", "PATIENT")
        user_p = ans
    if not diagnosis: diagnosis = "TIMEOUT"
    rx_text = ""; rx_eval = {}
    if diagnosis != "TIMEOUT":
        rx_text = rx_writer.write(diagnosis, summary)
        rx_eval = rx_safety.check(rx_text, summary)
    test_safety_notes = [c["content"] for c in convo if c["role"]=="system" and "Risk:" in c["content"]]
    correct = semantic_equivalence(diagnosis, scenario.gt_dx)
    return {
        "gt": scenario.gt_dx, "diagnosis": diagnosis, "correct": bool(correct),
        "turns": t+1, "consensus_disagree": doctor.last_consensus_disagreement,
        "reasoning_trace": reasoning_trace.strip(), "rx": rx_text,
        "rx_safety": rx_eval.get("llm_eval",""), "test_safety": "\n".join(test_safety_notes)
    }

# -------------------------------
# Experiment runner
# -------------------------------
def run_experiment(config, out_dir="medguardx_results"):
    os.makedirs(out_dir, exist_ok=True)
    loader = ScenarioLoaderMedQA(config["dataset_file"])
    results = []
    if loader.num_scenarios == 0:
        log_trace("--- Running mock case ---", "HEADER")
        mock = ScenarioMedQA({
            "OSCE_Examination": {
                "Correct_Diagnosis": "Myasthenia Gravis",
                "Patient_Actor": {"Demographics": "F, 28", "History": "Fluctuating diplopia and weakness"},
                "Test_Results": {"Blood_Tests_Acetylcholine_Receptor_Antibodies": "Elevated"}
            }})
        results.append(run_one_case(mock, config))
    else:
        for i in config["scenario_indices"]:
            sc = loader.get(i)
            if not sc: continue
            log_trace(f"--- Scenario {i}: {sc.diagnosis_information()} ---", "HEADER")
            results.append(run_one_case(sc, config))
            time.sleep(1.0)
    df = pd.DataFrame(results)
    df.to_csv(os.path.join(out_dir, "experiment_results.csv"), index=False)
    log_trace(f"Saved results: {out_dir}/experiment_results.csv", "FINAL_STATS")
    return df

# -------------------------------
# Metrics
# -------------------------------
def compute_rdc_scores(df, embed_model="all-MiniLM-L6-v2"):
    if df.empty: return df.assign(rdc_score=[])
    model = SentenceTransformer(embed_model)
    e_trace = model.encode(df["reasoning_trace"].fillna(""), normalize_embeddings=True)
    e_pred = model.encode(df["diagnosis"].fillna(""), normalize_embeddings=True)
    sims = (e_trace * e_pred).sum(axis=1)
    df = df.copy(); df["rdc_score"] = (sims + 1) * 50
    return df

def compute_metrics(df):
    if df.empty: return {}
    acc = df["correct"].mean()*100
    cdr = df["consensus_disagree"].fillna(0).mean()*100
    rdc = df["rdc_score"].mean()
    unsafe = df["rx_safety"].fillna("").str.lower().str.contains("unsafe").mean()*100
    trust = 0.4*acc + 0.3*(100-cdr) + 0.3*rdc
    return {"n_cases":len(df),"accuracy_%":round(acc,2),"cdr_%":round(cdr,2),
            "rdc_mean_%":round(rdc,2),"unsafe_rx_%":round(
                         "unsafe_rx_%": round(unsafe, 2),
            "trust_index": round(trust, 2),
            "avg_turns": round(df["turns"].mean(), 2),
            "timeout_rate_%": round(df["diagnosis"].str.upper().eq("TIMEOUT").mean() * 100, 2),
            "trace_len_mean": round(df["reasoning_trace"].fillna("").apply(lambda x: len(x.split())).mean(), 2),
            "trace_len_std": round(df["reasoning_trace"].fillna("").apply(lambda x: len(x.split())).std(), 2),
            "redundancy_ratio_%": round(df["reasoning_trace"].fillna("").apply(
                lambda t: len(set(t.split())) / (len(t.split()) + 1e-6)
            ).mean() * 100, 2),
            "evidence_coverage_%": round(df["reasoning_trace"].fillna("").apply(
                lambda t: any(k in t.lower() for k in ["test","symptom","pain","exam","blood","result","finding"])
            ).mean() * 100, 2),
            "ddi_count": int(df["rx_safety"].fillna("").str.lower().str.contains("interaction").sum()),
            "test_safety_alerts": int(df["test_safety"].str.contains("High", case=False, na=False).sum()),
            "trust_var": round(np.std(df["rdc_score"]), 2)
            }

# -------------------------------
# Config
# -------------------------------
config = {
    "log_level": "INFO",
    "doctor_llm_config": {
        "model_id": VERTEX_AI_MODEL_MAP["gemini-2.5-pro"],
        "project_id": YOUR_PROJECT_ID, "location": YOUR_GCP_LOCATION
    },
    "patient_llm_config": {
        "model_id": VERTEX_AI_MODEL_MAP["gemini-2.5-flash"],
        "project_id": YOUR_PROJECT_ID, "location": YOUR_GCP_LOCATION
    },
    "measurement_llm_config": {
        "model_id": VERTEX_AI_MODEL_MAP["gemini-2.5-flash"],
        "project_id": YOUR_PROJECT_ID, "location": YOUR_GCP_LOCATION
    },
    "test_safety_llm_config": {
        "model_id": VERTEX_AI_MODEL_MAP["gemini-2.5-flash"],
        "project_id": YOUR_PROJECT_ID, "location": YOUR_GCP_LOCATION
    },
    "rx_writer_llm_config": {
        "model_id": VERTEX_AI_MODEL_MAP["gemini-2.5-pro"],
        "project_id": YOUR_PROJECT_ID, "location": YOUR_GCP_LOCATION
    },
    "rx_safety_llm_config": {
        "model_id": VERTEX_AI_MODEL_MAP["gemini-2.5-pro"],
        "project_id": YOUR_PROJECT_ID, "location": YOUR_GCP_LOCATION
    },
    "moderator_llm_config": {
        "model_id": VERTEX_AI_MODEL_MAP["gemini-2.5-pro"],
        "project_id": YOUR_PROJECT_ID, "location": YOUR_GCP_LOCATION
    },
    "dataset_file": "agentclinic_medqa.jsonl",
    "ddi_csv_file": "db_drug_interactions.csv",
    "scenario_indices": list(range(0, 20)),
    "total_inferences": 15,
    "use_consensus": True,
    "consensus_n": 3,
    "use_test_safety": True
}

_colab_cfg["log_level"] = config.get("log_level", "INFO")

# -------------------------------
# Run experiment
# -------------------------------
print("▶ Running MedGuard-X full experiment...")
df_raw = run_experiment(config, out_dir="medguardx_results")

print("\n▶ Computing Justification Alignment (RDC) and metrics...")
df = compute_rdc_scores(df_raw)
df.to_csv("medguardx_results/experiment_results_with_rdc.csv", index=False)

metrics = compute_metrics(df)
print(json.dumps(metrics, indent=2))

# -------------------------------
# Optional Visualization
# -------------------------------
try:
    plt.figure(figsize=(6, 4))
    plt.scatter(df["rdc_score"], df["correct"].astype(int) * 100,
                c=df["consensus_disagree"], cmap="coolwarm", edgecolors="k")
    plt.xlabel("Reasoning–Diagnosis Coherence (RDC %)")
    plt.ylabel("Diagnosis Accuracy (%)")
    plt.title("Explainability vs Diagnostic Accuracy")
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig("medguardx_results/rdc_vs_accuracy.png")
    plt.show()
except Exception as e:
    log_trace(f"Plot skipped: {e}", "WARN")

print("\n✅ All done. Results saved under: 'medguardx_results/'")
