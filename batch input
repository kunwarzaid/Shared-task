\begin{abstract}
Large Language Models (LLMs) have recently demonstrated strong diagnostic and reasoning performance on medical benchmarks such as MedQA and PubMedQA, yet their deployment in clinical decision support remains constrained by concerns of reliability, transparency, and trustworthiness. Prior efforts have primarily focused on improving safety—for example, the MedGuard framework introduced multi-agent validation to reduce unsafe or contraindicated recommendations. However, safety alone does not ensure trust: little is known about \emph{why} medical LLMs reach specific conclusions, how consistently they reason across agents, or whether explainability correlates with diagnostic correctness.

In this work, we present \textbf{MedGuard-X}, an extension of the MedGuard system that embeds trust and interpretability into the diagnostic reasoning process itself. MedGuard-X integrates (i) multi-LLM consensus reasoning to estimate epistemic uncertainty, (ii) structured reasoning traces for each clinical action, and (iii) trustworthiness scoring through disagreement tracking and justification alignment—all within a simulated multi-agent clinical environment.

Experiments on 200 complex clinical scenarios demonstrate that while MedGuard-X improves transparency and reduces unsafe test and prescription recommendations, diagnostic accuracy remains inconsistent. In many cases, LLMs exhibit confident yet unjustified reasoning—revealing that explainability does not imply reliability. These findings highlight the need for structured accountability mechanisms and hybrid neuro-symbolic architectures to ensure that future medical LLMs are not only safe but genuinely trustworthy in real-world clinical settings.
\end{abstract}

\textbf{Keywords:} Large Language Models, Explainable AI, Trustworthy AI, Medical Diagnosis, Multi-Agent Systems, Clinical Reasoning, Consensus Learning, Causal Explainability, Neuro-Symbolic AI
