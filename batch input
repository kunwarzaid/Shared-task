@article{bai2023qwen,
  title={Qwen Technical Report},
  author={Bai, Yutong and Bai, Zhilin and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{dettmers2023bitsandbytes,
  title={Bitsandbytes: Optimizing Memory Efficiency in Large Language Model Training},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2303.05449},
  year={2023}
}

@article{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={NAACL-HLT},
  year={2019}
}

@article{lin2004rouge,
  title={ROUGE: A Package for Automatic Evaluation of Summaries},
  author={Lin, Chin-Yew},
  journal={Text Summarization Branches Out},
  year={2004}
}

@article{zhang2020bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{rei2020comet,
  title={COMET: A Neural Framework for MT Evaluation},
  author={Rei, Ricardo and Farinha, Ana C. and Lavie, Alon and Martins, Andr{\'e} F. T.},
  journal={EMNLP},
  year={2020}
}

@article{xu2023multilingual,
  title={Multilingual Large Language Models are Better at Cross-Lingual Understanding than Translation-Based Approaches},
  author={Xu, Zihan and Li, Xiang and others},
  journal={arXiv preprint arXiv:2305.14787},
  year={2023}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{gururangan2020don,
  title={Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
  booktitle={ACL},
  year={2020}
}
