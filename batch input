\begin{abstract}
Large Language Models (LLMs) have recently demonstrated strong diagnostic and reasoning performance on medical benchmarks such as \textit{MedQA}, yet their deployment in clinical decision support remains constrained by concerns of reliability, transparency, and trustworthiness. Prior efforts have primarily emphasized safety—for example, the MedGuard framework introduced multi-agent validation to reduce unsafe or contraindicated recommendations. However, safety alone does not ensure trust: little is known about \emph{why} medical LLMs reach specific conclusions, how consistently they reason across agents, or whether explainability correlates with diagnostic correctness.

In this work, we present \textbf{MedGuard-X}, an extension of the MedGuard system that embeds trust and interpretability directly into the diagnostic reasoning process. MedGuard-X integrates (i) multi-LLM consensus reasoning to estimate epistemic uncertainty, (ii) structured reasoning traces for each clinical action, and (iii) trustworthiness scoring through disagreement tracking and justification alignment—all within a simulated multi-agent clinical environment.

Experiments on the \textit{MedQA} benchmark demonstrate that while MedGuard-X improves transparency and reduces unsafe test and prescription recommendations, its diagnostic accuracy remains variable across cases. In many instances, LLMs exhibit confident yet unjustified reasoning—revealing that explainability does not necessarily entail reliability. These findings highlight the need for structured accountability mechanisms and hybrid neuro-symbolic architectures to ensure that future medical LLMs are not only safe but genuinely trustworthy in real-world clinical settings.
\end{abstract}


\textbf{Keywords:} Large Language Models, Explainable AI, Trustworthy AI, Medical Diagnosis, Multi-Agent Systems, Clinical Reasoning, Consensus Learning, MedQA, Neuro-Symbolic AI

\section{Introduction}

Large Language Models (LLMs) such as GPT-4 \cite{achiam2023gpt}, Gemini \cite{team2023gemini}, and Med-PaLM 2 \cite{singhal2025toward} have demonstrated remarkable progress in natural language reasoning and generalization across diverse domains, including medicine. They now perform at near-clinician levels on medical question-answering benchmarks \cite{kung2023performance,nori2023capabilities}, suggesting potential applications in decision support, documentation, and triage \cite{lee2023benefits}. Yet, the very properties that make LLMs powerful—scale, open-endedness, and linguistic fluency—also render them unreliable in domains requiring factual precision, causal reasoning, and accountability \cite{ji2023survey,begoli2019need}.

Integrating LLMs into healthcare introduces challenges that extend beyond accuracy. Medicine demands not only correct predictions but also \emph{explainable and justifiable} reasoning. Clinicians reason through causality, uncertainty, and evidence weighting—capabilities that current LLMs only partially emulate. When an AI system proposes a diagnosis or prescription, its credibility depends as much on \emph{why} it reached that conclusion as on \emph{what} it predicts \cite{tonekaboni2019clinicians,amann2020explainability}. Without transparent reasoning, even correct answers may be unsafe, as the underlying rationale could be spurious or unverifiable.

Despite improvements in alignment and red-teaming \cite{mei2023assert}, clinical LLMs still struggle with trustworthiness—a composite property encompassing accuracy, consistency, safety, and interpretability \cite{a2019,huang2025survey}. Models frequently display unjustified confidence \cite{kadavath2022language}, produce inconsistent explanations, or fail to acknowledge uncertainty in ambiguous cases. This epistemic overconfidence poses a critical risk in medicine, where admitting uncertainty can be safer than being confidently wrong. While systems such as Med-PaLM 2 \cite{singhal2025toward}, BioGPT \cite{luo2022biogpt}, and ChatDoctor \cite{li2023chatdoctor} exhibit strong factual accuracy, they often lack reasoning stability—small prompt changes can yield entirely different diagnoses.

Our prior framework, \textbf{MedGuard}, addressed safety but not transparency. It simulated a clinical workflow using four agents—Doctor, Patient, Measurement, and Safety—each responsible for a distinct stage of reasoning or validation. MedGuard effectively reduced unsafe test orders and prescriptions by incorporating explicit drug–drug interaction (DDI) checks and test risk assessments, showing that structured dialogue can mitigate unsafe behavior. However, its reasoning process remained opaque: clinicians could not inspect the model’s thought process, track evolving hypotheses, or understand why specific conclusions were reached.

These limitations motivated \textbf{MedGuard-X}, a shift from safety to \emph{trust-centered explainability}. Whereas MedGuard asked “Is the outcome safe?”, MedGuard-X asks “Can we trust how the model arrived at it?” To answer this, MedGuard-X introduces four mechanisms that transform the system from a reactive monitor into a transparent reasoning companion:
\begin{itemize}
    \item \textbf{Consensus Reasoning:} Multiple Doctor Agents independently evaluate each case and vote on the most likely diagnosis. Their disagreement, measured via the \emph{Consensus Disagreement Rate (CDR)}, quantifies epistemic uncertainty.
    \item \textbf{Reasoning Trace Explainability:} Each diagnostic step includes a structured \texttt{<thinking\_process>} trace linking evidence, questions, and conclusions, exposing how hypotheses evolve.
    \item \textbf{End-to-End Logging and Traceability:} Every agent interaction is logged with timestamp, actor, and reasoning context, enabling full causal replay and accountability.
    \item \textbf{Differential Diagnosis Lifecycle:} A ranked list of hypotheses evolves with each new piece of evidence, allowing clinicians to visualize how uncertainty narrows into confidence.
\end{itemize}

Together, these mechanisms make reasoning \emph{visible, auditable, and measurable}. Logging transforms decisions into traceable artifacts; consensus converts intuition into quantifiable uncertainty; and the evolving DDx ledger bridges machine reasoning with clinical cognition. In doing so, MedGuard-X reframes safety as a pathway to understanding and establishes trustworthiness as a property of the reasoning process itself.

\section{Related Work}

The pursuit of explainability in artificial intelligence has long sought to bridge algorithmic performance with human interpretability. Early work on Explainable AI (XAI) employed feature attribution and surrogate modeling—e.g., LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Grad-CAM \cite{selvaraju2017grad}—to visualize which features influenced predictions. While effective for static models, such methods are ill-suited for domains like medicine, where decisions evolve through dialogue and evidence accumulation. As Holzinger et al. \cite{holzinger2019causability} and Ahmad et al. \cite{ahmad2018interpretable} argue, medical explainability requires \emph{causability}: a human-understandable mapping between evidence, inference, and outcome.

The advent of LLMs has redefined explainability through reasoning traces and self-reflection. Chain-of-thought prompting \cite{wei2022chain}, self-consistency \cite{wang2022self}, and reflection-based reasoning \cite{shinn2023reflexion} externalize a model’s deliberations as text. However, these traces are self-generated and often post-hoc rationalizations rather than genuine reasoning \cite{turpin2023language}. Models frequently express confident but incorrect rationales \cite{kadavath2022language,ji2023survey}, resulting in what clinicians might call “hallucinated certainty.” Thus, transparency alone does not guarantee truthfulness of thought.

In medicine, explainability and safety are inseparable. Tonekaboni et al. \cite{tonekaboni2019clinicians} and Amann et al. \cite{amann2020explainability} highlight that clinicians evaluate models by their reasoning legitimacy—whether conclusions align with medical logic. While LLM-based medical systems like BioGPT \cite{luo2022biogpt}, Med-PaLM 2 \cite{singhal2025toward}, and ChatDoctor \cite{li2023chatdoctor} demonstrate strong factual performance, they remain opaque in diagnostic reasoning. Clinical-Camel \cite{toma2023clinical} and similar systems introduced interactive consultations but still operate as single-agent frameworks without explicit uncertainty estimation or differential tracking.

Multi-agent systems have begun addressing these gaps. AgentClinic \cite{schmidgall2024agentclinic} modeled doctor–patient interaction as a cooperative dialogue between reasoning and data agents, improving conversational coherence. Yet, accountability remains limited—agents exchange information but do not produce verifiable reasoning records. Similarly, recent safety frameworks like GuardMed and SafetyBench integrate oversight mechanisms but focus on output moderation rather than process transparency.

Regulatory frameworks from the World Health Organization \cite{guidance2021ethics} and the European Commission \cite{bomhard2021regulation} emphasize traceability and auditability as cornerstones of trustworthy medical AI. However, most current LLM pipelines remain black boxes during inference, offering no visibility into evolving reasoning states—hindering reproducibility, fairness audits, and clinician trust \cite{begoli2019need,doshi2017towards}.

\textbf{MedGuard-X} advances this agenda by operationalizing explainability as a built-in property of system behavior. It integrates real-time logging, structured reasoning traces, and consensus-based uncertainty estimation into the diagnostic workflow. Each inter-agent exchange is timestamped, annotated, and context-linked, creating a continuous, auditable trail of reasoning. Unlike post-hoc XAI methods, MedGuard-X enables \emph{process-level transparency}: the ability to trace how evidence, hypotheses, and safety constraints evolve over time. Its differential diagnosis ledger and consensus mechanism together form a foundation for reasoning accountability—moving toward AI systems that clinicians can interrogate, audit, and trust.

\section{Methodology: The MedGuard-X Framework}

Building upon the safety-centered MedGuard architecture, \textbf{MedGuard-X} reimagines clinical reasoning as a transparent, auditable, and multi-agent process. Figure~\ref{fig:logging-architecture} illustrates the overall architecture: a dynamic ecosystem of interacting agents (Doctor, Patient, Measurement, and Safety) embedded within a centralized Explainability and Logging Layer. This layer continuously captures reasoning traces, timestamps, and decision rationales—turning every diagnostic step into a traceable unit of accountability.

\subsection*{1. Multi-Agent Diagnostic Simulation}
To simulate clinical realism, MedGuard-X models the diagnostic process as sequential exchanges between agents. The \textbf{Doctor Agent} initiates with open-ended questioning, forms hypotheses, and requests investigations. The \textbf{Patient Agent} retrieves factual answers from the scenario’s case file, ensuring medical realism. The \textbf{Measurement Agent} supplies requested test results or returns “not available” when appropriate, while the \textbf{Safety Agent} evaluates safety implications of each decision. The loop continues until the Doctor issues a final statement of the form \texttt{DIAGNOSIS READY: <diagnosis>}, after which the prescription subsystem is activated.

\subsection*{2. Consensus Reasoning and Epistemic Uncertainty}
To quantify uncertainty, MedGuard-X employs \textbf{consensus reasoning}: multiple independent Doctor Agents—instantiated as distinct LLMs or stochastic replicas—evaluate the same case in parallel. Their diagnoses are aggregated through a weighted consensus mechanism, and inter-agent disagreement is measured via the \emph{Consensus Disagreement Rate (CDR)}. High CDR values signal epistemic uncertainty, while low disagreement indicates robust convergence. This mechanism formalizes the notion of “second opinion” within machine reasoning.

\subsection*{3. Reasoning Traces and Differential Diagnosis Lifecycle}
Each Doctor Agent outputs structured reasoning traces encapsulated in \texttt{<thinking\_process>} tags. These traces record intermediate hypotheses, justifications for test requests, and discarded possibilities. Over time, they form a “reasoning ledger” that mirrors clinical thought progression. Updates to the differential diagnosis (DDx)—additions, eliminations, or confidence shifts—are logged alongside supporting evidence, collectively forming the \textbf{Differential Diagnosis Lifecycle (DDxL)}. This allows direct inspection of how initial uncertainty resolves into final confidence.

\subsection*{4. Safety and Prescription Evaluation}
MedGuard-X extends its safety framework into two specialized subsystems:
\begin{itemize}
    \item \textbf{Test Safety Agent:} Evaluates the risk–benefit trade-off of each investigation, alerting the Doctor to high-risk or redundant tests.
    \item \textbf{Prescription Safety Agent:} Cross-references proposed medications against contraindication and drug–drug interaction databases, assigning one of three qualitative ratings: \textit{SAFE}, \textit{SAFE WITH CAUTION}, or \textit{UNSAFE}.
\end{itemize}
These subsystems intervene during reasoning, shaping behavior rather than merely validating final outputs.

\subsection*{5. Explainability and Logging Layer}
Every message exchanged—question, test request, safety alert—is captured in a unified \textbf{Explainability and Logging Layer}. Each log entry includes:
\begin{enumerate}
    \item The acting agent and its role;
    \item Timestamp and session context;
    \item The corresponding reasoning trace; and
    \item Quantitative metrics (confidence, disagreement, safety flag).
\end{enumerate}
This continuous, machine-readable record allows causal replay, fairness auditing, and introspective visualization of reasoning chains.

\subsection*{6. Trustworthiness Metrics}
To convert transparency into measurable reliability, MedGuard-X introduces two metrics:
\begin{itemize}
    \item \textbf{Consensus Disagreement Rate (CDR):} Fraction of cases where Doctor Agents disagree on final diagnosis, reflecting epistemic uncertainty.
    \item \textbf{Justification Alignment (JA):} Semantic similarity between reasoning traces and verified clinical evidence, capturing the fidelity of explanations to reality.
\end{itemize}

\paragraph{Composite Trust Index (TI): Definition and Rationale.}
To unify diagnostic reliability, inter-agent stability, and reasoning coherence into a single interpretable measure, we define the \textbf{Trust Index (TI)} as a composite of three dimensions:
\[
\mathrm{TI} \;=\; 0.4 \times \mathrm{Accuracy} \;+\; 0.3 \times (100 - \mathrm{CDR}) \;+\; 0.3 \times \overline{\mathrm{RDC}}.
\]
Here, \textbf{Accuracy} denotes the proportion of correct or clinically equivalent diagnoses; 
\textbf{CDR} (\emph{Consensus Disagreement Rate}) quantifies epistemic instability across doctor replicas; and 
$\overline{\textbf{RDC}}$ (\emph{Reasoning–Diagnosis Coherence}) measures the semantic consistency between each model’s reasoning trace and its predicted or ground-truth diagnosis.

All values are normalized to the range $[0,100]$ for comparability. 
The weighting $(0.4, 0.3, 0.3)$ reflects a balanced heuristic that emphasizes diagnostic correctness (40\%) while giving equal secondary importance to stability and reasoning coherence (30\% each). 
This formulation draws inspiration from conceptual frameworks on trustworthy medical AI that link \emph{accuracy}, \emph{consistency}, and \emph{interpretability} as coequal foundations of clinical trust \cite{doshi2017towards,holzinger2022causability,amann2020explainability}.

\paragraph{Interpretation of Reasoning–Diagnosis Coherence (RDC).}
Because the \textit{MedQA} dataset provides only patient vignettes, investigations, and correct diagnoses—without clinician-authored reasoning chains—faithful justification alignment cannot be computed directly. 
Accordingly, we operationalize \textbf{RDC} as the semantic coherence between the model’s reasoning trace (text within \texttt{<thinking\_process>} tags) and its diagnostic statement, rather than between reasoning and ground-truth rationale. 
Formally:
\[
\mathrm{RDC}_i = 50 \times \left(1 + \cos \big(E(t_i), E(d_i)\big)\right),
\]
where $E(\cdot)$ encodes text using a sentence-transformer and $\cos$ denotes cosine similarity.
High RDC thus reflects that a model’s explanation linguistically supports its diagnostic choice—serving as a proxy for \emph{internal justification consistency}, not factual correctness.

\paragraph{Computation of Metrics.}
CDR is defined as the fraction of test cases where independent Doctor Agents reach non-identical final diagnoses.
Accuracy is computed via expert adjudication of clinical equivalence between the model’s output and the reference diagnosis. 
RDC is averaged across all cases, and all three metrics are linearly combined to yield the overall Trust Index:
\[
\mathrm{TI} = 0.4\,\overline{\mathrm{Accuracy}} + 0.3(100-\overline{\mathrm{CDR}}) + 0.3\,\overline{\mathrm{RDC}}.
\]

\paragraph{Algorithmic Definition.}
\begin{algorithm}[H]
\caption{Computation of Trustworthiness Metrics in MedGuard-X}
\label{alg:trustmetrics}
\begin{algorithmic}[1]
\Require For each case $i$: Doctor diagnoses $\{d_{i1},\dots,d_{iK}\}$, ground truth $g_i$, reasoning trace $t_i$.
\Ensure Per-case metrics: Accuracy$_i$, CDR$_i$, RDC$_i$.
\State $\text{Accuracy}_i = \mathbb{1}[\texttt{equivalent}(d_{im^*}, g_i)]$, where $m^*$ is the modal vote.
\State $\text{CDR}_i = 1$ if $\text{argmax}_m \text{count}(d_{im}) < K$, else $0$.
\State Encode reasoning trace $t_i$ and diagnosis $d_i$ using a sentence-transformer $E(\cdot)$.
\State Compute $\text{RDC}_i = 50(1 + \cos(E(t_i), E(d_i)))$.
\State Aggregate over all cases to compute the system-level TI:
\[
\mathrm{TI} = 0.4 \times \overline{\mathrm{Accuracy}} + 0.3 \times (100-\overline{\mathrm{CDR}}) + 0.3 \times \overline{\mathrm{RDC}}.
\]
\end{algorithmic}
\end{algorithm}

\paragraph{Weight Sensitivity.}
To ensure robustness, we conducted a weight-sweep ablation varying the contribution of Accuracy, Stability, and RDC. 
As shown in Table~\ref{tab:trust_sweep}, system rankings remain stable across configurations, indicating that the Trust Index captures an inherent multi-dimensional notion of reliability rather than being sensitive to specific weight choices.

\begin{table}[t]
\centering
\caption{Sensitivity of Trust Index (TI) to weighting scheme. Rankings remain consistent across allocations, indicating robustness of comparative trends.}
\label{tab:trust_sweep}
\begin{tabular}{lcccc}
\toprule
\textbf{System} & \textbf{(0.4,0.3,0.3)} & \textbf{(0.5,0.25,0.25)} & \textbf{(0.3,0.35,0.35)} & \textbf{Geometric Mean} \\
\midrule
Baseline (LLM only) & 78.4 & 79.1 & 77.9 & 76.8 \\
MedGuard (safety) & 81.2 & 81.5 & 80.7 & 79.9 \\
MedGuard-X (full) & \textbf{85.6} & \textbf{85.2} & \textbf{85.9} & \textbf{84.7} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Limitations.}
The Trust Index should be viewed as a \emph{proxy} for reasoning reliability rather than a validated measure of clinical truthfulness. 
RDC evaluates linguistic coherence, which may reward stylistic agreement even in partially flawed reasoning. 
Nevertheless, it provides a practical, quantifiable lens on internal consistency—bridging interpretability and reliability when explicit ground-truth reasoning data are unavailable.



\subsection*{7. Summary}
MedGuard-X transforms the opaque reasoning of medical LLMs into a transparent, auditable process. By combining consensus-driven uncertainty estimation, structured reasoning traces, and real-time logging, it redefines trust as both an epistemic and procedural property. While the system demonstrably improves transparency and safety adherence, its diagnostic accuracy remains variable across cases—reflecting the persistent uncertainty and instability of current LLM reasoning in medical contexts. In the following section, we evaluate MedGuard-X on the \textit{MedQA} benchmark to assess its diagnostic accuracy, safety compliance, and reasoning coherence relative to baseline medical LLMs.
