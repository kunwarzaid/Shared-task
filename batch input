import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ======================================================
# 1Ô∏è‚É£ Load and clean the CSV
# ======================================================
CSV_PATH = "Shared_task_results(Zaid).csv"

# Read with multi-row header (first two rows)
df = pd.read_csv(CSV_PATH, header=[0, 1])

# Flatten header pairs like ('qna', 'f1') -> 'qna_f1'
df.columns = [
    f"{a}_{b}".strip("_").lower() for a, b in df.columns
]

# The first row likely contains "Language"
if "language" not in df.columns[0].lower():
    raise ValueError("Couldn't detect 'Language' column. Check your CSV structure.")
df.rename(columns={df.columns[0]: "Language"}, inplace=True)

print("‚úÖ Columns loaded:")
print(df.columns.tolist())

# Convert all numeric columns safely
for c in df.columns:
    if c != "Language":
        df[c] = pd.to_numeric(df[c], errors="coerce")

# ======================================================
# 2Ô∏è‚É£ Define tasks and metrics
# ======================================================
tasks = ["qna", "summary_text", "summary_knv"]
metrics = ["f1", "bertscore_f1", "cometscore"]

print("\n‚úÖ Tasks detected:", tasks)
print("‚úÖ Metrics detected:", metrics)

# ======================================================
# 3Ô∏è‚É£ Compute task-wise averages (macro-average across languages)
# ======================================================
task_avgs = {}
for task in tasks:
    task_avgs[task] = {}
    for metric in metrics:
        col = f"{task}_{metric}"
        if col in df.columns:
            task_avgs[task][metric] = df[col].mean(skipna=True)
        else:
            task_avgs[task][metric] = np.nan

print("\n‚úÖ Task-wise averages:")
for t, vals in task_avgs.items():
    print(f"  {t}: " + ", ".join([f"{m}={vals[m]:.3f}" for m in vals]))

# ======================================================
# 4Ô∏è‚É£ Compute language-wise averages across all tasks
# ======================================================
language_scores = df[["Language"]].copy()

for metric in metrics:
    metric_cols = [f"{t}_{metric}" for t in tasks if f"{t}_{metric}" in df.columns]
    language_scores[metric] = df[metric_cols].mean(axis=1, skipna=True)

print("\n‚úÖ Language-wise average metrics computed successfully.")
print(language_scores.head())

# ======================================================
# 5Ô∏è‚É£ Plot: Task-wise performance
# ======================================================
plt.figure(figsize=(8, 5))
x = np.arange(len(tasks))
width = 0.25
colors = {"f1": "#4e79a7", "bertscore_f1": "#59a14f", "cometscore": "#e15759"}

for i, metric in enumerate(metrics):
    vals = [task_avgs[t][metric] for t in tasks]
    plt.bar(x + (i - 1) * width, vals, width, label=metric.replace("_f1", "").upper(), color=colors[metric])
    for j, v in enumerate(vals):
        plt.text(x[j] + (i - 1) * width, v + 0.005, f"{v:.3f}", ha="center", fontsize=8)

plt.xticks(x, [t.replace("_", " ").title() for t in tasks])
plt.ylabel("Average Score", fontsize=12)
plt.ylim(0, 1)
plt.title("Average Performance Across Tasks", fontsize=14, weight="bold")
plt.legend(frameon=False, loc="upper right")
plt.tight_layout()
plt.savefig("taskwise_performance_final.png", dpi=300)
plt.show()

# ======================================================
# 6Ô∏è‚É£ Plot: Language-wise performance
# ======================================================
plt.figure(figsize=(10, 6))
langs = language_scores["Language"].tolist()
x = np.arange(len(langs))
width = 0.25

for i, metric in enumerate(metrics):
    vals = language_scores[metric].tolist()
    plt.bar(x + (i - 1) * width, vals, width, label=metric.replace("_f1", "").upper(), color=colors[metric])
    for j, v in enumerate(vals):
        plt.text(x[j] + (i - 1) * width, v + 0.005, f"{v:.2f}", ha="center", fontsize=7)

plt.xticks(x, langs, rotation=45, ha="right")
plt.ylabel("Score", fontsize=12)
plt.ylim(0, 1)
plt.title("Language-wise Performance Across Metrics", fontsize=14, weight="bold")
plt.legend(frameon=False, loc="upper right")
plt.tight_layout()
plt.savefig("languagewise_performance_final.png", dpi=300)
plt.show()

# ======================================================
# 7Ô∏è‚É£ Generate LaTeX table
# ======================================================
avg_f1 = np.nanmean(language_scores["f1"])
avg_bert = np.nanmean(language_scores["bertscore_f1"])
avg_comet = np.nanmean(language_scores["cometscore"])

best_f1_lang = language_scores.loc[language_scores["f1"].idxmax(), "Language"]
best_bert_lang = language_scores.loc[language_scores["bertscore_f1"].idxmax(), "Language"]
best_comet_lang = language_scores.loc[language_scores["cometscore"].idxmax(), "Language"]

latex = "\\begin{table}[h]\n\\centering\n\\small\n"
latex += "\\begin{tabular}{lccc}\n\\toprule\nLanguage & F1 & BERTScore & COMET \\\\\n\\midrule\n"

for _, row in language_scores.iterrows():
    lang = row["Language"]
    f, b, c = row["f1"], row["bertscore_f1"], row["cometscore"]
    f_str = f"\\textbf{{{f:.3f}}}" if lang == best_f1_lang else f"{f:.3f}"
    b_str = f"\\textbf{{{b:.3f}}}" if lang == best_bert_lang else f"{b:.3f}"
    c_str = f"\\textbf{{{c:.3f}}}" if lang == best_comet_lang else f"{c:.3f}"
    latex += f"{lang} & {f_str} & {b_str} & {c_str} \\\\\n"

latex += "\\midrule\n"
latex += f"\\textbf{{Average}} & {avg_f1:.3f} & {avg_bert:.3f} & {avg_comet:.3f} \\\\\n"
latex += "\\bottomrule\n\\end{tabular}\n"
latex += "\\caption{Language-wise performance across metrics for QnA, Summary (Text), and Summary (JSON).}\n"
latex += "\\label{tab:language_perf}\n\\end{table}\n"

with open("language_results_table_final.tex", "w", encoding="utf-8") as f:
    f.write(latex)

print("\n‚úÖ Analysis complete!")
print("üìä Saved plots:")
print(" - taskwise_performance_final.png")
print(" - languagewise_performance_final.png")
print("üìÑ LaTeX table saved as language_results_table_final.tex")
