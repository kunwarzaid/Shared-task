\section{Introduction}

Large Language Models (LLMs) such as GPT-4 \cite{achiam2023gpt}, Gemini \cite{team2023gemini}, and Med-PaLM 2 \cite{singhal2025toward} have demonstrated remarkable progress in natural language understanding, reasoning, and generalization across diverse domains, including medicine. They now perform at near-clinician levels on medical question-answering benchmarks \cite{kung2023performance,nori2023capabilities}, suggesting potential use in decision support, documentation, and triage \cite{lee2023benefits}. However, the very properties that make LLMs powerful—scale, open-endedness, and linguistic fluency—also make them unreliable in settings that demand factual precision, causal reasoning, and accountability \cite{ji2023survey,begoli2019need}.

Integrating LLMs into healthcare introduces challenges that go beyond predictive accuracy. Medicine is not only about prediction but about explanation, justification, and trust. Clinicians reason through causality, uncertainty, and evidence weighting—capabilities that current LLMs only partially emulate. When an AI system provides a diagnosis or recommends treatment, its credibility depends as much on \emph{why} it reached that conclusion as on \emph{what} it predicts \cite{tonekaboni2019clinicians,amann2020explainability}. Without a transparent reasoning process, even correct answers are unsafe, as the underlying rationale may be spurious or unverifiable.

Despite recent advances in alignment and red-teaming \cite{mei2023assert}, clinical LLMs still exhibit fundamental limitations in trustworthiness—a property encompassing accuracy, consistency, safety, and interpretability \cite{a2019,huang2025survey}. Many models produce internally inconsistent explanations, display unjustified confidence \cite{kadavath2022language}, or fail to recognize uncertainty in ambiguous cases. This epistemic overconfidence poses a severe risk in medicine, where admitting uncertainty is often safer than being confidently wrong. Similarly, while medical LLMs such as Med-PaLM 2 \cite{singhal2025toward}, BioGPT \cite{luo2022biogpt}, and ChatDoctor \cite{li2023chatdoctor} achieve high factual accuracy, they offer limited insight into reasoning stability: small prompt changes can yield substantially different diagnostic outcomes.

Our previous system, MedGuard, addressed safety but not transparency. It introduced a multi-agent simulation—Doctor, Patient, Measurement, and Safety Agents—that collectively mirrored a clinical workflow. Each agent had a defined responsibility: the Doctor reasoned and questioned, the Patient responded based on case data, the Measurement Agent provided or denied tests, and the Safety Agent evaluated prescription risks. MedGuard showed that general-purpose LLMs could act responsibly when embedded in structured, safety-aware environments, reducing unsafe test orders and prescriptions through drug–drug interaction (DDI) checks and risk-aware test assessment.

Yet evaluation revealed a critical gap: while MedGuard was \emph{safer}, it was not necessarily \emph{more trustworthy}. It could prevent unsafe outcomes but not explain how decisions were made. The reasoning process remained opaque, the differential diagnosis invisible, and the internal deliberations lost between prompts. Clinicians could not determine whether the system’s diagnostic path was coherent, evidence-grounded, or merely plausible.

This limitation motivated the development of MedGuard-X—a shift from compliance toward understanding. Whereas MedGuard asked “Is the outcome safe?”, MedGuard-X asks “Can we trust the reasoning process that produced it?” To answer that question, MedGuard-X introduces four integrated mechanisms that transform the system from a reactive monitor into a transparent reasoning companion:

\begin{itemize}
    \item \textbf{Consensus Reasoning:} Multiple Doctor Agents independently evaluate each case, compare hypotheses, and vote on the most likely diagnosis. The level of disagreement, quantified as the \emph{Consensus Disagreement Rate (CDR)}, measures epistemic uncertainty—an automated form of “second opinion.”
    \item \textbf{Reasoning Trace Explainability:} Every diagnostic step is recorded as a structured reasoning trace (\texttt{<thinking\_process>}) linking each decision to explicit evidence, patient statements, and test results. These traces make visible how evidence shapes evolving hypotheses.
    \item \textbf{End-to-End Logging and Traceability:} All agent interactions—who acted, why, and what evidence informed the decision—are logged in real time with timestamps and context hashes. These logs are not passive records but narrative reconstructions that enable causal debugging and retrospective auditing.
    \item \textbf{Differential Diagnosis Lifecycle:} The Doctor Agent maintains a ranked list of differential diagnoses (DDx) that evolves as new evidence emerges. Each DDx update—promotion, demotion, or exclusion—is logged, exposing the causal path from uncertainty to final confidence.
\end{itemize}

Together, these mechanisms form a foundation not only for safety but for \emph{credibility}. Logging transforms reasoning into a visible, auditable process; consensus converts intuition into measurable uncertainty; and the explicit DDx ledger bridges human and machine reasoning by making the model’s “thought process” inspectable. MedGuard-X thus reframes safety as a pathway to understanding. By embedding traceability, consensus, and explainability directly into its architecture, it brings interpretability into the core of model behavior rather than treating it as an external afterthought. The next question, then, is how this approach aligns with and diverges from prior efforts in explainable and trustworthy AI for healthcare.

\section{Related Work}

The pursuit of explainability in artificial intelligence has long sought to bridge the gap between algorithmic performance and human understanding. Early approaches to Explainable AI (XAI) emphasized feature attribution and surrogate modeling. Techniques such as LIME \cite{ribeiro2016should}, SHAP \cite{lundberg2017unified}, and Grad-CAM \cite{selvaraju2017grad} provided local interpretability for image and tabular models by identifying influential features. Yet such post-hoc methods are limited in reasoning-intensive domains like medicine, where decisions evolve sequentially through dialogue and evidence synthesis. As Holzinger et al. \cite{holzinger2022toward} and Ahmad et al. \cite{ahmad2018interpretable} observed, true medical explainability requires \emph{causability}—a human-understandable mapping between evidence, inference, and outcome.

The rise of LLMs has redefined explainability through explicit reasoning traces and self-reflection. Methods such as chain-of-thought prompting \cite{wei2022chain}, self-consistency \cite{wang2022self}, and reflection-based reasoning \cite{shinn2023reflexion} externalize a model’s internal deliberations as text. However, these explanations are self-generated and unverifiable, often serving as post-hoc rationalizations rather than faithful accounts of reasoning \cite{turpin2023language}. Studies show that models can express confident but incorrect rationales \cite{kadavath2022language,ji2023survey}, producing “hallucinated certainty” that undermines reliability. Thus, transparency alone does not ensure \emph{truthfulness of thought}.

In medicine, explainability and safety are deeply intertwined. Tonekaboni et al. \cite{tonekaboni2019clinicians} and Amann et al. \cite{amann2020explainability} emphasize that clinicians judge models not only by accuracy but by reasoning legitimacy—whether conclusions follow clinical logic. LLM-based medical assistants such as BioGPT \cite{luo2022biogpt}, Med-PaLM 2 \cite{singhal2025toward}, and ChatDoctor \cite{li2023chatdoctor} demonstrate strong factual performance yet remain opaque in diagnostic reasoning. Systems like Clinical-Camel \cite{wang2023clinicalcamel} and DoctorGPT \cite{yang2024doctorgpt} introduced interactive consultation dialogues but still operate as single-agent systems, lacking mechanisms for consistency verification, uncertainty calibration, or differential-diagnosis tracking.

Multi-agent frameworks have begun to address these issues by distributing cognitive roles among specialized agents. AgentClinic \cite{ma2024agentclinic} modeled doctor–patient interaction as a cooperative dialogue between reasoning and information agents, improving coherence and completeness of clinical exchanges. However, such frameworks stop short of embedding accountability: agents converse but do not \emph{explain themselves} in a machine-auditable way. Likewise, safety-focused systems such as GuardMed \cite{peters2023safeaimed} and SafetyBench \cite{ghosh2024alignment} introduce red-teaming and safety supervision but center on output control rather than reasoning transparency.

Parallel work in AI governance reinforces this need for auditability. The World Health Organization’s guidance on ethics and governance of AI in health \cite{guidance2021ethics} and the European Commission’s framework for trustworthy AI \cite{bomhard2021regulation} stress traceability and accountability as prerequisites for deployment. Yet most clinical LLM pipelines remain black boxes, offering no continuous record of decision evolution—undermining reproducibility and clinician trust \cite{begoli2019need,doshi2017towards}.

MedGuard-X advances this trajectory by operationalizing explainability as a system-level property. It integrates real-time logging, explicit reasoning traces, and consensus-based uncertainty estimation directly into the diagnostic process. Each agent interaction—question, test order, or prescription—is time-stamped and context-linked, forming a verifiable audit trail. Unlike static XAI approaches that reconstruct reasoning post hoc, MedGuard-X enables \emph{process-level transparency}: the ability to trace how evidence, hypotheses, and safety constraints evolve over time. Its explicit differential-diagnosis ledger reveals how uncertainty resolves into confidence, while the consensus module quantifies disagreement across agents. In doing so, MedGuard-X transforms explainability from an interpretive tool into an intrinsic property of clinical reasoning—moving toward AI systems clinicians can interrogate, audit, and ultimately trust.
